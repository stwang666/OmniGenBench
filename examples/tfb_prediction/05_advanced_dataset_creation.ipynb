{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d3f24c",
   "metadata": {},
   "source": [
    "# 🔬 Advanced Dataset Creation: From Raw Data to ML-Ready Datasets\n",
    "\n",
    "Welcome to the **advanced tutorial** in our series! Now that you've mastered the basics of data preparation and model training, it's time to tackle the real-world challenge every computational biologist faces: **creating custom datasets from your own biological data**.\n",
    "\n",
    "> 🎯 **Learning Objectives**: Master custom dataset creation, professional data templates, configuration management, and publication-ready data workflows\n",
    "\n",
    "---\n",
    "\n",
    "## 🧬 Why Custom Datasets Matter in Genomics Research\n",
    "\n",
    "In your research career, you'll encounter scenarios where existing datasets don't meet your needs:\n",
    "\n",
    "- 🔬 **Novel Research Questions**: Your hypothesis requires unique data combinations\n",
    "- 📊 **Proprietary Data**: Laboratory experiments generate exclusive datasets\n",
    "- 🎯 **Domain-Specific Tasks**: Specialized applications (drug discovery, agriculture, personalized medicine)\n",
    "- 📈 **Publication Requirements**: Original datasets strengthen research impact\n",
    "\n",
    "This tutorial teaches you to transform raw biological data into **publication-ready, machine learning datasets** that integrate seamlessly with OmniGenBench.\n",
    "\n",
    "### 🎓 Real-World Research Scenario\n",
    "\n",
    "Imagine you're investigating **gene regulation** and have:\n",
    "- DNA sequences from ChIP-seq experiments\n",
    "- Experimental validation of promoter activity\n",
    "- Metadata including tissue type, condition, GC content\n",
    "- A hypothesis about sequence-function relationships\n",
    "\n",
    "**Your Goal**: Build a robust classifier to predict promoter activity from DNA sequence alone.\n",
    "\n",
    "> 💡 **Research Impact**: This exact workflow has been used in breakthrough papers published in *Nature*, *Science*, and *Cell*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348e4d4",
   "metadata": {},
   "source": [
    "## 📋 The Universal Data Template: CSV Format Mastery\n",
    "\n",
    "OmniGenBench uses a **standardized CSV format** that works across all genomic tasks. This universality is what makes the framework so powerful for research.\n",
    "\n",
    "### 🔑 Core Template Structure\n",
    "\n",
    "| Field Name | Description | Required? | Data Type | Example |\n",
    "|------------|-------------|-----------|-----------|---------|\n",
    "| **`sequence`** | Your biological sequence (DNA/RNA/Protein) | ✅ **Required** | String | `ATCGATCGATCG...` |\n",
    "| **`label`** | The target variable you want to predict | ✅ **Required** | Numeric/String | `1`, `0.75`, `\"high\"` |\n",
    "| **`id`** | Unique identifier for each sample | ⚠️ **Recommended** | String | `seq_001`, `gene_XYZ` |\n",
    "| **`split`** | Dataset partition (train/valid/test) | ⚠️ **Recommended** | String | `train`, `valid`, `test` |\n",
    "\n",
    "### 🧬 Optional Biological Metadata\n",
    "\n",
    "The beauty of this format is **flexibility**! Add any biological annotations:\n",
    "\n",
    "| Field Type | Examples | Research Value |\n",
    "|------------|----------|----------------|\n",
    "| **Sequence Properties** | `gc_content`, `length`, `complexity` | Sequence-based features |\n",
    "| **Experimental Context** | `tissue`, `condition`, `treatment` | Biological context |\n",
    "| **Functional Annotations** | `gene_family`, `pathway`, `domain` | Functional categories |\n",
    "| **Quality Metrics** | `confidence_score`, `read_depth` | Data reliability |\n",
    "\n",
    "### 📊 Example CSV Structure\n",
    "\n",
    "```csv\n",
    "sequence,label,id,split,gc_content,tissue,confidence,source\n",
    "ATCGATCGATCGTACGAA,1,promoter_001,train,0.62,liver,0.95,human_genome\n",
    "GGGGCCCCAAAATTTTCC,0,random_001,train,0.50,liver,0.87,synthetic\n",
    "TACGATCGAAATTTCGAT,1,promoter_002,valid,0.56,brain,0.91,mouse_genome\n",
    "AAAAAAAATTTTTTTTGG,0,random_002,test,0.20,brain,0.79,synthetic\n",
    "```\n",
    "\n",
    "> 🔬 **Pro Tip**: This same template works for **all biological sequence tasks** - just change the `label` column meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5285b",
   "metadata": {},
   "source": [
    "### ⚠️ Dataset Synthesis & Label Semantics (Important)\n",
    "\n",
    "All sequence examples in this tutorial are **synthetic and illustrative**. Labels from each data source reflect *different biological notions* and are **not directly comparable** until explicitly harmonized:\n",
    "\n",
    "| Source | Column `label` Meaning | Positive Class (1) | Negative Class (0) |\n",
    "|--------|------------------------|--------------------|--------------------|\n",
    "| FASTA | Promoter-like vs background sequence | Promoter-like | Background/intergenic |\n",
    "| GFF | Gene biotype functional potential | protein_coding | Non-coding / other biotypes |\n",
    "| BED | Regulatory activity status | promoter / enhancer / silencer / insulator | background / other |\n",
    "| JSON | Expression-level categorization | High expression (>= threshold) | Low expression |\n",
    "\n",
    "During integration we add a `label_origin` column to preserve provenance. If you intend to train a single model, **first define a unified task** (e.g., “active regulatory regions”) and remap labels accordingly.\n",
    "\n",
    "> Disclaimer: Do **not** use these synthetic examples for biological inference; they are provided solely for pipeline illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357db031",
   "metadata": {},
   "source": [
    "## 🔄 Converting Biological Data Formats to CSV/TSV\n",
    "\n",
    "In genomics research, biological data comes in various specialized formats (FASTA, GFF, BED, JSON, etc.). This section teaches you how to systematically convert these common formats into machine learning-ready CSV/TSV datasets compatible with OmniGenBench.\n",
    "\n",
    "### 📁 Common Biological Data Format Conversion Matrix\n",
    "\n",
    "| Source Format | Description | Target Output | Research Applications |\n",
    "|---------------|-------------|---------------|----------------------|\n",
    "| **FASTA** | Sequence data (DNA/RNA/Protein) | CSV with sequences | Sequence classification, function prediction |\n",
    "| **GFF/GTF** | Genomic annotation files | CSV with genomic features | Gene expression, regulatory analysis |\n",
    "| **BED** | Genomic coordinate intervals | CSV with positional data | Peak detection, interval analysis |\n",
    "| **VCF** | Variant call format | CSV with genetic variants | Variant effect prediction |\n",
    "| **JSON** | Structured experimental data | CSV with metadata | Complex feature engineering |\n",
    "\n",
    "### 🎯 Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will master:\n",
    "1. **Format Recognition**: Understand characteristics of different biological data formats\n",
    "2. **Systematic Conversion**: Implement automated conversion pipelines using Python\n",
    "3. **Quality Control**: Validate data integrity and biological relevance\n",
    "4. **Best Practices**: Apply professional standards for reproducible research\n",
    "\n",
    "Let's start with a step-by-step approach to handle each format:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6dc6",
   "metadata": {},
   "source": [
    "## Environment Setup and Library Imports\n",
    "\n",
    "First, let's set up our working environment and import the necessary Python libraries for biological data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for biological data processing and conversion\n",
    "import pandas as pd          # Data manipulation and CSV handling\n",
    "import numpy as np           # Numerical operations and random data generation\n",
    "import os                    # File system operations\n",
    "import json                  # JSON data parsing\n",
    "from io import StringIO      # String buffer operations\n",
    "\n",
    "print(\"🔬 Biological Data Format Conversion Tutorial\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ All required libraries imported successfully\")\n",
    "\n",
    "# Create working directory for conversion examples\n",
    "os.makedirs('conversion_examples', exist_ok=True)\n",
    "print(\"📂 Created working directory: conversion_examples/\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"🎲 Random seed set to 42 for reproducible results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ecdbe",
   "metadata": {},
   "source": [
    "## FASTA Format Conversion\n",
    "\n",
    "FASTA is the most common format for biological sequences. Let's learn how to convert FASTA files into CSV format suitable for machine learning.\n",
    "\n",
    "### 🧬 FASTA Format Structure\n",
    "```\n",
    ">sequence_id|annotation_info\n",
    "ATCGATCGATCG...\n",
    ">next_sequence_id|annotation\n",
    "GCGCGCGC...\n",
    "```\n",
    "\n",
    "### 🎯 Conversion Strategy\n",
    "1. **Parse headers**: Extract sequence IDs and annotations\n",
    "2. **Extract sequences**: Read multi-line sequences \n",
    "3. **Generate labels**: Create target variables based on biological annotations\n",
    "4. **Create CSV**: Structure data in OmniGenBench-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ea944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 2: FASTA → CSV Conversion\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample FASTA file with realistic biological sequences\n",
    "fasta_content = \"\"\">seq1|promoter_human_chr1|BRCA1_upstream\n",
    "ATCGATCGATCGTACGAATTCCGGAAATTTCCCGGGAAATTTGGGCCCAAATTTAAAGGG\n",
    ">seq2|random_sequence_1|intergenic_control\n",
    "AAAATTTTCCCCGGGGAAAATTTTCCCCGGGGAAAATTTTCCCCGGGGAAAATTTT\n",
    ">seq3|promoter_mouse_chrX|TP53_regulatory\n",
    "GCGCGCGCATATATATATGCGCGCGCATATATATATGCGCGCGCATATATATAT\n",
    ">seq4|intergenic_region_1|background_control\n",
    "TTTTTTTTAAAAAAAACCCCCCCCGGGGGGGGTTTTTTTAAAAAAAACCCCCCCC\n",
    ">seq5|promoter_rat_chr2|MYC_enhancer\n",
    "TATAAAAAGCGCGCGCCCCCGGGGAAAATTTTTCCCGGGAAATTTAGCTAGCTAG\n",
    "\"\"\"\n",
    "\n",
    "# Save sample FASTA file\n",
    "fasta_path = 'conversion_examples/sample_sequences.fasta'\n",
    "with open(fasta_path, 'w') as f:\n",
    "    f.write(fasta_content)\n",
    "print(f\"✅ Created sample FASTA file: {fasta_path}\")\n",
    "\n",
    "def parse_fasta_to_csv(fasta_file, output_csv):\n",
    "    \"\"\"\n",
    "    Convert FASTA format to CSV with biological annotations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fasta_file : str\n",
    "        Path to input FASTA file\n",
    "    output_csv : str \n",
    "        Path for output CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed dataset with sequences and labels\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    annotations = []\n",
    "    \n",
    "    with open(fasta_file, 'r') as f:\n",
    "        current_seq = \"\"\n",
    "        current_id = \"\"\n",
    "        current_annotation = \"\"\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence if exists\n",
    "                if current_seq:\n",
    "                    sequences.append(current_seq)\n",
    "                    ids.append(current_id)\n",
    "                    annotations.append(current_annotation)\n",
    "                    \n",
    "                    # Generate binary label based on biological annotation\n",
    "                    # 1 = functional sequence (promoter), 0 = background/control\n",
    "                    label = 1 if 'promoter' in current_annotation.lower() else 0\n",
    "                    labels.append(label)\n",
    "                \n",
    "                # Parse new header: >id|type|annotation\n",
    "                header_parts = line[1:].split('|')\n",
    "                current_id = header_parts[0] if len(header_parts) > 0 else \"unknown\"\n",
    "                current_annotation = '|'.join(header_parts[1:]) if len(header_parts) > 1 else \"no_annotation\"\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                # Accumulate sequence lines\n",
    "                current_seq += line\n",
    "        \n",
    "        # Handle last sequence\n",
    "        if current_seq:\n",
    "            sequences.append(current_seq)\n",
    "            ids.append(current_id)\n",
    "            annotations.append(current_annotation)\n",
    "            label = 1 if 'promoter' in current_annotation.lower() else 0\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Create structured DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'sequence': sequences,\n",
    "        'label': labels,\n",
    "        'id': ids,\n",
    "        'annotation': annotations,\n",
    "        'split': 'train'  # Default assignment - can be modified later\n",
    "    })\n",
    "    \n",
    "    # Add sequence-level features for quality control\n",
    "    df['sequence_length'] = df['sequence'].apply(len)\n",
    "    df['gc_content'] = df['sequence'].apply(\n",
    "        lambda seq: (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# Execute FASTA conversion\n",
    "csv_path = 'conversion_examples/fasta_converted.csv'\n",
    "fasta_df = parse_fasta_to_csv(fasta_path, csv_path)\n",
    "\n",
    "print(f\"✅ Conversion completed: {len(fasta_df)} sequences processed\")\n",
    "print(f\"📊 Dataset composition:\")\n",
    "print(f\"   - Promoter sequences: {fasta_df['label'].sum()}\")\n",
    "print(f\"   - Background sequences: {len(fasta_df) - fasta_df['label'].sum()}\")\n",
    "print(f\"📁 Output saved: {csv_path}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\n📋 Sample converted data:\")\n",
    "print(fasta_df[['id', 'sequence_length', 'gc_content', 'label', 'annotation']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e3741",
   "metadata": {},
   "source": [
    "#### 🔎 Label Meaning (FASTA)\n",
    "`label = 1` → sequence header contains a promoter-like keyword; `0` → background / control. This is a heuristic.\n",
    "\n",
    "> If you need multi-class (e.g., promoter subclasses), add a `promoter_type` column instead of overloading `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631aca4",
   "metadata": {},
   "source": [
    "## GFF/GTF Format Conversion\n",
    "\n",
    "GFF (General Feature Format) and GTF (Gene Transfer Format) files contain genomic annotations. These are tab-delimited files describing genomic features and their coordinates.\n",
    "\n",
    "### 📋 GFF/GTF Structure\n",
    "```\n",
    "seqname  source  feature  start  end  score  strand  frame  attributes\n",
    "chr1     RefSeq  gene     1000   2000   .      +       .     ID=gene1;Name=BRCA1\n",
    "```\n",
    "\n",
    "### 🎯 Conversion Strategy\n",
    "1. **Parse coordinates**: Extract genomic positions (chromosome, start, end)\n",
    "2. **Extract features**: Focus on specific feature types (genes, exons, etc.)\n",
    "3. **Process attributes**: Parse semicolon-separated attribute fields\n",
    "4. **Generate sequences**: Create mock sequences based on coordinates (or extract from genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a83521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 3: GFF/GTF → CSV Conversion\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample GFF file with realistic genomic annotations\n",
    "gff_content = \"\"\"##gff-version 3\n",
    "chr1\tRefSeq\tgene\t1000\t2000\t.\t+\t.\tID=gene1;Name=BRCA1;biotype=protein_coding;description=tumor_suppressor\n",
    "chr1\tRefSeq\texon\t1000\t1200\t.\t+\t.\tID=exon1;Parent=gene1;exon_number=1\n",
    "chr1\tRefSeq\tCDS\t1050\t1150\t.\t+\t0\tID=cds1;Parent=gene1\n",
    "chr2\tRefSeq\tgene\t5000\t6000\t.\t-\t.\tID=gene2;Name=TP53;biotype=protein_coding;description=tumor_suppressor\n",
    "chr2\tRefSeq\texon\t5000\t5300\t.\t-\t.\tID=exon2;Parent=gene2;exon_number=1\n",
    "chr3\tRefSeq\tgene\t8000\t9500\t.\t+\t.\tID=gene3;Name=MYC;biotype=protein_coding;description=oncogene\n",
    "chr3\tRefSeq\tpseudogene\t12000\t12500\t.\t+\t.\tID=pseudo1;Name=PSEU1;biotype=pseudogene;description=non_coding\n",
    "chrX\tRefSeq\tgene\t15000\t16000\t.\t-\t.\tID=geneX;Name=XIST;biotype=lncRNA;description=X_inactivation\n",
    "\"\"\"\n",
    "\n",
    "# Save sample GFF file\n",
    "gff_path = 'conversion_examples/sample_annotations.gff'\n",
    "with open(gff_path, 'w') as f:\n",
    "    f.write(gff_content)\n",
    "print(f\"✅ Created sample GFF file: {gff_path}\")\n",
    "\n",
    "def parse_gff_to_csv(gff_file, output_csv, feature_type='gene', max_sequence_length=200):\n",
    "    \"\"\"\n",
    "    Convert GFF annotations to CSV format with genomic features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gff_file : str\n",
    "        Path to input GFF file\n",
    "    output_csv : str\n",
    "        Path for output CSV file  \n",
    "    feature_type : str\n",
    "        GFF feature type to extract (default: 'gene')\n",
    "    max_sequence_length : int\n",
    "        Maximum length for generated sequences\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed dataset with genomic features\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(gff_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "                \n",
    "            # Parse GFF fields\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) < 9:\n",
    "                continue\n",
    "                \n",
    "            seqname, source, feature, start, end, score, strand, frame, attributes = fields\n",
    "            \n",
    "            # Process only specified feature type\n",
    "            if feature == feature_type:\n",
    "                # Parse attributes field (key=value pairs separated by semicolons)\n",
    "                attr_dict = {}\n",
    "                for attribute in attributes.split(';'):\n",
    "                    if '=' in attribute:\n",
    "                        key, value = attribute.split('=', 1)\n",
    "                        attr_dict[key.strip()] = value.strip()\n",
    "                \n",
    "                # Extract key information\n",
    "                gene_id = attr_dict.get('ID', 'unknown')\n",
    "                gene_name = attr_dict.get('Name', 'unnamed')\n",
    "                biotype = attr_dict.get('biotype', 'unknown')\n",
    "                description = attr_dict.get('description', 'no_description')\n",
    "                \n",
    "                # Generate realistic sequence based on coordinates\n",
    "                feature_length = min(int(end) - int(start), max_sequence_length)\n",
    "                \n",
    "                # Create biologically relevant sequences based on gene type\n",
    "                if 'tumor_suppressor' in description:\n",
    "                    # Tumor suppressors often have CG-rich promoters\n",
    "                    mock_sequence = ''.join(np.random.choice(['G', 'C', 'A', 'T'], \n",
    "                                                           feature_length, \n",
    "                                                           p=[0.3, 0.3, 0.2, 0.2]))\n",
    "                elif 'oncogene' in description:\n",
    "                    # Oncogenes may have different sequence characteristics\n",
    "                    mock_sequence = ''.join(np.random.choice(['A', 'T', 'G', 'C'], \n",
    "                                                           feature_length, \n",
    "                                                           p=[0.3, 0.3, 0.2, 0.2]))\n",
    "                else:\n",
    "                    # Default random sequence\n",
    "                    mock_sequence = ''.join(np.random.choice(['A', 'T', 'C', 'G'], feature_length))\n",
    "                \n",
    "                # Create binary classification labels based on biotype\n",
    "                # 1 = protein_coding genes, 0 = non-coding elements\n",
    "                label = 1 if biotype == 'protein_coding' else 0\n",
    "                \n",
    "                # Compile feature data\n",
    "                feature_data = {\n",
    "                    'sequence': mock_sequence,\n",
    "                    'label': label,\n",
    "                    'id': f\"{gene_name}_{seqname}_{start}_{end}\",\n",
    "                    'chromosome': seqname,\n",
    "                    'start_position': int(start),\n",
    "                    'end_position': int(end),\n",
    "                    'strand': strand,\n",
    "                    'gene_name': gene_name,\n",
    "                    'biotype': biotype,\n",
    "                    'description': description,\n",
    "                    'split': 'train'\n",
    "                }\n",
    "                \n",
    "                data.append(feature_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add computed features\n",
    "    df['feature_length'] = df['end_position'] - df['start_position']\n",
    "    df['gc_content'] = df['sequence'].apply(\n",
    "        lambda seq: (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# Execute GFF conversion\n",
    "gff_csv_path = 'conversion_examples/gff_converted.csv'\n",
    "gff_df = parse_gff_to_csv(gff_path, gff_csv_path, feature_type='gene')\n",
    "\n",
    "print(f\"✅ GFF conversion completed: {len(gff_df)} genomic features processed\")\n",
    "print(f\"📊 Gene classification:\")\n",
    "print(f\"   - Protein-coding genes: {gff_df['label'].sum()}\")\n",
    "print(f\"   - Non-coding features: {len(gff_df) - gff_df['label'].sum()}\")\n",
    "print(f\"📁 Output saved: {gff_csv_path}\")\n",
    "\n",
    "# Display sample results  \n",
    "print(f\"\\n📋 Sample genomic features:\")\n",
    "print(gff_df[['gene_name', 'biotype', 'chromosome', 'feature_length', 'gc_content', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf96bd8",
   "metadata": {},
   "source": [
    "#### 🔎 Label Meaning (GFF/GTF)\n",
    "`label = 1` → feature biotype == `protein_coding`; `0` → other (lncRNA, pseudogene, etc.).\n",
    "\n",
    "> For transcript-level tasks, consider constructing exon/CDS aggregation features instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dc074",
   "metadata": {},
   "source": [
    "## BED Format Conversion\n",
    "\n",
    "BED (Browser Extensible Data) format describes genomic intervals and is widely used for ChIP-seq peaks, regulatory elements, and genomic annotations.\n",
    "OmniGenBench has been ready to load BED data directly, but understanding how to convert and structure your BED files is crucial for effective dataset creation.\n",
    "\n",
    "### 📋 BED Format Structure\n",
    "```\n",
    "chr1    1000    2000    peak1    100    +    enhancer\n",
    "chr2    3000    3500    peak2     85    -    promoter\n",
    "```\n",
    "\n",
    "### 📊 Column Definitions\n",
    "- **Columns 1-3**: Required (chromosome, start, end)\n",
    "- **Columns 4-6**: Optional (name, score, strand)  \n",
    "- **Column 7+**: Custom annotations (regulatory type, cell type, etc.)\n",
    "\n",
    "### 🎯 Conversion Strategy\n",
    "1. **Parse coordinates**: Extract genomic intervals\n",
    "2. **Score analysis**: Interpret peak scores or confidence values\n",
    "3. **Regulatory classification**: Use annotations for label generation\n",
    "4. **Quality metrics**: Add peak width and score-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a12cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 4: BED → CSV Conversion\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample BED file with ChIP-seq peaks and regulatory annotations\n",
    "bed_content = \"\"\"chr1\\t1000\\t2000\\tenhancer_peak_1\\t150\\t+\\tenhancer\\tH3K27ac\\tliver_specific\n",
    "chr1\\t3000\\t3500\\tpromoter_peak_1\\t200\\t+\\tpromoter\\tH3K4me3\\thousekeeping\n",
    "chr2\\t5000\\t5300\\tenhancer_peak_2\\t120\\t-\\tenhancer\\tH3K4me1\\ttissue_specific\n",
    "chr3\\t8000\\t8200\\tpromoter_peak_2\\t180\\t+\\tpromoter\\tH3K4me3\\tinducible\n",
    "chr3\\t9000\\t9400\\tsilencer_peak_1\\t95\\t-\\tsilencer\\tH3K27me3\\trepressive\n",
    "chr4\\t12000\\t12600\\tinsulator_peak_1\\t110\\t+\\tinsulator\\tCTCF\\tboundary\n",
    "chrX\\t15000\\t15400\\tenhancer_peak_3\\t140\\t-\\tenhancer\\tH3K27ac\\tsex_specific\n",
    "chr2\\t20000\\t20250\\tbackground_1\\t60\\t.\\tbackground\\tlow_signal\\tcontrol\n",
    "\"\"\"\n",
    "\n",
    "# Save sample BED file\n",
    "bed_path = 'conversion_examples/sample_peaks.bed'\n",
    "with open(bed_path, 'w') as f:\n",
    "    f.write(bed_content)\n",
    "print(f\"✅ Created sample BED file: {bed_path}\")\n",
    "\n",
    "def parse_bed_to_csv(bed_file, output_csv, min_score=50):\n",
    "    \"\"\"\n",
    "    Convert BED format peaks to CSV with regulatory annotations.\n",
    "    Safe against uniform-score division and supports source retention.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(bed_file, 'r') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) >= 3:\n",
    "                chromosome = fields[0]\n",
    "                try:\n",
    "                    start = int(fields[1]); end = int(fields[2])\n",
    "                except ValueError:\n",
    "                    continue  # skip malformed\n",
    "                peak_name = fields[3] if len(fields) > 3 else f\"peak_{start}_{end}\"\n",
    "                try:\n",
    "                    score = int(fields[4]) if len(fields) > 4 else 0\n",
    "                except ValueError:\n",
    "                    score = 0\n",
    "                strand = fields[5] if len(fields) > 5 else '.'\n",
    "                regulatory_type = fields[6] if len(fields) > 6 else 'unknown'\n",
    "                histone_mark = fields[7] if len(fields) > 7 else 'no_mark'\n",
    "                functional_class = fields[8] if len(fields) > 8 else 'unclassified'\n",
    "\n",
    "                if score >= min_score:\n",
    "                    peak_width = max(1, end - start)\n",
    "                    if regulatory_type == 'promoter':\n",
    "                        seq_pattern = ['G', 'C'] * 3 + ['A', 'T'] * 2\n",
    "                        if np.random.random() > 0.4:\n",
    "                            tata_seq = list(\"TATAAA\")\n",
    "                            body_len = max(0, peak_width - 6)\n",
    "                            mock_sequence = ''.join(np.random.choice(seq_pattern, body_len)) + ''.join(tata_seq)\n",
    "                        else:\n",
    "                            mock_sequence = ''.join(np.random.choice(seq_pattern, peak_width))\n",
    "                    elif regulatory_type == 'enhancer':\n",
    "                        seq_pattern = ['A', 'T', 'G', 'C'] * 2\n",
    "                        mock_sequence = ''.join(np.random.choice(seq_pattern, peak_width))\n",
    "                    elif regulatory_type == 'silencer':\n",
    "                        seq_pattern = ['A', 'T'] * 3 + ['G', 'C']\n",
    "                        mock_sequence = ''.join(np.random.choice(seq_pattern, peak_width))\n",
    "                    else:\n",
    "                        mock_sequence = ''.join(np.random.choice(['A', 'T', 'C', 'G'], peak_width))\n",
    "\n",
    "                    mock_sequence = mock_sequence[:500]\n",
    "                    active_elements = ['promoter', 'enhancer', 'silencer', 'insulator']\n",
    "                    label = 1 if regulatory_type in active_elements else 0\n",
    "\n",
    "                    data.append({\n",
    "                        'sequence': mock_sequence,\n",
    "                        'label': label,\n",
    "                        'id': peak_name,\n",
    "                        'chromosome': chromosome,\n",
    "                        'start_position': start,\n",
    "                        'end_position': end,\n",
    "                        'peak_width': peak_width,\n",
    "                        'peak_score': score,\n",
    "                        'strand': strand,\n",
    "                        'regulatory_type': regulatory_type,\n",
    "                        'histone_mark': histone_mark,\n",
    "                        'functional_class': functional_class,\n",
    "                        'split': 'train'\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if len(df) == 0:\n",
    "        print(\"⚠️ No peaks passed filters; empty DataFrame returned.\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        return df\n",
    "\n",
    "    df['sequence_length'] = df['sequence'].apply(len)\n",
    "    df['gc_content'] = df['sequence'].apply(lambda seq: (seq.count('G') + seq.count('C')) / len(seq) if len(seq) > 0 else 0)\n",
    "\n",
    "    score_range = df['peak_score'].max() - df['peak_score'].min()\n",
    "    if score_range > 0:\n",
    "        df['normalized_score'] = (df['peak_score'] - df['peak_score'].min()) / score_range\n",
    "    else:\n",
    "        df['normalized_score'] = 0.5  # uniform fallback\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# Execute BED conversion\n",
    "bed_csv_path = 'conversion_examples/bed_converted.csv'\n",
    "bed_df = parse_bed_to_csv(bed_path, bed_csv_path, min_score=60)\n",
    "\n",
    "print(f\"✅ BED conversion completed: {len(bed_df)} regulatory peaks processed\")\n",
    "print(f\"📊 Regulatory element classification:\")\n",
    "print(f\"   - Active regulatory elements: {bed_df['label'].sum()}\")\n",
    "print(f\"   - Background regions: {len(bed_df) - bed_df['label'].sum()}\")\n",
    "print(f\"📋 Regulatory type distribution:\")\n",
    "for rt, cnt in bed_df['regulatory_type'].value_counts().items():\n",
    "    print(f\"   - {rt}: {cnt} peaks\")\n",
    "print(f\"📁 Output saved: {bed_csv_path}\")\n",
    "print(\"\\n📋 Sample regulatory elements:\")\n",
    "print(bed_df[['id', 'regulatory_type', 'peak_score', 'gc_content', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab2950",
   "metadata": {},
   "source": [
    "#### 🔎 Label Meaning (BED)\n",
    "`label = 1` → regulatory_type in {promoter, enhancer, silencer, insulator}; `0` → background / other.\n",
    "\n",
    "> In real studies, silencers and insulators may be modeled separately; avoid collapsing if downstream task differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af5f61",
   "metadata": {},
   "source": [
    "## JSON Format Conversion\n",
    "\n",
    "JSON format is increasingly common for complex experimental data with nested metadata, multi-omics datasets, and high-throughput screening results. \n",
    "\n",
    "OmniGenBench has been ready to load JSON data directly, but understanding how to convert and structure your JSON files is crucial for effective dataset creation.\n",
    "\n",
    "### 📋 JSON Structure Example\n",
    "```json\n",
    "{\n",
    "  \"experiment_id\": \"exp_001\",\n",
    "  \"sequence\": \"ATCGATCG...\",\n",
    "  \"measurements\": {\n",
    "    \"expression\": 8.5,\n",
    "    \"binding_affinity\": 0.75\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"tissue\": \"liver\", \n",
    "    \"condition\": \"treatment\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 🎯 Conversion Strategy\n",
    "1. **Flatten nested structures**: Convert hierarchical data to flat columns\n",
    "2. **Handle arrays**: Process list-type experimental measurements\n",
    "3. **Type conversion**: Ensure proper data types for ML compatibility\n",
    "4. **Missing data**: Implement robust handling of incomplete records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52390ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 5: JSON → CSV Conversion\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample JSON data representing multi-omics experimental results\n",
    "experimental_data = [\n",
    "    {\n",
    "        \"experiment_id\": \"EXPR_001\",\n",
    "        \"sequence\": \"ATCGATCGATCGTACGAATTCCGGAAATTTCCCGGGAAATTTGGGCCCAAATTTAAAGGG\",\n",
    "        \"measurements\": {\n",
    "            \"rna_expression\": 8.5,\n",
    "            \"protein_abundance\": 12.3,\n",
    "            \"binding_affinity\": 0.82,\n",
    "            \"stability_score\": 7.1\n",
    "        },\n",
    "        \"experimental_conditions\": {\n",
    "            \"tissue_type\": \"liver\",\n",
    "            \"treatment\": \"control\",\n",
    "            \"timepoint\": \"24h\",\n",
    "            \"replicate\": 1\n",
    "        },\n",
    "        \"sequence_features\": {\n",
    "            \"gc_content\": 0.62,\n",
    "            \"length\": 60,\n",
    "            \"conservation_score\": 0.85,\n",
    "            \"secondary_structure_energy\": -15.2\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"sequencing_depth\": 1250,\n",
    "            \"mapping_quality\": 0.95,\n",
    "            \"technical_noise\": 0.08\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"experiment_id\": \"EXPR_002\", \n",
    "        \"sequence\": \"GCGCGCGCATATATATATGCGCGCGCATATATATATGCGCGCGCATATATATAT\",\n",
    "        \"measurements\": {\n",
    "            \"rna_expression\": 15.7,\n",
    "            \"protein_abundance\": 8.9,\n",
    "            \"binding_affinity\": 0.91,\n",
    "            \"stability_score\": 9.2\n",
    "        },\n",
    "        \"experimental_conditions\": {\n",
    "            \"tissue_type\": \"brain\",\n",
    "            \"treatment\": \"drug_A\",\n",
    "            \"timepoint\": \"48h\",\n",
    "            \"replicate\": 1\n",
    "        },\n",
    "        \"sequence_features\": {\n",
    "            \"gc_content\": 0.58,\n",
    "            \"length\": 54,\n",
    "            \"conservation_score\": 0.92,\n",
    "            \"secondary_structure_energy\": -22.1\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"sequencing_depth\": 2100,\n",
    "            \"mapping_quality\": 0.98,\n",
    "            \"technical_noise\": 0.04\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"experiment_id\": \"EXPR_003\",\n",
    "        \"sequence\": \"AAAATTTTCCCCGGGGAAAATTTTCCCCGGGGAAAATTTTCCCCGGGGAAAATTTT\",\n",
    "        \"measurements\": {\n",
    "            \"rna_expression\": 2.8,\n",
    "            \"protein_abundance\": 3.1,\n",
    "            \"binding_affinity\": 0.45,\n",
    "            \"stability_score\": 4.2\n",
    "        },\n",
    "        \"experimental_conditions\": {\n",
    "            \"tissue_type\": \"liver\", \n",
    "            \"treatment\": \"drug_B\",\n",
    "            \"timepoint\": \"72h\",\n",
    "            \"replicate\": 2\n",
    "        },\n",
    "        \"sequence_features\": {\n",
    "            \"gc_content\": 0.50,\n",
    "            \"length\": 56,\n",
    "            \"conservation_score\": 0.45,\n",
    "            \"secondary_structure_energy\": -8.3\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"sequencing_depth\": 890,\n",
    "            \"mapping_quality\": 0.89,\n",
    "            \"technical_noise\": 0.12\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"experiment_id\": \"EXPR_004\",\n",
    "        \"sequence\": \"TATAAAAAGCGCGCGCCCCCGGGGAAAATTTTTCCCGGGAAATTTAGCTAGCTAG\",\n",
    "        \"measurements\": {\n",
    "            \"rna_expression\": 11.2,\n",
    "            \"protein_abundance\": 14.6,\n",
    "            \"binding_affinity\": 0.78,\n",
    "            \"stability_score\": 8.9\n",
    "        },\n",
    "        \"experimental_conditions\": {\n",
    "            \"tissue_type\": \"heart\",\n",
    "            \"treatment\": \"control\",\n",
    "            \"timepoint\": \"12h\",\n",
    "            \"replicate\": 1\n",
    "        },\n",
    "        \"sequence_features\": {\n",
    "            \"gc_content\": 0.54,\n",
    "            \"length\": 55,\n",
    "            \"conservation_score\": 0.76,\n",
    "            \"secondary_structure_energy\": -18.7\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"sequencing_depth\": 1780,\n",
    "            \"mapping_quality\": 0.94,\n",
    "            \"technical_noise\": 0.06\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample JSON data\n",
    "json_path = 'conversion_examples/experimental_data.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(experimental_data, f, indent=2)\n",
    "print(f\"✅ Created sample JSON file: {json_path}\")\n",
    "\n",
    "def parse_json_to_csv(json_file, output_csv, expression_threshold=7.0):\n",
    "    \"\"\"\n",
    "    Convert JSON experimental data to CSV format with flattened structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_file : str\n",
    "        Path to input JSON file\n",
    "    output_csv : str\n",
    "        Path for output CSV file\n",
    "    expression_threshold : float\n",
    "        Threshold for binary classification of expression levels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Processed dataset with flattened experimental data\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    flattened_records = []\n",
    "    \n",
    "    for record in data:\n",
    "        # Start with core fields\n",
    "        flat_record = {\n",
    "            'sequence': record.get('sequence', ''),\n",
    "            'id': record.get('experiment_id', 'unknown_id')\n",
    "        }\n",
    "        \n",
    "        # Flatten measurements (main experimental outcomes)\n",
    "        if 'measurements' in record:\n",
    "            for key, value in record['measurements'].items():\n",
    "                flat_record[f'measurement_{key}'] = value\n",
    "        \n",
    "        # Flatten experimental conditions\n",
    "        if 'experimental_conditions' in record:\n",
    "            for key, value in record['experimental_conditions'].items():\n",
    "                flat_record[f'condition_{key}'] = value\n",
    "        \n",
    "        # Flatten sequence features\n",
    "        if 'sequence_features' in record:\n",
    "            for key, value in record['sequence_features'].items():\n",
    "                flat_record[f'feature_{key}'] = value\n",
    "        \n",
    "        # Flatten quality metrics\n",
    "        if 'quality_metrics' in record:\n",
    "            for key, value in record['quality_metrics'].items():\n",
    "                flat_record[f'quality_{key}'] = value\n",
    "        \n",
    "        # Create classification labels based on RNA expression\n",
    "        rna_expr = flat_record.get('measurement_rna_expression', 0)\n",
    "        flat_record['label'] = 1 if rna_expr >= expression_threshold else 0\n",
    "        \n",
    "        # Add data split assignment\n",
    "        flat_record['split'] = 'train'\n",
    "        \n",
    "        flattened_records.append(flat_record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(flattened_records)\n",
    "    \n",
    "    # Add derived features for ML compatibility\n",
    "    df['sequence_length'] = df['sequence'].apply(len)\n",
    "    \n",
    "    # Calculate composite scores\n",
    "    if 'measurement_rna_expression' in df.columns and 'measurement_protein_abundance' in df.columns:\n",
    "        df['expression_protein_ratio'] = df['measurement_rna_expression'] / (df['measurement_protein_abundance'] + 1e-6)\n",
    "    \n",
    "    if 'measurement_binding_affinity' in df.columns and 'measurement_stability_score' in df.columns:\n",
    "        df['functional_score'] = (df['measurement_binding_affinity'] * df['measurement_stability_score']) / 10\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "    \n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        if col not in ['sequence', 'id', 'split']:\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# Execute JSON conversion\n",
    "json_csv_path = 'conversion_examples/json_converted.csv'\n",
    "json_df = parse_json_to_csv(json_path, json_csv_path, expression_threshold=7.0)\n",
    "\n",
    "print(f\"✅ JSON conversion completed: {len(json_df)} experimental records processed\")\n",
    "print(f\"📊 Expression classification (threshold ≥ 7.0):\")\n",
    "print(f\"   - High expression: {json_df['label'].sum()}\")\n",
    "print(f\"   - Low expression: {len(json_df) - json_df['label'].sum()}\")\n",
    "\n",
    "# Show column structure after flattening\n",
    "print(f\"\\n📋 Flattened data structure ({len(json_df.columns)} columns):\")\n",
    "column_groups = {}\n",
    "for col in json_df.columns:\n",
    "    if col.startswith('measurement_'):\n",
    "        column_groups.setdefault('Measurements', []).append(col)\n",
    "    elif col.startswith('condition_'):\n",
    "        column_groups.setdefault('Conditions', []).append(col)\n",
    "    elif col.startswith('feature_'):\n",
    "        column_groups.setdefault('Features', []).append(col)\n",
    "    elif col.startswith('quality_'):\n",
    "        column_groups.setdefault('Quality', []).append(col)\n",
    "    else:\n",
    "        column_groups.setdefault('Core', []).append(col)\n",
    "\n",
    "for group, cols in column_groups.items():\n",
    "    print(f\"   - {group}: {len(cols)} columns\")\n",
    "\n",
    "print(f\"📁 Output saved: {json_csv_path}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\n📋 Sample flattened experimental data:\")\n",
    "display_cols = ['id', 'measurement_rna_expression', 'condition_tissue_type', 'feature_gc_content', 'label']\n",
    "print(json_df[display_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738673a8",
   "metadata": {},
   "source": [
    "#### 🔎 Label Meaning (JSON)\n",
    "`label = 1` → RNA expression ≥ configurable threshold (default 7.0); `0` otherwise.\n",
    "\n",
    "> Prefer percentile-based thresholds (e.g., top 30%) for heterogeneous datasets instead of fixed numeric cutoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44029f",
   "metadata": {},
   "source": [
    "## Multi-Source Data Integration and Quality Control\n",
    "\n",
    "Now that we have converted data from multiple formats, let's integrate them into a unified dataset and perform comprehensive quality control checks.\n",
    "\n",
    "### 🎯 Integration Objectives\n",
    "1. **Standardize Schema**: Align column names and data types across sources\n",
    "2. **Quality Filtering**: Remove low-quality or problematic sequences\n",
    "3. **Dataset Balancing**: Ensure appropriate class distribution\n",
    "4. **Split Strategy**: Implement biologically meaningful train/validation/test splits\n",
    "5. **Validation Pipeline**: Comprehensive data integrity checks\n",
    "\n",
    "### 🔍 Quality Control Metrics\n",
    "- **Sequence validity**: Check for non-standard nucleotides\n",
    "- **Length distribution**: Identify outliers and truncation needs\n",
    "- **Label balance**: Assess class distribution for ML compatibility\n",
    "- **Missing data**: Quantify and handle incomplete records\n",
    "- **Biological plausibility**: Validate GC content and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344469bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 6: Multi-Source Data Integration & Quality Control\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Added: optional harmonization switches\n",
    "ENABLE_LABEL_ORIGIN = True\n",
    "ADD_UNIFIED_FUNCTIONAL_LABEL = True  # demonstration: create 'functional_activity' meta label\n",
    "\n",
    "FUNCTIONAL_POSITIVE_SOURCES = {\n",
    "    'FASTA': lambda row: row['label'] == 1,  # promoter-like\n",
    "    'GFF': lambda row: row['label'] == 1,    # protein-coding\n",
    "    'BED': lambda row: row['label'] == 1,    # active regulatory\n",
    "    'JSON': lambda row: row['label'] == 1    # high expression\n",
    "}\n",
    "\n",
    "def validate_dna_sequence(sequence):\n",
    "    \"\"\"Validate DNA sequence contains only standard nucleotides.\"\"\"\n",
    "    valid_bases = set('ATCGN')\n",
    "    return all(base.upper() in valid_bases for base in sequence)\n",
    "\n",
    "# ...existing code (complexity, integrate_multi_source_data unchanged up to combination)...\n",
    "\n",
    "def calculate_sequence_complexity(sequence):\n",
    "    if len(sequence) == 0:\n",
    "        return 0\n",
    "    base_counts = {}\n",
    "    for base in sequence.upper():\n",
    "        base_counts[base] = base_counts.get(base, 0) + 1\n",
    "    length = len(sequence)\n",
    "    entropy = 0\n",
    "    for count in base_counts.values():\n",
    "        if count > 0:\n",
    "            prob = count / length\n",
    "            entropy -= prob * np.log2(prob)\n",
    "    return entropy\n",
    "\n",
    "def integrate_multi_source_data():\n",
    "    print(\"📊 Loading converted datasets...\")\n",
    "    core_columns = ['sequence', 'label', 'id', 'split']\n",
    "    integrated_data = []\n",
    "    datasets_info = [\n",
    "        ('conversion_examples/fasta_converted.csv', 'FASTA'),\n",
    "        ('conversion_examples/gff_converted.csv', 'GFF'),\n",
    "        ('conversion_examples/bed_converted.csv', 'BED'),\n",
    "        ('conversion_examples/json_converted.csv', 'JSON')\n",
    "    ]\n",
    "    for file_path, source_type in datasets_info:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                missing_cols = [c for c in core_columns if c not in df.columns]\n",
    "                if missing_cols:\n",
    "                    print(f\"⚠️ {source_type}: Missing {missing_cols}, skipped\")\n",
    "                    continue\n",
    "                standardized_df = df[core_columns].copy()\n",
    "                standardized_df['source'] = source_type\n",
    "                if 'gc_content' not in df.columns:\n",
    "                    standardized_df['gc_content'] = df['sequence'].apply(lambda s: (s.count('G') + s.count('C'))/len(s) if len(s)>0 else 0)\n",
    "                else:\n",
    "                    standardized_df['gc_content'] = df['gc_content']\n",
    "                integrated_data.append(standardized_df)\n",
    "                print(f\"   ✅ {source_type}: {len(standardized_df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ {source_type}: load error {e}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ {source_type}: file not found\")\n",
    "    if not integrated_data:\n",
    "        raise ValueError(\"No datasets loaded\")\n",
    "    combined_df = pd.concat(integrated_data, ignore_index=True)\n",
    "    if ENABLE_LABEL_ORIGIN:\n",
    "        combined_df['label_origin'] = combined_df['source']\n",
    "    if ADD_UNIFIED_FUNCTIONAL_LABEL:\n",
    "        # Meta functional activity: any positive biological evidence across sources\n",
    "        combined_df['functional_activity'] = combined_df.apply(lambda r: int(FUNCTIONAL_POSITIVE_SOURCES.get(r['source'], lambda x: False)(r)), axis=1)\n",
    "    print(f\"\\n📋 Initial integration: {len(combined_df)} records\")\n",
    "    return combined_df\n",
    "\n",
    "# ...rest of existing QC, splitting, saving code remains identical...\n",
    "\n",
    "def perform_quality_control(df, min_length=10, max_length=1000, min_complexity=0.5, gc_range=(0.1,0.9)):\n",
    "    print(\"🔍 Performing Quality Control Checks...\")\n",
    "    initial_count = len(df)\n",
    "    qc_report = {'initial_count': initial_count}\n",
    "    valid_mask = df['sequence'].apply(validate_dna_sequence)\n",
    "    df_valid = df[valid_mask].copy(); qc_report['invalid_sequences']= initial_count-len(df_valid)\n",
    "    df_valid['seq_length'] = df_valid['sequence'].apply(len)\n",
    "    length_mask = (df_valid['seq_length']>=min_length)&(df_valid['seq_length']<=max_length)\n",
    "    df_length = df_valid[length_mask].copy(); qc_report['length_filtered']= len(df_valid)-len(df_length)\n",
    "    df_length['complexity'] = df_length['sequence'].apply(calculate_sequence_complexity)\n",
    "    comp_mask = df_length['complexity']>=min_complexity\n",
    "    df_comp = df_length[comp_mask].copy(); qc_report['low_complexity']= len(df_length)-len(df_comp)\n",
    "    gc_mask = (df_comp['gc_content']>=gc_range[0])&(df_comp['gc_content']<=gc_range[1])\n",
    "    df_gc = df_comp[gc_mask].copy(); qc_report['gc_outliers']= len(df_comp)-len(df_gc)\n",
    "    before_dup = len(df_gc)\n",
    "    df_unique = df_gc.drop_duplicates(subset=['sequence','source']).copy(); qc_report['duplicates']= before_dup-len(df_unique)\n",
    "    qc_report['final_count']= len(df_unique)\n",
    "    qc_report['retention_rate']= qc_report['final_count']/qc_report['initial_count']\n",
    "    print(f\"   Retained {qc_report['final_count']} / {initial_count} ({qc_report['retention_rate']:.1%})\")\n",
    "    return df_unique, qc_report\n",
    "\n",
    "def create_balanced_splits(df, test_size=0.15, valid_size=0.15, stratify_column='label', random_state=42):\n",
    "    print(f\"📊 Creating balanced data splits (stratified by {stratify_column})...\")\n",
    "    np.random.seed(random_state)\n",
    "    classes = df[stratify_column].unique()\n",
    "    split_indices = {'train':[], 'valid':[], 'test':[]}\n",
    "    for c in classes:\n",
    "        idx = df[df[stratify_column]==c].index.tolist()\n",
    "        np.random.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        n_test = int(n*test_size); n_valid = int(n*valid_size); n_train = n - n_test - n_valid\n",
    "        split_indices['test'].extend(idx[:n_test])\n",
    "        split_indices['valid'].extend(idx[n_test:n_test+n_valid])\n",
    "        split_indices['train'].extend(idx[n_test+n_valid:])\n",
    "        print(f\"   Class {c}: {n_train} train / {n_valid} valid / {n_test} test\")\n",
    "    df_split = df.copy()\n",
    "    for name, inds in split_indices.items():\n",
    "        df_split.loc[inds,'split']= name\n",
    "    dist = df_split['split'].value_counts()\n",
    "    for part in ['train','valid','test']:\n",
    "        cnt = dist.get(part,0); print(f\"   {part}: {cnt} samples\")\n",
    "    return df_split\n",
    "\n",
    "try:\n",
    "    integrated_df = integrate_multi_source_data()\n",
    "    qc_df, qc_report = perform_quality_control(integrated_df, min_length=20, max_length=500, min_complexity=0.8, gc_range=(0.2,0.8))\n",
    "    # Choose which label to stratify: original 'label' OR new 'functional_activity'\n",
    "    stratify_col = 'functional_activity' if ADD_UNIFIED_FUNCTIONAL_LABEL else 'label'\n",
    "    final_df = create_balanced_splits(qc_df, test_size=0.15, valid_size=0.15, stratify_column=stratify_col)\n",
    "    output_path = 'conversion_examples/integrated_multi_source_dataset.csv'\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(\"\\n🎉 Integration Complete!\")\n",
    "    print(f\"📁 Integrated dataset saved: {output_path}\")\n",
    "    print(f\"📊 Final dataset: {len(final_df):,} sequences\")\n",
    "    if 'functional_activity' in final_df.columns:\n",
    "        print(f\"🧪 Functional Activity Distribution: {final_df['functional_activity'].value_counts().to_dict()}\")\n",
    "    print(f\"🏷️ Label distribution: {final_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"📋 Source distribution: {final_df['source'].value_counts().to_dict()}\")\n",
    "    print(\"\\n📋 Sample integrated data:\")\n",
    "    sample_cols = [c for c in ['id','source','seq_length','gc_content','complexity','label','functional_activity','split'] if c in final_df.columns]\n",
    "    print(final_df[sample_cols].head())\n",
    "except Exception as e:\n",
    "    print(f\"❌ Integration failed: {e}\")\n",
    "    print(\"   Ensure previous steps completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1474c43",
   "metadata": {},
   "source": [
    "## Best Practices and Professional Guidelines\n",
    "\n",
    "This section consolidates the essential best practices for biological data conversion and provides professional guidelines for research-grade dataset creation.\n",
    "\n",
    "### ✅ Data Conversion Success Factors\n",
    "\n",
    "| Factor | Importance | Implementation Guidelines |\n",
    "|--------|------------|--------------------------|\n",
    "| **🔑 Standardized Schema** | ⭐⭐⭐⭐⭐ | Always use `sequence`, `label`, `id`, `split` as core columns |\n",
    "| **📊 Quality Control** | ⭐⭐⭐⭐⭐ | Validate sequence format, length distribution, missing values |\n",
    "| **🔄 Data Validation** | ⭐⭐⭐⭐ | Check sequence validity (ATCG only), label ranges, biological plausibility |\n",
    "| **📈 Class Balance** | ⭐⭐⭐⭐ | Ensure reasonable positive/negative ratios, avoid severe imbalance |\n",
    "| **🗂️ Metadata Preservation** | ⭐⭐⭐ | Retain biological annotations for downstream analysis and interpretation |\n",
    "\n",
    "### ⚠️ Common Pitfalls and Solutions\n",
    "\n",
    "#### 🧬 Sequence Format Issues\n",
    "- **❌ Problem**: Non-standard nucleotides (N, R, Y, etc.) causing tokenization errors\n",
    "- **✅ Solution**: Filter ambiguous bases or use specialized tokenizers, document handling approach\n",
    "\n",
    "#### 📏 Length Inconsistency  \n",
    "- **❌ Problem**: Extreme length variation causing training instability\n",
    "- **✅ Solution**: Implement length normalization, truncation, or dynamic padding strategies\n",
    "\n",
    "#### 🏷️ Label Encoding Errors\n",
    "- **❌ Problem**: String labels (\"positive\"/\"negative\") not converted to numeric format\n",
    "- **✅ Solution**: Explicit label mapping with clear documentation (0/1 for binary, 0/1/2... for multi-class)\n",
    "\n",
    "#### 📊 Data Leakage Risks\n",
    "- **❌ Problem**: Related sequences (same gene, homologs) split across train/test sets\n",
    "- **✅ Solution**: Gene/protein-level splitting rather than random sequence splitting\n",
    "\n",
    "#### 🔍 Insufficient Quality Control\n",
    "- **❌ Problem**: Low-quality, duplicate, or biologically implausible sequences\n",
    "- **✅ Solution**: Multi-step QC pipeline with sequence validation, complexity analysis, duplicate removal\n",
    "\n",
    "### 🚀 Advanced Dataset Enhancement Techniques\n",
    "\n",
    "#### 📊 Biological Feature Engineering\n",
    "```python\n",
    "# Example: Advanced sequence features for ML enhancement\n",
    "def calculate_biological_features(sequence):\n",
    "    \"\"\"Calculate comprehensive biological features.\"\"\"\n",
    "    seq = sequence.upper()\n",
    "    length = len(seq)\n",
    "    \n",
    "    return {\n",
    "        'gc_content': (seq.count('G') + seq.count('C')) / length,\n",
    "        'purine_content': (seq.count('A') + seq.count('G')) / length,\n",
    "        'pyrimidine_content': (seq.count('C') + seq.count('T')) / length,\n",
    "        'dinucleotide_diversity': len(set([seq[i:i+2] for i in range(length-1)])),\n",
    "        'cpg_sites': seq.count('CG'),\n",
    "        'repeat_content': max([seq.count(base * 3) for base in 'ATCG']) / length\n",
    "    }\n",
    "```\n",
    "\n",
    "#### 🔄 Data Augmentation Strategies\n",
    "- **Reverse Complement**: Generate biologically valid sequence variants\n",
    "- **Sliding Windows**: Create overlapping subsequences for longer genomic regions  \n",
    "- **Homolog Integration**: Include sequences from related species for robustness\n",
    "- **Synthetic Generation**: Use generative models for balanced dataset creation\n",
    "\n",
    "### 📖 Recommended Professional Tools\n",
    "\n",
    "| Tool Category | Recommended Tools | Use Case |\n",
    "|---------------|-------------------|----------|\n",
    "| **🧬 Sequence Processing** | BioPython, pysam, BLAST+ | FASTA/FASTQ parsing, sequence alignment |\n",
    "| **🔧 Genomic Annotations** | pyranges, pybedtools, HTSeq | BED/GFF processing, interval operations |\n",
    "| **📊 Data Manipulation** | pandas, polars, dask | Large-scale CSV processing, data integration |\n",
    "| **🎯 ML Pipeline** | scikit-learn, imbalanced-learn | Data splitting, preprocessing, validation |\n",
    "| **🧪 Quality Control** | FastQC, MultiQC, custom scripts | Sequence quality assessment, batch processing |\n",
    "\n",
    "### 🎓 Research Publication Guidelines\n",
    "\n",
    "When publishing research using converted datasets:\n",
    "\n",
    "1. **📋 Methods Section**: Document exact conversion parameters, QC thresholds, software versions\n",
    "2. **📊 Data Availability**: Provide processed datasets and conversion scripts for reproducibility  \n",
    "3. **🔍 Quality Metrics**: Report retention rates, class distributions, validation results\n",
    "4. **⚖️ Bias Assessment**: Analyze potential biases from source data integration\n",
    "5. **🧪 Validation**: Include biological validation or benchmark comparisons\n",
    "\n",
    "### 💡 Key Takeaways for Success\n",
    "\n",
    "1. **🎯 Plan Early**: Design conversion strategy before data collection\n",
    "2. **📋 Document Everything**: Maintain detailed logs of processing steps and decisions\n",
    "3. **🔄 Validate Continuously**: Implement checkpoints throughout the conversion pipeline\n",
    "4. **🧬 Think Biologically**: Ensure computational decisions align with biological knowledge\n",
    "5. **📈 Iterate and Improve**: Refine conversion based on downstream model performance\n",
    "\n",
    "> **🎉 Congratulations!** You now have the expertise to convert any biological data format into research-grade, ML-ready datasets compatible with OmniGenBench and other genomics frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8779",
   "metadata": {},
   "source": [
    "### 7.6 (Optional Preview) Minimal VCF Parsing Stub\n",
    "Below is an (inactive by default) illustrative code stub showing how a lightweight VCF parsing function could be structured. It is intentionally not executed to avoid implying a full reference genome dependency. This will be formalized in the future extension.\n",
    "\n",
    "```python\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "VCF_HEADER_PREFIX = '#'\n",
    "\n",
    "VCF_INFO_AF_PATTERN = re.compile(r'(^|;)AF=([^;]+)')\n",
    "\n",
    "def parse_vcf_minimal(vcf_path: str, max_records: int = 1000) -> pd.DataFrame:\n",
    "    rows: List[Dict] = []\n",
    "    with open(vcf_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(VCF_HEADER_PREFIX):\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 8:\n",
    "                continue\n",
    "            chrom, pos, vid, ref, alts, qual, flt, info = parts[:8]\n",
    "            pos_int = int(pos)\n",
    "            alt_list = alts.split(',')\n",
    "            # Extract allele frequency (if present)\n",
    "            m = VCF_INFO_AF_PATTERN.search(info)\n",
    "            af_values = []\n",
    "            if m:\n",
    "                try:\n",
    "                    af_values = [float(x) for x in m.group(2).split(',') if x.strip()]\n",
    "                except ValueError:\n",
    "                    af_values = []\n",
    "            for i, alt in enumerate(alt_list):\n",
    "                af = af_values[i] if i < len(af_values) else None\n",
    "                variant_id = f\"{chrom}:{pos}:{ref}:{alt}\"\n",
    "                # Placeholder: no sequence context extraction here\n",
    "                rows.append({\n",
    "                    'variant_id': variant_id,\n",
    "                    'chrom': chrom,\n",
    "                    'position': pos_int,\n",
    "                    'ref_base': ref,\n",
    "                    'alt_base': alt,\n",
    "                    'allele_frequency': af,\n",
    "                    # Heuristic binary label example (rare = 1):\n",
    "                    'label': 1 if (af is not None and af < 0.01) else 0,\n",
    "                    'source': 'VCF'\n",
    "                })\n",
    "                if len(rows) >= max_records:\n",
    "                    break\n",
    "            if len(rows) >= max_records:\n",
    "                break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example (commented out):\n",
    "# vcf_df = parse_vcf_minimal('path/to/example.vcf')\n",
    "# vcf_df.to_csv('conversion_examples/vcf_converted.csv', index=False)\n",
    "# print(vcf_df.head())\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- Full integration would add sequence window extraction around POS using a reference genome FASTA.\n",
    "- Additional INFO fields (e.g., CADD, SIFT, ClinVar pathogenicity) can enrich multi-task labeling.\n",
    "- Multi-allelic sites are expanded one row per ALT allele.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ebe8b",
   "metadata": {},
   "source": [
    "### 7.5 Upcoming Extension: VCF (Variant Call Format) Integration\n",
    "(Planned) A future subsection will cover parsing VCF files to derive variant-centric features (REF/ALT allele context, functional annotations from INFO, derived k-mer windows) and integrating them as additional rows or auxiliary tables. This preserves current tutorial scope while signaling forthcoming capability.\n",
    "\n",
    "Planned minimal columns will include:\n",
    "- variant_id (chrom:pos:ref:alt)\n",
    "- sequence (local ±k bp window around the variant)\n",
    "- ref_base / alt_base\n",
    "- label (e.g., pathogenic vs benign heuristic or allele frequency derived threshold)\n",
    "- allele_frequency (parsed from INFO (e.g., AF=) or external annotation)\n",
    "- source = 'VCF'\n",
    "\n",
    "Harmonization Strategy (to be shown):\n",
    "1. Parse core fields CHROM, POS, ID, REF, ALT, INFO.\n",
    "2. Extract AF (allele frequency) or set placeholder if absent.\n",
    "3. Construct local reference window from a provided reference genome (requires FASTA index; not included in lightweight example).\n",
    "4. Optional label derivation: AF < 0.01 → rare (potential functional), else common.\n",
    "5. Integrate with existing pipeline via the same normalization and QC steps.\n",
    "\n",
    "Rationale: Including VCF expands the tutorial beyond regulatory and expression-centric labeling into variant effect modeling, bridging toward variant effect prediction tasks already present elsewhere in OmniGenomeBench.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daed10",
   "metadata": {},
   "source": [
    "### 7.4 TSV Export (Optional)\n",
    "If a tab-delimited copy is required for downstream bioinformatics tools (which often default to TSV):\n",
    "\n",
    "```python\n",
    "# Export the final integrated dataset as TSV\n",
    "final_df.to_csv('conversion_examples/integrated_multi_source_dataset.tsv', sep='\\t', index=False)\n",
    "print('TSV written to conversion_examples/integrated_multi_source_dataset.tsv')\n",
    "```\n",
    "\n",
    "Why TSV?\n",
    "- Reduces ambiguity when free‐text metadata fields may contain commas.\n",
    "- Many command-line genomics utilities (awk, cut, bedtools wrappers) expect or more easily parse tab separation.\n",
    "- Stay consistent with UTF-8 encoding; avoid Excel re-saving which may alter line endings or truncate long sequences.\n",
    "\n",
    "Verification Tip:\n",
    "```python\n",
    "import pandas as pd\n",
    "check_tsv = pd.read_csv('conversion_examples/integrated_multi_source_dataset.tsv', sep='\\t')\n",
    "print(check_tsv.head(2))\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
