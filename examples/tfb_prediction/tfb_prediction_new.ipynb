{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning OmniGenome for Transcription Factor Binding Prediction\n",
    "\n",
    "This tutorial provides a comprehensive guide to fine-tuning a pre-trained genomic foundation model, [OmniGenome-52M](https://huggingface.co/yangheng/OmniGenome-52M), for a Transcription Factor Binding (TFB) prediction task. We will use the [DeepSEA](https://www.nature.com/articles/nmeth.3547) dataset and the OmniGenBench library to build a multi-label classifier that predicts the binding sites of 919 different transcription factors from a raw DNA sequence.\n",
    "\n",
    "## Task Overview\n",
    "The goal is to perform multi-label binary classification. Given a DNA sequence, the model must predict for each of the 919 possible chromatin features whether the sequence is a binding site (label 1) or not (label 0).\n",
    "\n",
    "##  Dataset Description\n",
    "The data for this tutorial is a preprocessed version of the DeepSEA dataset, which was originally designed for studying the effects of non-coding genetic variants. It consists of:\n",
    "\n",
    "- Inputs: DNA sequences of 1000 base pairs (bp).\n",
    "\n",
    "- Labels: 919 binary labels corresponding to various chromatin features, including transcription factor binding, DNase I sensitivity, histone marks, etc.\n",
    "\n",
    "We will use a version of this dataset hosted on Hugging Face at [yangheng/tfb_prediction](https://huggingface.co/datasets/yangheng/tfb_prediction).\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "**The total runtime depends on your hardware.**\n",
    "\n",
    "- **Full Run**: On a single NVIDIA RTX 4090 GPU, training with the default settings (MAX_EXAMPLES=100000, EPOCHS=10) takes approximately 1-2 hours.\n",
    "\n",
    "- **Quick Test**: For a quick test run with MAX_EXAMPLES=1000, it should take about 5-10 minutes.\n",
    "\n",
    "## Notebook Structure\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. **Setup & Installation**: Prepares the environment by installing necessary libraries.\n",
    "\n",
    "2. **Import Libraries**: Loads all required Python packages.\n",
    "\n",
    "3. **Configuration**: Centralizes all hyperparameters and paths for easy modification.\n",
    "\n",
    "4. **Model Definition**: Implements a custom PyTorch model that integrates the OmniGenome backbone with a classification head.\n",
    "\n",
    "5. **Data Loading and Preprocessing**: Defines the dataset class and logic for loading and tokenizing the DNA sequences.\n",
    "\n",
    "6. **Initialization**: Instantiates the tokenizer, model, and datasets, preparing them for training.\n",
    "\n",
    "7. **Training the Model**: Fine-tunes the model using the efficient AccelerateTrainer.\n",
    "\n",
    "8. **Evaluation**: Assesses the final model's performance on the test set using the area under the ROC curve (AUC-ROC) metric.\n",
    "\n",
    "9. **Inference Example**: Shows how to use the trained model for predictions on a new DNA sequence.\n",
    "\n",
    "10. **Conclusion**: Summarizes the tutorial and suggests next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Installation\n",
    "\n",
    "Before we begin, you need to prepare your environment. This involves installing Git LFS to download the large dataset files from Hugging Face and then installing the required Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Install and Initialize Git LFS\n",
    "If you don't have Git LFS, you must install it to download the data. Run the appropriate command for your system in a **terminal**, not in this notebook.\n",
    "\n",
    "- **On Debian/Ubuntu**: `sudo apt-get install git-lfs`\n",
    "\n",
    "- **On macOS (with Homebrew)**:`brew install git-lfs`\n",
    "\n",
    "- **On Windows**: Download and run the installer from the [official Git LFS website](https://git-lfs.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Install Python Libraries\n",
    "\n",
    "Now, run the following cell to install the necessary Python packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install the necessary packages\n",
    "# !pip install torch numpy transformers omnigenbench autocuda findfile accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Libraries\n",
    "Next, we import the necessary libraries for data manipulation, model building, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import autocuda\n",
    "import findfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from omnigenbench import (\n",
    "    AccelerateTrainer,\n",
    "    ClassificationMetric,\n",
    "    OmniDataset,\n",
    "    OmniLoraModel,\n",
    "    OmniModel,\n",
    "    OmniPooling,\n",
    ")\n",
    "from transformers import AutoModel, AutoTokenizer, BatchEncoding\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Configuration\n",
    "In this section, we will set up all the necessary parameters for our experiment. This includes downloading the data, selecting a model, defining training hyperparameters, and configuring the hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Download and Verify the Dataset\n",
    "First, we define the location of our dataset. The following code will download the `tfb_prediction` dataset from Hugging Face using `git`, extract it if necessary, and then verify that the required data files (`.npy`) are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already found in tfb_prediction_dataset.\n",
      "All data files found successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Data Source and Local Directory ---\n",
    "DATASET_URL = \"https://huggingface.co/datasets/yangheng/tfb_prediction\"\n",
    "LOCAL_DIR = \"tfb_prediction_dataset\"\n",
    "\n",
    "# --- Download and Extract ---\n",
    "if not os.path.isdir(LOCAL_DIR):\n",
    "    print(f\"Cloning dataset from {DATASET_URL} into {LOCAL_DIR}...\")\n",
    "    os.system(f\"git clone {DATASET_URL} {LOCAL_DIR}\")\n",
    "    \n",
    "    zip_path = os.path.join(LOCAL_DIR, \"tfb_dataset.zip\")\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Extracting {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(LOCAL_DIR)\n",
    "        os.remove(zip_path) # Clean up the zip file\n",
    "else:\n",
    "    print(f\"Dataset already found in {LOCAL_DIR}.\")\n",
    "\n",
    "# --- Define and Verify File Paths ---\n",
    "TRAIN_FILE = os.path.join(LOCAL_DIR, \"train_tfb.npy\")\n",
    "TEST_FILE = os.path.join(LOCAL_DIR, \"test_tfb.npy\")\n",
    "VALID_FILE = os.path.join(LOCAL_DIR, \"valid_tfb.npy\")\n",
    "\n",
    "# Verify that the files exist to prevent errors later\n",
    "if not os.path.exists(TRAIN_FILE):\n",
    "    raise FileNotFoundError(f\"Training file not found at {TRAIN_FILE}. Please check the download step.\")\n",
    "if not os.path.exists(TEST_FILE):\n",
    "    raise FileNotFoundError(f\"Test file not found at {TEST_FILE}.\")\n",
    "if not os.path.exists(VALID_FILE):\n",
    "    print(f\"Warning: Validation file not found at {VALID_FILE}. Skipping validation.\")\n",
    "else:\n",
    "    print(\"All data files found successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Select the Foundation Model\n",
    "Here, you can choose which pre-trained genomic foundation model to fine-tune. We have provided a list of compatible models. To switch models, simply change the index for `AVAILABLE_MODELS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: yangheng/OmniGenome-52M\n"
     ]
    }
   ],
   "source": [
    "# --- Model Configuration ---\n",
    "AVAILABLE_MODELS = [\n",
    "    'yangheng/OmniGenome-52M',      # Default model for this tutorial\n",
    "    'yangheng/OmniGenome-186M',     # A larger OmniGenome model\n",
    "    'DNABERT-2-117M',               # DNABERT-2 by ailab\n",
    "    'InstaDeepAI/nucleotide-transformer-500m-human-ref', # Nucleotide Transformer\n",
    "    # 'DNABERT-2-117M',  # You can add more models here as needed,\n",
    "    # 'LongSafari/hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-500m-human-ref',\n",
    "    # 'multimolecule/rnafm', # RNA-specific models\n",
    "    # 'multimolecule/rnamsm',\n",
    "    # 'multimolecule/rnabert',\n",
    "    # 'SpliceBERT-510nt', # Splice-specific model\n",
    "]\n",
    "\n",
    "# Select the model by its index in the list (0 = OmniGenome-52M)\n",
    "MODEL_NAME_OR_PATH = AVAILABLE_MODELS[0]\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME_OR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Set Training Hyperparameters\n",
    "\n",
    "This step defines the key parameters that control the training process, such as learning rate, batch size, and the number of epochs. You can also set a limit on the number of training examples for a quicker test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 100000 examples for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 10                  # Number of training epochs\n",
    "LEARNING_RATE = 2e-5         # Optimizer learning rate\n",
    "WEIGHT_DECAY = 1e-5          # Weight decay for regularization\n",
    "BATCH_SIZE = 16              # Number of samples per batch\n",
    "PATIENCE = 3                 # Number of epochs with no improvement to wait before stopping\n",
    "MAX_LENGTH = 200             # Sequence length to process (the central 200bp of the 1000bp sequence)\n",
    "SEED = 42                    # Random seed for reproducibility\n",
    "MAX_EXAMPLES = 100000        # Max examples for training/testing. Use a small number (e.g., 1000) for a quick test run.\n",
    "GRADIENT_ACCUMULATION_STEPS = 1 # Accumulates gradients over multiple steps for a larger effective batch size\n",
    "\n",
    "print(f\"Training with {MAX_EXAMPLES} examples for {EPOCHS} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: Configure Hardware Device\n",
    "Finally, we'll set the device for training. The `autocuda` library will automatically select an available NVIDIA GPU if possible, otherwise it will fall back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup ---\n",
    "DEVICE = autocuda.auto_cuda()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Definition\n",
    "Here, we define our custom model, `OmniModelForMultiLabelClassification`. This class inherits from `OmniModel` and wraps a pre-trained Transformer (like OmniGenome) with a classification head suitable for our 919-label task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniModelForMultiLabelClassification defined.\n"
     ]
    }
   ],
   "source": [
    "class OmniModelForMultiLabelClassification(OmniModel):\n",
    "    \"\"\"\n",
    "    Multi-label sequence classification model based on OmniGenome-52M.\n",
    "\n",
    "    This model replaces the original DeepSEA CNN architecture with a pretrained\n",
    "    Transformer encoder from the OmniGenome family. Optionally, convolutional\n",
    "    layers can be stacked on top of the Transformer outputs for additional\n",
    "    feature extraction.\n",
    "\n",
    "    Parameters:\n",
    "        config_or_model (PretrainedConfig or nn.Module):\n",
    "            Configuration or instance of the pretrained Transformer model,\n",
    "            typically obtained via AutoModel.from_pretrained().\n",
    "        tokenizer (PreTrainedTokenizer):\n",
    "            Tokenizer compatible with the Transformer encoder, used to convert\n",
    "            DNA sequences into model inputs.\n",
    "        threshold (float, optional):\n",
    "            Probability threshold for binary decisions in predict(), defaults to 0.5.\n",
    "        use_conv (bool, optional):\n",
    "            If True, apply convolutional layers after the Transformer encoder\n",
    "            for enhanced feature extraction, defaults to False.\n",
    "        *args, **kwargs: Additional arguments passed to the base OmniModel class.\n",
    "\n",
    "    Attributes:\n",
    "        threshold (float):\n",
    "            Probability cutoff for generating binary predictions.\n",
    "        deepsea_classifier (nn.Sequential):\n",
    "            Classification head consisting of a Tanh activation followed by\n",
    "            a linear layer mapping to the number of labels.\n",
    "        loss_fn (nn.BCEWithLogitsLoss):\n",
    "            Binary cross-entropy loss with logits, using pos_weight to balance\n",
    "            positive and negative samples.\n",
    "        pooler (OmniPooling):\n",
    "            Utility for pooling token-level outputs into a sequence-level vector.\n",
    "        sigmoid (nn.Sigmoid):\n",
    "            Activation used to convert logits to probabilities during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        self.threshold = kwargs.pop(\"threshold\", 0.5)\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = \"OmniModelForMultiLabelClassification\"\n",
    "\n",
    "        # Classification head based directly on Transformer outputs\n",
    "        conv_output_dim = self.config.hidden_size\n",
    "        self.deepsea_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(conv_output_dim, self.config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Use pos_weight to address class imbalance in BCEWithLogitsLoss\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0]))\n",
    "        self.pooler = OmniPooling(self.config)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model: encode, pool, classify, and optionally compute loss.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict):\n",
    "                Must contain 'input_ids', 'attention_mask', etc., and optionally 'labels'.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'logits': Tensor of shape (batch_size, num_labels), raw scores before\n",
    "                          sigmoid activation.\n",
    "                'last_hidden_state': Tensor of shape (batch_size, seq_len, hidden_size),\n",
    "                                     the last layer hidden states from the Transformer.\n",
    "                'loss' (optional): Computed BCEWithLogitsLoss if 'labels' provided.\n",
    "            }\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If all provided labels are zero or if logits and labels shapes mismatch.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "\n",
    "        # Encode inputs\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "\n",
    "        # Pooling strategy\n",
    "        if self.pooler._is_causal_lm():\n",
    "            pad_token_id = getattr(self.config, \"pad_token_id\", -100)\n",
    "            sequence_lengths = inputs[\"input_ids\"].ne(pad_token_id).sum(dim=1) - 1\n",
    "            pooled_output = last_hidden_state[\n",
    "                torch.arange(inputs[\"input_ids\"].size(0), device=last_hidden_state.device),\n",
    "                sequence_lengths,\n",
    "            ]\n",
    "        else:\n",
    "            pooled_output = self.pooler(inputs, last_hidden_state)\n",
    "\n",
    "        logits = self.deepsea_classifier(pooled_output)\n",
    "        outputs = {\"logits\": logits, \"last_hidden_state\": last_hidden_state}\n",
    "\n",
    "        if labels is not None:\n",
    "            if torch.sum(labels[labels != -100]) == 0:\n",
    "                raise ValueError(\"Labels cannot be all zeros.\")\n",
    "            labels = labels[labels != -100]\n",
    "            loss = self.loss_fn(logits.view(-1), labels.view(-1).to(torch.float32))\n",
    "            outputs[\"loss\"] = loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform inference on raw sequences or tokenized inputs, returning probabilities and predictions.\n",
    "\n",
    "        Args:\n",
    "            sequence_or_inputs (str, BatchEncoding, or dict):\n",
    "                Raw DNA string or pre-tokenized inputs.\n",
    "            padding (str, optional): Padding strategy for tokenizer, defaults to 'max_length'.\n",
    "            max_length (int, optional): Maximum sequence length, defaults to 1024.\n",
    "            **kwargs: Additional tokenizer arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'predictions': Tensor of binary labels,\n",
    "                'probabilities': Tensor of positive class probabilities,\n",
    "                'logits': Tensor of raw scores,\n",
    "                'last_hidden_state': Transformer outputs.\n",
    "            }\n",
    "        \"\"\"\n",
    "        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(\n",
    "                sequence_or_inputs, dict\n",
    "        ):\n",
    "            inputs = self.tokenizer(\n",
    "                sequence_or_inputs,\n",
    "                padding=kwargs.pop(\"padding\", \"max_length\"),\n",
    "                max_length=kwargs.pop(\"max_length\", 1024),\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            inputs = sequence_or_inputs\n",
    "        inputs = inputs.to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(**inputs)\n",
    "        logits = outputs[\"logits\"]\n",
    "        last_hidden_state = outputs[\"last_hidden_state\"]\n",
    "\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        predictions = (probabilities >= self.threshold).to(torch.int)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": probabilities,\n",
    "            \"logits\": logits,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Compute BCEWithLogitsLoss for multi-label classification.\n",
    "\n",
    "        Args:\n",
    "            logits (Tensor): Raw output scores, shape (batch_size, num_labels).\n",
    "            labels (Tensor): Ground-truth labels as floats, same shape as logits.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If logits and labels shapes do not match.\n",
    "        \"\"\"\n",
    "        valid_labels = labels.to(torch.float32)\n",
    "        if logits.shape != valid_labels.shape:\n",
    "            raise ValueError(f\"Shape mismatch between logits {logits.shape} and labels {valid_labels.shape}\")\n",
    "        return self.loss_fn(logits, valid_labels)\n",
    "\n",
    "\n",
    "print(\"OmniModelForMultiLabelClassification defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Loading and Preprocessing\n",
    "This section defines our `DeepSEADataset` class, which handles loading the `.npy` files and preparing the DNA sequences for the model.\n",
    "\n",
    "**Important Note on Tokenization**: Most genomic foundation models (like DNABERT and OmniGenome) are trained on sequences where each nucleotide is separated by a space (e.g., `\"A T C G\"` instead of `\"ATCG\"`). Our dataset class must perform this conversion before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSEADataset(OmniDataset):\n",
    "    \"\"\"\n",
    "    Dataset designed for the DeepSEA task, handling the conversion from DNA sequences to tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metadata[key] = value\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        \"\"\"\n",
    "        Prepare input data for DeepSEA\n",
    "\n",
    "        Expected instance format:\n",
    "        {\n",
    "            'sequence': DNA sequence string (e.g., \"ATCGATCG...\")\n",
    "            'labels': binary labels as numpy array of shape (919,)\n",
    "        }\n",
    "        \"\"\"\n",
    "        labels = None\n",
    "        if isinstance(instance, str):\n",
    "            sequence = instance\n",
    "        elif isinstance(instance, dict):\n",
    "            sequence = (\n",
    "                instance.get(\"seq\", None)\n",
    "                if \"seq\" in instance\n",
    "                else instance.get(\"sequence\", None)\n",
    "            )\n",
    "            label = instance.get(\"label\", None)\n",
    "            labels = instance.get(\"labels\", None)\n",
    "            labels = labels if labels is not None else label\n",
    "        else:\n",
    "            raise Exception(\"Unknown instance format.\")\n",
    "\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Sequence is required\")\n",
    "\n",
    "        # Convert DNA sequence to space-separated format for tokenizer\n",
    "        # e.g., \"ATCG\" -> \"A T C G\"\n",
    "        if isinstance(sequence, str):\n",
    "            spaced_sequence = ' '.join(list(sequence))\n",
    "        else:\n",
    "            # If sequence is one-hot encoded, convert to string first\n",
    "            if isinstance(sequence, np.ndarray) and sequence.shape[1] == 4:\n",
    "                # one-hot to sequence string\n",
    "                base_map = {0: 'A', 1: 'T', 2: 'C', 3: 'G'}\n",
    "                sequence_str = ''.join([base_map[np.argmax(sequence[i])] for i in range(len(sequence))])\n",
    "                spaced_sequence = ' '.join(list(sequence_str))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported sequence format: {type(sequence)}\")\n",
    "\n",
    "        # Use tokenizer to process the sequence\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            spaced_sequence[500-self.max_length//2:500+self.max_length//2],  # DeepSEA usually processes 200bp sequences\n",
    "            # spaced_sequence,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Squeeze dimensions\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "\n",
    "        if labels is not None:\n",
    "            # For sequence classification, labels should be a fixed-length vector\n",
    "            if isinstance(labels, np.ndarray):\n",
    "                labels = torch.from_numpy(labels).float()\n",
    "            elif not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "        return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Initialization and Data Inspection\n",
    "Now, we instantiate the tokenizer, model, and datasets, bringing all the components together. We will also inspect a sample to see what our processed data looks like before feeding it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Initialize Tokenizer, Model, and Datasets\n",
    "This step loads the pre-trained model and its corresponding tokenizer from Hugging Face, wraps it in our custom `OmniModelForMultiLabelClassification` class, and then creates the `Dataset` objects for our training, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer and Model\n",
    "print(\"--- Initializing Tokenizer and Model ---\")\n",
    "if \"multimolecule\" in MODEL_NAME_OR_PATH.lower():\n",
    "    from multimolecule import AutoModelForTokenPrediction, RnaTokenizer\n",
    "    base_model = AutoModelForTokenPrediction.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True).base_model\n",
    "    tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    base_model = AutoModel.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "\n",
    "model = OmniModelForMultiLabelClassification(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    num_labels=919,  # DeepSEA has 919 binary labels for different chromatin features\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "# If you want to use LoRA, uncomment the following lines\n",
    "# lora_config = {\n",
    "#     \"lora_r\": 8,  # Rank of the LoRA layers\n",
    "#     \"lora_alpha\": 16,  # Scaling factor for LoRA\n",
    "#     \"lora_dropout\": 0.1,  # Dropout rate for LoRA layers\n",
    "#     \"target_modules\": [\"deepsea_classifier\"],  # Target modules to apply LoRA\n",
    "# }\n",
    "# model = OmniLoraModel(model, lora_config=lora_config)\n",
    "\n",
    "model.to(DEVICE).to(torch.float32) # Move model to the selected device\n",
    "# 2. Create Datasets\n",
    "print(\"\\n--- Creating Datasets ---\")\n",
    "train_set = DeepSEADataset(\n",
    "    data_source=TRAIN_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "test_set = DeepSEADataset(\n",
    "    data_source=TEST_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "valid_set = DeepSEADataset(\n",
    "    data_source=VALID_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ") if os.path.exists(VALID_FILE) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initialization Complete ---\n",
      "Training set size: 100000\n",
      "Test set size: 100000\n",
      "Validation set size: 8000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Initialization Complete ---\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "if valid_set:\n",
    "    print(f\"Validation set size: {len(valid_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2. Inspect a Data Sample\n",
    "Let's examine a single sample from our `train_set` to understand its structure. Our `DeepSEADataset` class processes the raw data into tokenized tensors (`input_ids`) and a label tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "------------------------------\n",
      "Input IDs Tensor Shape: torch.Size([102])\n",
      "Decoded Sequence (first 60 chars): 'G T T C A A G A A T G C A T A A A T T G T A T C T T C A G A ...'\n",
      "------------------------------\n",
      "Labels Tensor Shape: torch.Size([919])\n",
      "Labels Tensor Dtype: torch.float32\n",
      "First 20 labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample from the training set\n",
    "sample = train_set[0]\n",
    "\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Inspect Input ---\n",
    "input_ids = sample['input_ids']\n",
    "print(f\"Input IDs Tensor Shape: {input_ids.shape}\")\n",
    "\n",
    "# Decode the input_ids back to a human-readable sequence\n",
    "decoded_sequence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(f\"Decoded Sequence (first 60 chars): '{decoded_sequence[:60]}...'\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Inspect Labels ---\n",
    "labels = sample['labels']\n",
    "print(f\"Labels Tensor Shape: {labels.shape}\")\n",
    "print(f\"Labels Tensor Dtype: {labels.dtype}\")\n",
    "\n",
    "# Show the first 20 labels for this sequence\n",
    "print(f\"First 20 labels: {labels[:20].int().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training the Model\n",
    "We are now ready to train. We will use the `AccelerateTrainer` from `omnigenbench`, which simplifies the training and evaluation loop, handles device placement, and integrates with `torch.utils.data.DataLoader` and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 500/500 [00:08<00:00, 62.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.49679106328943734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss: 0.6642: 100%|██████████| 6250/6250 [06:12<00:00, 16.76it/s]\n",
      "Evaluating: 100%|██████████| 500/500 [00:07<00:00, 65.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6626522016257755}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Loss: 0.6421: 100%|██████████| 6250/6250 [06:11<00:00, 16.82it/s]\n",
      "Evaluating: 100%|██████████| 500/500 [00:08<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6548992354311663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Loss: 0.6299: 100%|██████████| 6250/6250 [06:15<00:00, 16.66it/s]\n",
      "Evaluating: 100%|██████████| 500/500 [00:08<00:00, 61.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.655689085400226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Loss: 0.6026: 100%|██████████| 6250/6250 [06:10<00:00, 16.89it/s]\n",
      "Evaluating: 100%|██████████| 500/500 [00:08<00:00, 60.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6620117452714009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Loss: 0.5628: 100%|██████████| 6250/6250 [06:13<00:00, 16.74it/s]\n",
      "Evaluating: 100%|██████████| 500/500 [00:07<00:00, 62.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6503828399274646}\n",
      "Early stopping at epoch 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 6250/6250 [01:42<00:00, 61.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.6840973996364564}\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility across all libraries\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE) if valid_set else None\n",
    "\n",
    "# Define the metric for evaluation. For DeepSEA, ROC AUC is a standard metric.\n",
    "metrics = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=valid_loader, # Use validation set for early stopping and checkpointing\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    compute_metrics=metrics,\n",
    "    patience=PATIENCE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"--- Starting Training ---\")\n",
    "metrics = trainer.train()\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluation\n",
    "After training, the `AccelerateTrainer` automatically loads the best model checkpoint (based on validation set performance). It then runs a final evaluation on the held-out test set to provide an unbiased measure of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating on Test Set ---\n",
      "All metrics: {'valid': [{'roc_auc_score': 0.49679106328943734}, {'roc_auc_score': 0.6626522016257755}, {'roc_auc_score': 0.6548992354311663}, {'roc_auc_score': 0.655689085400226}, {'roc_auc_score': 0.6620117452714009}, {'roc_auc_score': 0.6503828399274646}], 'best_valid': {'roc_auc_score': 0.6626522016257755}, 'test': [{'roc_auc_score': 0.6840973996364564}]}\n",
      "Test metric:  {'roc_auc_score': 0.6840973996364564}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating on Test Set ---\")\n",
    "# The evaluation is automated by the trainer\n",
    "print(f\"All metrics:\", metrics)\n",
    "for metric in metrics['test']:\n",
    "    print(f\"Test metric:  {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Inference Example\n",
    "Finally, let's see how to use our fine-tuned model on a real-world example. Instead of a synthetic sequence, we'll take the first sample from our held-out test set. This is a powerful way to gut-check the model's performance, as we can directly compare its predictions with the actual ground truth labels for that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sequence: G C C A T T G G C C G T C T G T G C C A C C T G C C C A C T G T G A A G G C A T G T G A C T T G G A T C C T G G T G A A G G A G G T G G C T G T G T G G C G G G G T G G G C A G G T A A A G A A G C A G\n",
      "Input Sequence (from test set): '{'input_ids': tensor([[0, 6, 5, 5, 4, 7, 7, 6, 6, 5, 5, 6, 7, 5, 7, 6, 7, 6, 5, 5, 4, 5, 5, 7,\n",
      "         6, 5, 5, 5, 4, 5, 7, 6, 7, 6, 4, 4, 6, 6, 5, 4, 7, 6, 7, 6, 4, 5, 7, 7,\n",
      "         6, 6, 4, 7, 5, 5, 7, 6, 6, 7, 6, 4, 4, 6, 6, 4, 6, 6, 7, 6, 6, 5, 7, 6,\n",
      "         7, 6, 7, 6, 6, 5, 6, 6, 6, 6, 7, 6, 6, 6, 5, 4, 6, 6, 7, 4, 4, 4, 6, 4,\n",
      "         4, 6, 5, 4, 6, 2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}...'\n",
      "-----------------------------------------------------------------\n",
      "--- Inference on a Test Sample (first 20 labels) ---\n",
      "Label #    | Prediction    | Ground Truth  | Probability\n",
      "-----------------------------------------------------------------\n",
      "Label 1       | Binds         | Does not bind | 0.5301  false\n",
      "Label 2       | Does not bind | Does not bind | 0.4706  correct\n",
      "Label 3       | Binds         | Does not bind | 0.5001  false\n",
      "Label 4       | Does not bind | Does not bind | 0.4076  correct\n",
      "Label 5       | Binds         | Does not bind | 0.6681  false\n",
      "Label 6       | Binds         | Does not bind | 0.5687  false\n",
      "Label 7       | Binds         | Does not bind | 0.5205  false\n",
      "Label 8       | Does not bind | Does not bind | 0.4923  correct\n",
      "Label 9       | Binds         | Does not bind | 0.5204  false\n",
      "Label 10      | Does not bind | Does not bind | 0.4523  correct\n",
      "Label 11      | Binds         | Does not bind | 0.5381  false\n",
      "Label 12      | Does not bind | Does not bind | 0.4835  correct\n",
      "Label 13      | Binds         | Does not bind | 0.5223  false\n",
      "Label 14      | Binds         | Does not bind | 0.5478  false\n",
      "Label 15      | Does not bind | Does not bind | 0.4196  correct\n",
      "Label 16      | Does not bind | Does not bind | 0.4903  correct\n",
      "Label 17      | Does not bind | Does not bind | 0.4962  correct\n",
      "Label 18      | Does not bind | Does not bind | 0.4642  correct\n",
      "Label 19      | Does not bind | Does not bind | 0.4898  correct\n",
      "Label 20      | Binds         | Does not bind | 0.5522  false\n",
      "-----------------------------------------------------------------\n",
      "Accuracy for this single sample: 85.96%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Select the first sample from the test set\n",
    "test_sequence = tokenizer.decode(test_set[0]['input_ids'], skip_special_tokens=True) # you can also use any DNA sequence you want\n",
    "print(f\"Test Sequence: {test_sequence}\")\n",
    "true_labels = test_set[0]['labels'].int() # The ground truth\n",
    "\n",
    "\n",
    "\n",
    "# 2. Prepare the sequence for the model (add spaces between characters)\n",
    "spaced_sequence = ' '.join(list(test_sequence))\n",
    "inputs = tokenizer(spaced_sequence, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n",
    "print(f\"Input Sequence (from test set): '{inputs[:80]}...'\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 4. Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model.predict(inputs)\n",
    "\n",
    "# 5. Extract predictions and probabilities\n",
    "# We take the first element [0] as our batch size is 1.\n",
    "predictions = outputs['predictions'][0].cpu().numpy()\n",
    "probabilities = outputs['probabilities'][0].cpu().numpy()\n",
    "\n",
    "# 6. Compare predictions with true labels for the first 20 TFs\n",
    "print(\"--- Inference on a Test Sample (first 20 labels) ---\")\n",
    "print(f\"{'Label #':<10} | {'Prediction':<13} | {'Ground Truth':<13} | {'Probability'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(20):\n",
    "    pred_label = 'Binds' if predictions[i] == 1 else 'Does not bind'\n",
    "    true_label = 'Binds' if true_labels[i] == 1 else 'Does not bind'\n",
    "    prob = probabilities[i]\n",
    "    # Add a checkmark if the prediction is correct\n",
    "    correct = \"correct\" if pred_label == true_label else \"false\"\n",
    "    \n",
    "    print(f\"Label {i+1:<7} | {pred_label:<13} | {true_label:<13} | {prob:.4f}  {correct}\")\n",
    "\n",
    "# Optional: Calculate and print the accuracy just for this single sample\n",
    "accuracy = (predictions == true_labels.cpu().numpy()).mean()\n",
    "print(\"-\" * 65)\n",
    "print(f\"Accuracy for this single sample: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "Congratulations! You have successfully fine-tuned a genomic foundation model for transcription factor binding prediction.\n",
    "\n",
    "In this tutorial, you have learned how to:\n",
    "\n",
    "- Set up a project environment for genomic deep learning.\n",
    "\n",
    "- Load and preprocess the DeepSEA dataset using a custom `OmniDataset` class.\n",
    "\n",
    "- Define a multi-label classification model by adding a custom head to a pre-trained `OmniGenome` backbone.\n",
    "\n",
    "- Train and evaluate the model efficiently using the `AccelerateTrainer` from OmniGenBench.\n",
    "\n",
    "- Use the final model to make predictions on new DNA sequences.\n",
    "\n",
    "From here, you could explore:\n",
    "\n",
    "- Experimenting with other models: Try different backbones from the `AVAILABLE_MODELS` list (see section 3.2).\n",
    "\n",
    "- Hyperparameter tuning: Adjust `LEARNING_RATE`, `BATCH_SIZE`, or `WEIGHT_DECAY` to improve performance (see section 3.3).\n",
    "\n",
    "- Using LoRA: Uncomment the `OmniLoraModel` code in the Initialization section to try parameter-efficient fine-tuning (see section 6.1).\n",
    "\n",
    "- Applying to other tasks: Adapt the pipeline for other genomic classification tasks, such as predicting promoter regions or splice sites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
