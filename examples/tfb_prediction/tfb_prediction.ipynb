{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69da32c4",
   "metadata": {},
   "source": [
    "# Transcription Factor Binding Prediction with OmniGenBench\n",
    "\n",
    "This notebook provides a step-by-step guide to extend OmniGenBench to the TFB task based on the **OmniGenome-52M** model on the **DeepSEA dataset**. The goal is to perform multi-label classification to predict the binding sites of various transcription factors based on DNA sequences.\n",
    "\n",
    "**Dataset Description:**\n",
    "The dataset used in this notebook is derived from the DeepSEA dataset, which is designed for studying the effects of non-coding variants. It consists of DNA sequences of 1000 base pairs, each associated with 919 binary labels corresponding to various chromatin features (transcription factor binding, DNase I sensitivity, and histone marks). For this task, we use a preprocessed version available from the `yangheng/tfb_prediction` dataset on Hugging Face.\n",
    "\n",
    "**Estimated Runtime:**\n",
    "The total runtime for this notebook depends on the hardware and the number of training examples (`MAX_EXAMPLES`). On a single NVIDIA RTX 4090 GPU, training with the default settings (`MAX_EXAMPLES=100000`, `EPOCHS=10`) takes approximately **1–2 hours**. For a quick test run with `MAX_EXAMPLES=1000`, it should take about **5–10 minutes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c55114efe0a7ac",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized into concise sections. Most core logic is moved to `examples/tfb_prediction/utils.py` and imported here:\n",
    "\n",
    "1. **Setup & Installation**: Ensures all required libraries and dependencies are installed.\n",
    "2. **Import Libraries**: Loads the necessary Python libraries for genomic data processing, model inference, and analysis.\n",
    "3. **Configuration**: Defines key parameters such as file paths, model selection, and training hyperparameters.\n",
    "4. **Model and Dataset Initialization**: Initializes the tokenizer, model, datasets, and data loaders.\n",
    "5. **Finetuning**: Fine-tunes the model using `AccelerateTrainer` via utility functions.\n",
    "6. **Inference Example**: Uses the trained model to make predictions on a new DNA sequence.\n",
    "\n",
    "Follow the notebook sequentially to execute the TFB prediction pipeline effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc06cade3b7f54",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's ensure all the required packages are installed. If you have already installed them, you can skip this cell. Otherwise, uncomment and run the cell to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "id": "75f3cf1fddccbf3f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# !pip install -U torch numpy transformers omnigenbench autocuda"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc07288065bc2cfe",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for genomic data processing, model inference, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c068af0adca99",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import autocuda\n",
    "import importlib, sys\n",
    "\n",
    "import findfile\n",
    "\n",
    "utils_spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(utils_spec)\n",
    "utils_spec.loader.exec_module(utils)\n",
    "sys.modules[\"utils\"] = utils\n",
    "\n",
    "# Import reusable interfaces from local utils\n",
    "from utils import (\n",
    "    download_deepsea_dataset,\n",
    "    load_tokenizer_and_model,\n",
    "    build_datasets,\n",
    "    create_dataloaders,\n",
    "    run_finetuning,\n",
    "    run_inference,\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b033cd9d3d8d8abe",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Here, we define all the hyperparameters and settings for our experiment. This centralized configuration makes it easy to modify parameters and track experiments."
   ]
  },
  {
   "cell_type": "code",
   "id": "a48cc627b656704a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# --- Data File Paths ---\n",
    "LOCAL_PATH = \"deepsea_tfb_prediction\"\n",
    "download_deepsea_dataset(LOCAL_PATH)\n",
    "TRAIN_FILE = findfile.find_cwd_file(['train', 'jsonl'])\n",
    "TEST_FILE =  findfile.find_cwd_file(['test', 'jsonl'])\n",
    "VALID_FILE =  findfile.find_cwd_file(['valid', 'jsonl'])\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# --- Available Models for Testing ---\n",
    "AVAILABLE_MODELS = [\n",
    "    'yangheng/OmniGenome-52M',\n",
    "    'yangheng/OmniGenome-186M',\n",
    "    'yangheng/OmniGenome-v1.5',\n",
    "    # You can add more models here as needed,\n",
    "    # 'DNABERT-2-117M',\n",
    "    # 'hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-500m-human-ref',\n",
    "    # 'multimolecule/rnafm', # RNA-specific models\n",
    "    # 'multimolecule/rnabert',\n",
    "    # 'SpliceBERT-510nt', # Splice-specific model\n",
    "]\n",
    "\n",
    "MODEL_NAME_OR_PATH = AVAILABLE_MODELS[0]\n",
    "USE_CONV_LAYERS = False  # Set to True to add DeepSEA-style convolutional layers on top of OmniGenome (not used in this demo)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3  # For early stopping\n",
    "MAX_LENGTH = 200  # The length of the DNA sequence to be processed\n",
    "SEED = 45\n",
    "# LABEL_INDICES = [0]  # Example indices for the first 10 transcription factors\n",
    "LABEL_INDICES = list(range(919))\n",
    "MAX_EXAMPLES = 1000000  # Use a smaller number for quick testing (e.g., 1000), or None for all data\n",
    "\n",
    "DEVICE = autocuda.auto_cuda()\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b42bcd07c87b441",
   "metadata": {},
   "source": [
    "## 4. Model and Dataset Initialization\n",
    "\n",
    "Initialize tokenizer and model, then build datasets and dataloaders using utilities for a concise workflow."
   ]
  },
  {
   "cell_type": "code",
   "id": "a9cb399ba133d2fd",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Initialize Tokenizer and Model\n",
    "print(\"--- Initializing Tokenizer and Model ---\")\n",
    "\n",
    "# Use utility to load tokenizer and model\n",
    "label_count = len(LABEL_INDICES)\n",
    "tokenizer, model = load_tokenizer_and_model(\n",
    "    MODEL_NAME_OR_PATH,\n",
    "    num_labels=label_count,\n",
    "    threshold=0.5,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# 2. Create Datasets via utility\n",
    "print(\"\\n--- Creating Datasets ---\")\n",
    "train_set, valid_set, test_set = build_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    train_file=TRAIN_FILE,\n",
    "    test_file=TEST_FILE,\n",
    "    valid_file=VALID_FILE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    label_indices=LABEL_INDICES,\n",
    ")\n",
    "\n",
    "# Create DataLoaders for batching (utils)\n",
    "train_loader, valid_loader, test_loader = create_dataloaders(\n",
    "    train_set=train_set,\n",
    "    valid_set=valid_set,\n",
    "    test_set=test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Initialization Complete ---\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "if valid_set:\n",
    "    print(f\"Validation set size: {len(valid_set)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac184bbe6d421581",
   "metadata": {},
   "source": [
    "## 5. Finetuning\n",
    "\n",
    "Fine-tune the model using `AccelerateTrainer` (invoked through the `run_finetuning` compatibility wrapper). Early stopping monitors validation ROC AUC when a validation set is provided."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac0566f9ac4b9ce3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Train with utilities\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer, metrics_best = run_finetuning(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    patience=PATIENCE,\n",
    "    device=DEVICE,\n",
    "    save_dir=\"tfb_model\",\n",
    ")\n",
    "print(metrics_best)\n",
    "print(\"--- Training Finished ---\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56210d0c5584f5b6",
   "metadata": {},
   "source": [
    "## 6. Inference Example\n",
    "\n",
    "Run a single-sequence prediction using the persisted fine-tuned model. The same preprocessing pathway (`encode_tokens`) ensures parity with training."
   ]
  },
  {
   "cell_type": "code",
   "id": "51a2e82084f46cd4",
   "metadata": {},
   "source": [
    "\n",
    "sample_sequence = \"AGCT\" * (MAX_LENGTH // 4)  # Construct sequence of required length\n",
    "\n",
    "outputs = run_inference(\n",
    "    model_dir=\"tfb_model\",\n",
    "    tokenizer=tokenizer,\n",
    "    sample_sequence=sample_sequence,\n",
    "    max_length=MAX_LENGTH,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "predictions = outputs.get('predictions', None)\n",
    "probabilities = outputs.get('probabilities', None)\n",
    "\n",
    "print(f\"Input sequence length: {len(sample_sequence)} bp\")\n",
    "if predictions is not None:\n",
    "    print(f\"Number of predicted labels: {len(predictions)}\")\n",
    "    print(\"\\n--- Predictions for the first 10 TFs ---\")\n",
    "    for i in range(min(10, len(predictions))):\n",
    "        pred_label = 'Binds' if int(predictions[i]) == 1 else 'Does not bind'\n",
    "        if probabilities is not None:\n",
    "            try:\n",
    "                p = float(probabilities[i])\n",
    "                print(f\"Label {i+1}: Prediction={pred_label}, Prob={p:.4f}\")\n",
    "            except Exception:\n",
    "                print(f\"Label {i+1}: Prediction={pred_label}\")\n",
    "        else:\n",
    "            print(f\"Label {i+1}: Prediction={pred_label}\")\n",
    "else:\n",
    "    print(\"No 'predictions' returned by model.inference; verify the saved model and inference API.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
