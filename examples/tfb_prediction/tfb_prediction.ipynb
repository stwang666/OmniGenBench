{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Factor Binding Prediction with OmniGenBench\n",
    "\n",
    "This notebook provides a step-by-step guide to extend OmniGenBench to the TFB task based on the **OmniGenome-52M** model on the **DeepSEA dataset**. The goal is to perform multi-label classification to predict the binding sites of various transcription factors based on DNA sequences.\n",
    "\n",
    "**Dataset Description:**\n",
    "The dataset used in this notebook is derived from the DeepSEA dataset, which is designed for studying the effects of non-coding variants. It consists of DNA sequences of 1000 base pairs, each associated with 919 binary labels corresponding to various chromatin features (transcription factor binding, DNase I sensitivity, and histone marks). For this task, we use a preprocessed version available from the `yangheng/tfb_prediction` dataset on Hugging Face.\n",
    "\n",
    "**Estimated Runtime:**\n",
    "The total runtime for this notebook depends on the hardware and the number of training examples (`MAX_EXAMPLES`). On a single NVIDIA RTX 4090 GPU, training with the default settings (`MAX_EXAMPLES=100000`, `EPOCHS=10`) takes approximately **1 - 2 hours**. For a quick test run with `MAX_EXAMPLES=1000`, it should take about **5-10 minutes**.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized into several sections, each focusing on a specific aspect of the Transcription Factor Binding (TFB) prediction pipeline. Below is an overview of the structure:\n",
    "\n",
    "1. **Setup & Installation**: Ensures all required libraries and dependencies are installed.\n",
    "2. **Import Libraries**: Loads the necessary Python libraries for genomic data processing, model inference, and analysis.\n",
    "3. **Configuration**: Defines key parameters such as file paths, model selection, and training hyperparameters.\n",
    "4. **Model Definition**: Implements a custom model class that integrates the OmniGenome backbone with a classification head tailored for the DeepSEA task.\n",
    "5. **Data Loading and Preprocessing**: Handles the loading and preprocessing of the DeepSEA dataset, converting DNA sequences into tokenized inputs.\n",
    "6. **Initialization**: Sets up the tokenizer, model, datasets, and data loaders for training and evaluation.\n",
    "7. **Training the Model**: Fine-tunes the model using the `AccelerateTrainer` for efficient training and evaluation.\n",
    "8. **Evaluation**: Assesses the model's performance on the test set using metrics such as ROC AUC.\n",
    "9. **Inference Example**: Demonstrates how to use the trained model to make predictions on new DNA sequences.\n",
    "\n",
    "Each section is designed to be modular, allowing for easy customization and extension. Follow the notebook sequentially to understand and execute the TFB prediction pipeline effectively."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's ensure all the required packages are installed. If you have already installed them, you can skip this cell. Otherwise, uncomment and run the cell to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:21:18.487951Z",
     "start_time": "2025-08-06T15:21:18.483935Z"
    }
   },
   "source": "# Uncomment the following line to install the necessary packages\n# !pip install torch numpy transformers omnigenbench autocuda",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for genomic data processing, model inference, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:21:18.522279Z",
     "start_time": "2025-08-06T15:21:18.515513Z"
    }
   },
   "source": [
    "import random\n",
    "import os\n",
    "import autocuda\n",
    "import findfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "from omnigenbench import (\n",
    "    OmniDataset,\n",
    "    OmniModel,\n",
    "    OmniPooling,\n",
    "    Trainer,\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    OmniLoraModel\n",
    ")\n",
    "import zipfile\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Configuration\n\nHere, we define all the hyperparameters and settings for our experiment. This centralized configuration makes it easy to modify parameters and track experiments."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:25:04.719727Z",
     "start_time": "2025-08-06T15:21:18.566927Z"
    }
   },
   "source": [
    "# --- Data File Paths ---\n",
    "# Download tfb_prediction dataset using git clone\n",
    "local_dir = \"tfb_prediction_dataset\"\n",
    "\n",
    "if not findfile.find_cwd_dir(local_dir):\n",
    "    git_url = \"https://huggingface.co/datasets/yangheng/tfb_prediction\"\n",
    "    os.system(f\"git clone {git_url} {local_dir}\") # Use subprocess.run if you prefer\n",
    "    # import subprocess\n",
    "    # subprocess.run([\"git\", \"clone\", git_url, local_dir])\n",
    "    print(f\"Cloned tfb_prediction dataset from {git_url} into {local_dir}\")\n",
    "\n",
    "    # Unzip the dataset if the zip file exists\n",
    "    ZIP_DATASET = findfile.find_cwd_file(\"tfb_dataset.zip\")\n",
    "    if ZIP_DATASET:\n",
    "        with zipfile.ZipFile(ZIP_DATASET, 'r') as zip_ref:\n",
    "            zip_ref.extractall(local_dir)\n",
    "        print(f\"Extracted tfb_dataset.zip into {local_dir}\")\n",
    "        os.remove(ZIP_DATASET)\n",
    "    else:\n",
    "        print(\"tfb_dataset.zip not found. Skipping extraction.\")\n",
    "\n",
    "TRAIN_FILE = findfile.find_cwd_file(\"train_tfb.npy\")\n",
    "if not TRAIN_FILE:\n",
    "    raise FileNotFoundError(\"Training file not found. Please ensure the dataset is downloaded and extracted correctly.\")\n",
    "TEST_FILE = findfile.find_cwd_file(\"test_tfb.npy\")\n",
    "if not TEST_FILE:\n",
    "    raise FileNotFoundError(\"Test file not found. Please ensure the dataset is downloaded and extracted correctly.\")\n",
    "VALID_FILE = findfile.find_cwd_file(\"valid_tfb.npy\")\n",
    "if not VALID_FILE:\n",
    "    print(\"Validation file not found. Skipping validation set.\")\n",
    "# TRAIN_FILE = \"tfb_prediction_dataset/train_tfb.npy\"\n",
    "# TEST_FILE = \"tfb_prediction_dataset/test_tfb.npy\"\n",
    "# VALID_FILE = \"tfb_prediction_dataset/valid_tfb.npy\"\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# --- Available Models for Testing ---\n",
    "AVAILABLE_MODELS = [\n",
    "    'yangheng/OmniGenome-52M',\n",
    "    'yangheng/OmniGenome-186M',\n",
    "    'yangheng/OmniGenome-v1.5',\n",
    "\n",
    "    # 'DNABERT-2-117M',  # You can add more models here as needed,\n",
    "    # 'LongSafari/hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-500m-human-ref',\n",
    "    # 'multimolecule/rnafm', # RNA-specific models\n",
    "    # 'multimolecule/rnamsm',\n",
    "    # 'multimolecule/rnabert',\n",
    "    # 'SpliceBERT-510nt', # Splice-specific model\n",
    "]\n",
    "\n",
    "MODEL_NAME_OR_PATH = AVAILABLE_MODELS[0]\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 3  # For early stopping\n",
    "MAX_LENGTH = 200  # The length of the DNA sequence to be processed\n",
    "SEED = 45\n",
    "MAX_EXAMPLES = 100000  # Use a smaller number for quick testing (e.g., 1000), or None for all data\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "# --- Device Setup ---\n",
    "DEVICE = autocuda.auto_cuda()\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned tfb_prediction dataset from https://huggingface.co/datasets/yangheng/tfb_prediction into tfb_prediction_dataset\n",
      "Extracted tfb_dataset.zip into tfb_prediction_dataset\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Definition\n\nWe define the `OmniModelForMultiLabelClassification`, which wraps the OmniGenome transformer. This class adds a classification head on top of the pre-trained backbone, tailored for the DeepSEA multi-label prediction task. It also includes an option to add convolutional layers, allowing for a hybrid architecture that combines the strengths of both transformers and CNNs."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:25:04.840481Z",
     "start_time": "2025-08-06T15:25:04.823585Z"
    }
   },
   "source": [
    "class OmniModelForMultiLabelClassification(OmniModel):\n",
    "    \"\"\"\n",
    "    Multi-label sequence classification model based on OmniGenome-52M.\n",
    "\n",
    "    This model replaces the original DeepSEA CNN architecture with a pretrained\n",
    "    Transformer encoder from the OmniGenome family. Optionally, convolutional\n",
    "    layers can be stacked on top of the Transformer outputs for additional\n",
    "    feature extraction.\n",
    "\n",
    "    Parameters:\n",
    "        config_or_model (PretrainedConfig or nn.Module):\n",
    "            Configuration or instance of the pretrained Transformer model,\n",
    "            typically obtained via AutoModel.from_pretrained().\n",
    "        tokenizer (PreTrainedTokenizer):\n",
    "            Tokenizer compatible with the Transformer encoder, used to convert\n",
    "            DNA sequences into model inputs.\n",
    "        threshold (float, optional):\n",
    "            Probability threshold for binary decisions in predict(), defaults to 0.5.\n",
    "        use_conv (bool, optional):\n",
    "            If True, apply convolutional layers after the Transformer encoder\n",
    "            for enhanced feature extraction, defaults to False.\n",
    "        *args, **kwargs: Additional arguments passed to the base OmniModel class.\n",
    "\n",
    "    Attributes:\n",
    "        threshold (float):\n",
    "            Probability cutoff for generating binary predictions.\n",
    "        deepsea_classifier (nn.Sequential):\n",
    "            Classification head consisting of a Tanh activation followed by\n",
    "            a linear layer mapping to the number of labels.\n",
    "        loss_fn (nn.BCEWithLogitsLoss):\n",
    "            Binary cross-entropy loss with logits, using pos_weight to balance\n",
    "            positive and negative samples.\n",
    "        pooler (OmniPooling):\n",
    "            Utility for pooling token-level outputs into a sequence-level vector.\n",
    "        sigmoid (nn.Sigmoid):\n",
    "            Activation used to convert logits to probabilities during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        self.threshold = kwargs.pop(\"threshold\", 0.5)\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = \"OmniModelForMultiLabelClassification\"\n",
    "\n",
    "        # Classification head based directly on Transformer outputs\n",
    "        conv_output_dim = self.config.hidden_size\n",
    "        self.deepsea_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(conv_output_dim, self.config.num_labels),\n",
    "        )\n",
    "\n",
    "        # Use pos_weight to address class imbalance in BCEWithLogitsLoss\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0]))\n",
    "        self.pooler = OmniPooling(self.config)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model: encode, pool, classify, and optionally compute loss.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict):\n",
    "                Must contain 'input_ids', 'attention_mask', etc., and optionally 'labels'.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'logits': Tensor of shape (batch_size, num_labels), raw scores before\n",
    "                          sigmoid activation.\n",
    "                'last_hidden_state': Tensor of shape (batch_size, seq_len, hidden_size),\n",
    "                                     the last layer hidden states from the Transformer.\n",
    "                'loss' (optional): Computed BCEWithLogitsLoss if 'labels' provided.\n",
    "            }\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If all provided labels are zero or if logits and labels shapes mismatch.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "\n",
    "        # Encode inputs\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "\n",
    "        # Pooling strategy\n",
    "        if self.pooler._is_causal_lm():\n",
    "            pad_token_id = getattr(self.config, \"pad_token_id\", -100)\n",
    "            sequence_lengths = inputs[\"input_ids\"].ne(pad_token_id).sum(dim=1) - 1\n",
    "            pooled_output = last_hidden_state[\n",
    "                torch.arange(inputs[\"input_ids\"].size(0), device=last_hidden_state.device),\n",
    "                sequence_lengths,\n",
    "            ]\n",
    "        else:\n",
    "            pooled_output = self.pooler(inputs, last_hidden_state)\n",
    "\n",
    "        logits = self.deepsea_classifier(pooled_output)\n",
    "        outputs = {\"logits\": logits, \"last_hidden_state\": last_hidden_state}\n",
    "\n",
    "        if labels is not None:\n",
    "            if torch.sum(labels[labels != -100]) == 0:\n",
    "                raise ValueError(\"Labels cannot be all zeros.\")\n",
    "            labels = labels[labels != -100]\n",
    "            loss = self.loss_fn(logits.view(-1), labels.view(-1).to(torch.float32))\n",
    "            outputs[\"loss\"] = loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform inference on raw sequences or tokenized inputs, returning probabilities and predictions.\n",
    "\n",
    "        Args:\n",
    "            sequence_or_inputs (str, BatchEncoding, or dict):\n",
    "                Raw DNA string or pre-tokenized inputs.\n",
    "            padding (str, optional): Padding strategy for tokenizer, defaults to 'max_length'.\n",
    "            max_length (int, optional): Maximum sequence length, defaults to 1024.\n",
    "            **kwargs: Additional tokenizer arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'predictions': Tensor of binary labels,\n",
    "                'probabilities': Tensor of positive class probabilities,\n",
    "                'logits': Tensor of raw scores,\n",
    "                'last_hidden_state': Transformer outputs.\n",
    "            }\n",
    "        \"\"\"\n",
    "        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(\n",
    "                sequence_or_inputs, dict\n",
    "        ):\n",
    "            inputs = self.tokenizer(\n",
    "                sequence_or_inputs,\n",
    "                padding=kwargs.pop(\"padding\", \"max_length\"),\n",
    "                max_length=kwargs.pop(\"max_length\", 1024),\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            inputs = sequence_or_inputs\n",
    "        inputs = inputs.to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(**inputs)\n",
    "        logits = outputs[\"logits\"]\n",
    "        last_hidden_state = outputs[\"last_hidden_state\"]\n",
    "\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        predictions = (probabilities >= self.threshold).to(torch.int)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": probabilities,\n",
    "            \"logits\": logits,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Compute BCEWithLogitsLoss for multi-label classification.\n",
    "\n",
    "        Args:\n",
    "            logits (Tensor): Raw output scores, shape (batch_size, num_labels).\n",
    "            labels (Tensor): Ground-truth labels as floats, same shape as logits.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If logits and labels shapes do not match.\n",
    "        \"\"\"\n",
    "        valid_labels = labels.to(torch.float32)\n",
    "        if logits.shape != valid_labels.shape:\n",
    "            raise ValueError(f\"Shape mismatch between logits {logits.shape} and labels {valid_labels.shape}\")\n",
    "        return self.loss_fn(logits, valid_labels)\n",
    "\n",
    "\n",
    "print(\"OmniModelForMultiLabelClassification defined.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniModelForMultiLabelClassification defined.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Loading and Preprocessing\n",
    "\n",
    "This section handles the data loading. We define a helper function `load_deepsea_npy_data` to parse the specific format of the DeepSEA `.npy` files. Then, we create a `DeepSEADataset` class that inherits from `OmniDataset` and uses this loader. The dataset class is responsible for converting DNA sequences into a format suitable for the OmniGenome tokenizer (i.e., space-separated tokens)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:25:04.857660Z",
     "start_time": "2025-08-06T15:25:04.847143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class DeepSEADataset(OmniDataset):\n",
    "    \"\"\"\n",
    "    Dataset designed for the DeepSEA task, handling the conversion from DNA sequences to tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metadata[key] = value\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        \"\"\"\n",
    "        Prepare input data for DeepSEA\n",
    "\n",
    "        Expected instance format:\n",
    "        {\n",
    "            'sequence': DNA sequence string (e.g., \"ATCGATCG...\")\n",
    "            'labels': binary labels as numpy array of shape (919,)\n",
    "        }\n",
    "        \"\"\"\n",
    "        labels = None\n",
    "        if isinstance(instance, str):\n",
    "            sequence = instance\n",
    "        elif isinstance(instance, dict):\n",
    "            sequence = (\n",
    "                instance.get(\"seq\", None)\n",
    "                if \"seq\" in instance\n",
    "                else instance.get(\"sequence\", None)\n",
    "            )\n",
    "            label = instance.get(\"label\", None)\n",
    "            labels = instance.get(\"labels\", None)\n",
    "            labels = labels if labels is not None else label\n",
    "        else:\n",
    "            raise Exception(\"Unknown instance format.\")\n",
    "\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Sequence is required\")\n",
    "\n",
    "        # Convert DNA sequence to space-separated format for tokenizer\n",
    "        # e.g., \"ATCG\" -> \"A T C G\"\n",
    "        if isinstance(sequence, str):\n",
    "            spaced_sequence = ' '.join(list(sequence))\n",
    "        else:\n",
    "            # If sequence is one-hot encoded, convert to string first\n",
    "            if isinstance(sequence, np.ndarray) and sequence.shape[1] == 4:\n",
    "                # one-hot to sequence string\n",
    "                base_map = {0: 'A', 1: 'T', 2: 'C', 3: 'G'}\n",
    "                sequence_str = ''.join([base_map[np.argmax(sequence[i])] for i in range(len(sequence))])\n",
    "                spaced_sequence = ' '.join(list(sequence_str))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported sequence format: {type(sequence)}\")\n",
    "\n",
    "        # Use tokenizer to process the sequence\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            spaced_sequence[500-self.max_length//2:500+self.max_length//2],  # DeepSEA usually processes 200bp sequences\n",
    "            # spaced_sequence,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Squeeze dimensions\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "\n",
    "        if labels is not None:\n",
    "            # For sequence classification, labels should be a fixed-length vector\n",
    "            if isinstance(labels, np.ndarray):\n",
    "                labels = torch.from_numpy(labels).float()\n",
    "            elif not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Initialization\n",
    "\n",
    "Now, let's initialize the tokenizer, the model, and the datasets. This step brings everything together and prepares for the training phase."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:29:12.408203Z",
     "start_time": "2025-08-06T15:25:04.864185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Tokenizer and Model\n",
    "print(\"--- Initializing Tokenizer and Model ---\")\n",
    "if \"multimolecule\" in MODEL_NAME_OR_PATH.lower():\n",
    "    from multimolecule import AutoModelForTokenPrediction, RnaTokenizer\n",
    "    base_model = AutoModelForTokenPrediction.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True).base_model\n",
    "    tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    base_model = AutoModel.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "\n",
    "model = OmniModelForMultiLabelClassification(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    num_labels=919,  # DeepSEA has 919 binary labels for different chromatin features\n",
    "    threshold=0.5,\n",
    ")\n",
    "\n",
    "# If you want to use LoRA, uncomment the following lines\n",
    "# lora_config = {\n",
    "#     \"lora_r\": 8,  # Rank of the LoRA layers\n",
    "#     \"lora_alpha\": 16,  # Scaling factor for LoRA\n",
    "#     \"lora_dropout\": 0.1,  # Dropout rate for LoRA layers\n",
    "#     \"target_modules\": [\"deepsea_classifier\"],  # Target modules to apply LoRA\n",
    "# }\n",
    "# model = OmniLoraModel(model, lora_config=lora_config)\n",
    "\n",
    "model.to(DEVICE).to(torch.float32) # Move model to the selected device\n",
    "# 2. Create Datasets\n",
    "print(\"\\n--- Creating Datasets ---\")\n",
    "train_set = DeepSEADataset(\n",
    "    data_source=TRAIN_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "test_set = DeepSEADataset(\n",
    "    data_source=TEST_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ")\n",
    "valid_set = DeepSEADataset(\n",
    "    data_source=VALID_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    force_padding=False,  # DeepSEA does not require padding\n",
    ") if os.path.exists(VALID_FILE) else None\n",
    "\n",
    "print(\"\\n--- Initialization Complete ---\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "if valid_set:\n",
    "    print(f\"Validation set size: {len(valid_set)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Tokenizer and Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RnaMsmForTokenPrediction were not initialized from the model checkpoint at multimolecule/rnamsm and are newly initialized: ['token_head.decoder.bias', 'token_head.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: OmniModelForMultiLabelClassification\n",
      "Model Metadata: {'library_name': 'omnigenbench', 'omnigenbench_version': '0.3.7alpha', 'torch_version': '2.7.0+cu128+cu12.8+git134179474539648ba7dee1317959529fbd0e7f89', 'transformers_version': '4.53.3', 'model_cls': 'OmniModelForMultiLabelClassification', 'tokenizer_cls': 'RnaTokenizer', 'model_name': 'OmniModelForMultiLabelClassification'}\n",
      "Base Model Name: multimolecule/rnamsm\n",
      "Model Type: rnamsm\n",
      "Model Architecture: ['RnaMsmForPreTraining']\n",
      "Model Parameters: 95.329792 M\n",
      "Model Config: RnaMsmConfig {\n",
      "  \"architectures\": [\n",
      "    \"RnaMsmForPreTraining\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_type\": \"standard\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embed_positions_msa\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\",\n",
      "    \"20\": \"20\",\n",
      "    \"21\": \"21\",\n",
      "    \"22\": \"22\",\n",
      "    \"23\": \"23\",\n",
      "    \"24\": \"24\",\n",
      "    \"25\": \"25\",\n",
      "    \"26\": \"26\",\n",
      "    \"27\": \"27\",\n",
      "    \"28\": \"28\",\n",
      "    \"29\": \"29\",\n",
      "    \"30\": \"30\",\n",
      "    \"31\": \"31\",\n",
      "    \"32\": \"32\",\n",
      "    \"33\": \"33\",\n",
      "    \"34\": \"34\",\n",
      "    \"35\": \"35\",\n",
      "    \"36\": \"36\",\n",
      "    \"37\": \"37\",\n",
      "    \"38\": \"38\",\n",
      "    \"39\": \"39\",\n",
      "    \"40\": \"40\",\n",
      "    \"41\": \"41\",\n",
      "    \"42\": \"42\",\n",
      "    \"43\": \"43\",\n",
      "    \"44\": \"44\",\n",
      "    \"45\": \"45\",\n",
      "    \"46\": \"46\",\n",
      "    \"47\": \"47\",\n",
      "    \"48\": \"48\",\n",
      "    \"49\": \"49\",\n",
      "    \"50\": \"50\",\n",
      "    \"51\": \"51\",\n",
      "    \"52\": \"52\",\n",
      "    \"53\": \"53\",\n",
      "    \"54\": \"54\",\n",
      "    \"55\": \"55\",\n",
      "    \"56\": \"56\",\n",
      "    \"57\": \"57\",\n",
      "    \"58\": \"58\",\n",
      "    \"59\": \"59\",\n",
      "    \"60\": \"60\",\n",
      "    \"61\": \"61\",\n",
      "    \"62\": \"62\",\n",
      "    \"63\": \"63\",\n",
      "    \"64\": \"64\",\n",
      "    \"65\": \"65\",\n",
      "    \"66\": \"66\",\n",
      "    \"67\": \"67\",\n",
      "    \"68\": \"68\",\n",
      "    \"69\": \"69\",\n",
      "    \"70\": \"70\",\n",
      "    \"71\": \"71\",\n",
      "    \"72\": \"72\",\n",
      "    \"73\": \"73\",\n",
      "    \"74\": \"74\",\n",
      "    \"75\": \"75\",\n",
      "    \"76\": \"76\",\n",
      "    \"77\": \"77\",\n",
      "    \"78\": \"78\",\n",
      "    \"79\": \"79\",\n",
      "    \"80\": \"80\",\n",
      "    \"81\": \"81\",\n",
      "    \"82\": \"82\",\n",
      "    \"83\": \"83\",\n",
      "    \"84\": \"84\",\n",
      "    \"85\": \"85\",\n",
      "    \"86\": \"86\",\n",
      "    \"87\": \"87\",\n",
      "    \"88\": \"88\",\n",
      "    \"89\": \"89\",\n",
      "    \"90\": \"90\",\n",
      "    \"91\": \"91\",\n",
      "    \"92\": \"92\",\n",
      "    \"93\": \"93\",\n",
      "    \"94\": \"94\",\n",
      "    \"95\": \"95\",\n",
      "    \"96\": \"96\",\n",
      "    \"97\": \"97\",\n",
      "    \"98\": \"98\",\n",
      "    \"99\": \"99\",\n",
      "    \"100\": \"100\",\n",
      "    \"101\": \"101\",\n",
      "    \"102\": \"102\",\n",
      "    \"103\": \"103\",\n",
      "    \"104\": \"104\",\n",
      "    \"105\": \"105\",\n",
      "    \"106\": \"106\",\n",
      "    \"107\": \"107\",\n",
      "    \"108\": \"108\",\n",
      "    \"109\": \"109\",\n",
      "    \"110\": \"110\",\n",
      "    \"111\": \"111\",\n",
      "    \"112\": \"112\",\n",
      "    \"113\": \"113\",\n",
      "    \"114\": \"114\",\n",
      "    \"115\": \"115\",\n",
      "    \"116\": \"116\",\n",
      "    \"117\": \"117\",\n",
      "    \"118\": \"118\",\n",
      "    \"119\": \"119\",\n",
      "    \"120\": \"120\",\n",
      "    \"121\": \"121\",\n",
      "    \"122\": \"122\",\n",
      "    \"123\": \"123\",\n",
      "    \"124\": \"124\",\n",
      "    \"125\": \"125\",\n",
      "    \"126\": \"126\",\n",
      "    \"127\": \"127\",\n",
      "    \"128\": \"128\",\n",
      "    \"129\": \"129\",\n",
      "    \"130\": \"130\",\n",
      "    \"131\": \"131\",\n",
      "    \"132\": \"132\",\n",
      "    \"133\": \"133\",\n",
      "    \"134\": \"134\",\n",
      "    \"135\": \"135\",\n",
      "    \"136\": \"136\",\n",
      "    \"137\": \"137\",\n",
      "    \"138\": \"138\",\n",
      "    \"139\": \"139\",\n",
      "    \"140\": \"140\",\n",
      "    \"141\": \"141\",\n",
      "    \"142\": \"142\",\n",
      "    \"143\": \"143\",\n",
      "    \"144\": \"144\",\n",
      "    \"145\": \"145\",\n",
      "    \"146\": \"146\",\n",
      "    \"147\": \"147\",\n",
      "    \"148\": \"148\",\n",
      "    \"149\": \"149\",\n",
      "    \"150\": \"150\",\n",
      "    \"151\": \"151\",\n",
      "    \"152\": \"152\",\n",
      "    \"153\": \"153\",\n",
      "    \"154\": \"154\",\n",
      "    \"155\": \"155\",\n",
      "    \"156\": \"156\",\n",
      "    \"157\": \"157\",\n",
      "    \"158\": \"158\",\n",
      "    \"159\": \"159\",\n",
      "    \"160\": \"160\",\n",
      "    \"161\": \"161\",\n",
      "    \"162\": \"162\",\n",
      "    \"163\": \"163\",\n",
      "    \"164\": \"164\",\n",
      "    \"165\": \"165\",\n",
      "    \"166\": \"166\",\n",
      "    \"167\": \"167\",\n",
      "    \"168\": \"168\",\n",
      "    \"169\": \"169\",\n",
      "    \"170\": \"170\",\n",
      "    \"171\": \"171\",\n",
      "    \"172\": \"172\",\n",
      "    \"173\": \"173\",\n",
      "    \"174\": \"174\",\n",
      "    \"175\": \"175\",\n",
      "    \"176\": \"176\",\n",
      "    \"177\": \"177\",\n",
      "    \"178\": \"178\",\n",
      "    \"179\": \"179\",\n",
      "    \"180\": \"180\",\n",
      "    \"181\": \"181\",\n",
      "    \"182\": \"182\",\n",
      "    \"183\": \"183\",\n",
      "    \"184\": \"184\",\n",
      "    \"185\": \"185\",\n",
      "    \"186\": \"186\",\n",
      "    \"187\": \"187\",\n",
      "    \"188\": \"188\",\n",
      "    \"189\": \"189\",\n",
      "    \"190\": \"190\",\n",
      "    \"191\": \"191\",\n",
      "    \"192\": \"192\",\n",
      "    \"193\": \"193\",\n",
      "    \"194\": \"194\",\n",
      "    \"195\": \"195\",\n",
      "    \"196\": \"196\",\n",
      "    \"197\": \"197\",\n",
      "    \"198\": \"198\",\n",
      "    \"199\": \"199\",\n",
      "    \"200\": \"200\",\n",
      "    \"201\": \"201\",\n",
      "    \"202\": \"202\",\n",
      "    \"203\": \"203\",\n",
      "    \"204\": \"204\",\n",
      "    \"205\": \"205\",\n",
      "    \"206\": \"206\",\n",
      "    \"207\": \"207\",\n",
      "    \"208\": \"208\",\n",
      "    \"209\": \"209\",\n",
      "    \"210\": \"210\",\n",
      "    \"211\": \"211\",\n",
      "    \"212\": \"212\",\n",
      "    \"213\": \"213\",\n",
      "    \"214\": \"214\",\n",
      "    \"215\": \"215\",\n",
      "    \"216\": \"216\",\n",
      "    \"217\": \"217\",\n",
      "    \"218\": \"218\",\n",
      "    \"219\": \"219\",\n",
      "    \"220\": \"220\",\n",
      "    \"221\": \"221\",\n",
      "    \"222\": \"222\",\n",
      "    \"223\": \"223\",\n",
      "    \"224\": \"224\",\n",
      "    \"225\": \"225\",\n",
      "    \"226\": \"226\",\n",
      "    \"227\": \"227\",\n",
      "    \"228\": \"228\",\n",
      "    \"229\": \"229\",\n",
      "    \"230\": \"230\",\n",
      "    \"231\": \"231\",\n",
      "    \"232\": \"232\",\n",
      "    \"233\": \"233\",\n",
      "    \"234\": \"234\",\n",
      "    \"235\": \"235\",\n",
      "    \"236\": \"236\",\n",
      "    \"237\": \"237\",\n",
      "    \"238\": \"238\",\n",
      "    \"239\": \"239\",\n",
      "    \"240\": \"240\",\n",
      "    \"241\": \"241\",\n",
      "    \"242\": \"242\",\n",
      "    \"243\": \"243\",\n",
      "    \"244\": \"244\",\n",
      "    \"245\": \"245\",\n",
      "    \"246\": \"246\",\n",
      "    \"247\": \"247\",\n",
      "    \"248\": \"248\",\n",
      "    \"249\": \"249\",\n",
      "    \"250\": \"250\",\n",
      "    \"251\": \"251\",\n",
      "    \"252\": \"252\",\n",
      "    \"253\": \"253\",\n",
      "    \"254\": \"254\",\n",
      "    \"255\": \"255\",\n",
      "    \"256\": \"256\",\n",
      "    \"257\": \"257\",\n",
      "    \"258\": \"258\",\n",
      "    \"259\": \"259\",\n",
      "    \"260\": \"260\",\n",
      "    \"261\": \"261\",\n",
      "    \"262\": \"262\",\n",
      "    \"263\": \"263\",\n",
      "    \"264\": \"264\",\n",
      "    \"265\": \"265\",\n",
      "    \"266\": \"266\",\n",
      "    \"267\": \"267\",\n",
      "    \"268\": \"268\",\n",
      "    \"269\": \"269\",\n",
      "    \"270\": \"270\",\n",
      "    \"271\": \"271\",\n",
      "    \"272\": \"272\",\n",
      "    \"273\": \"273\",\n",
      "    \"274\": \"274\",\n",
      "    \"275\": \"275\",\n",
      "    \"276\": \"276\",\n",
      "    \"277\": \"277\",\n",
      "    \"278\": \"278\",\n",
      "    \"279\": \"279\",\n",
      "    \"280\": \"280\",\n",
      "    \"281\": \"281\",\n",
      "    \"282\": \"282\",\n",
      "    \"283\": \"283\",\n",
      "    \"284\": \"284\",\n",
      "    \"285\": \"285\",\n",
      "    \"286\": \"286\",\n",
      "    \"287\": \"287\",\n",
      "    \"288\": \"288\",\n",
      "    \"289\": \"289\",\n",
      "    \"290\": \"290\",\n",
      "    \"291\": \"291\",\n",
      "    \"292\": \"292\",\n",
      "    \"293\": \"293\",\n",
      "    \"294\": \"294\",\n",
      "    \"295\": \"295\",\n",
      "    \"296\": \"296\",\n",
      "    \"297\": \"297\",\n",
      "    \"298\": \"298\",\n",
      "    \"299\": \"299\",\n",
      "    \"300\": \"300\",\n",
      "    \"301\": \"301\",\n",
      "    \"302\": \"302\",\n",
      "    \"303\": \"303\",\n",
      "    \"304\": \"304\",\n",
      "    \"305\": \"305\",\n",
      "    \"306\": \"306\",\n",
      "    \"307\": \"307\",\n",
      "    \"308\": \"308\",\n",
      "    \"309\": \"309\",\n",
      "    \"310\": \"310\",\n",
      "    \"311\": \"311\",\n",
      "    \"312\": \"312\",\n",
      "    \"313\": \"313\",\n",
      "    \"314\": \"314\",\n",
      "    \"315\": \"315\",\n",
      "    \"316\": \"316\",\n",
      "    \"317\": \"317\",\n",
      "    \"318\": \"318\",\n",
      "    \"319\": \"319\",\n",
      "    \"320\": \"320\",\n",
      "    \"321\": \"321\",\n",
      "    \"322\": \"322\",\n",
      "    \"323\": \"323\",\n",
      "    \"324\": \"324\",\n",
      "    \"325\": \"325\",\n",
      "    \"326\": \"326\",\n",
      "    \"327\": \"327\",\n",
      "    \"328\": \"328\",\n",
      "    \"329\": \"329\",\n",
      "    \"330\": \"330\",\n",
      "    \"331\": \"331\",\n",
      "    \"332\": \"332\",\n",
      "    \"333\": \"333\",\n",
      "    \"334\": \"334\",\n",
      "    \"335\": \"335\",\n",
      "    \"336\": \"336\",\n",
      "    \"337\": \"337\",\n",
      "    \"338\": \"338\",\n",
      "    \"339\": \"339\",\n",
      "    \"340\": \"340\",\n",
      "    \"341\": \"341\",\n",
      "    \"342\": \"342\",\n",
      "    \"343\": \"343\",\n",
      "    \"344\": \"344\",\n",
      "    \"345\": \"345\",\n",
      "    \"346\": \"346\",\n",
      "    \"347\": \"347\",\n",
      "    \"348\": \"348\",\n",
      "    \"349\": \"349\",\n",
      "    \"350\": \"350\",\n",
      "    \"351\": \"351\",\n",
      "    \"352\": \"352\",\n",
      "    \"353\": \"353\",\n",
      "    \"354\": \"354\",\n",
      "    \"355\": \"355\",\n",
      "    \"356\": \"356\",\n",
      "    \"357\": \"357\",\n",
      "    \"358\": \"358\",\n",
      "    \"359\": \"359\",\n",
      "    \"360\": \"360\",\n",
      "    \"361\": \"361\",\n",
      "    \"362\": \"362\",\n",
      "    \"363\": \"363\",\n",
      "    \"364\": \"364\",\n",
      "    \"365\": \"365\",\n",
      "    \"366\": \"366\",\n",
      "    \"367\": \"367\",\n",
      "    \"368\": \"368\",\n",
      "    \"369\": \"369\",\n",
      "    \"370\": \"370\",\n",
      "    \"371\": \"371\",\n",
      "    \"372\": \"372\",\n",
      "    \"373\": \"373\",\n",
      "    \"374\": \"374\",\n",
      "    \"375\": \"375\",\n",
      "    \"376\": \"376\",\n",
      "    \"377\": \"377\",\n",
      "    \"378\": \"378\",\n",
      "    \"379\": \"379\",\n",
      "    \"380\": \"380\",\n",
      "    \"381\": \"381\",\n",
      "    \"382\": \"382\",\n",
      "    \"383\": \"383\",\n",
      "    \"384\": \"384\",\n",
      "    \"385\": \"385\",\n",
      "    \"386\": \"386\",\n",
      "    \"387\": \"387\",\n",
      "    \"388\": \"388\",\n",
      "    \"389\": \"389\",\n",
      "    \"390\": \"390\",\n",
      "    \"391\": \"391\",\n",
      "    \"392\": \"392\",\n",
      "    \"393\": \"393\",\n",
      "    \"394\": \"394\",\n",
      "    \"395\": \"395\",\n",
      "    \"396\": \"396\",\n",
      "    \"397\": \"397\",\n",
      "    \"398\": \"398\",\n",
      "    \"399\": \"399\",\n",
      "    \"400\": \"400\",\n",
      "    \"401\": \"401\",\n",
      "    \"402\": \"402\",\n",
      "    \"403\": \"403\",\n",
      "    \"404\": \"404\",\n",
      "    \"405\": \"405\",\n",
      "    \"406\": \"406\",\n",
      "    \"407\": \"407\",\n",
      "    \"408\": \"408\",\n",
      "    \"409\": \"409\",\n",
      "    \"410\": \"410\",\n",
      "    \"411\": \"411\",\n",
      "    \"412\": \"412\",\n",
      "    \"413\": \"413\",\n",
      "    \"414\": \"414\",\n",
      "    \"415\": \"415\",\n",
      "    \"416\": \"416\",\n",
      "    \"417\": \"417\",\n",
      "    \"418\": \"418\",\n",
      "    \"419\": \"419\",\n",
      "    \"420\": \"420\",\n",
      "    \"421\": \"421\",\n",
      "    \"422\": \"422\",\n",
      "    \"423\": \"423\",\n",
      "    \"424\": \"424\",\n",
      "    \"425\": \"425\",\n",
      "    \"426\": \"426\",\n",
      "    \"427\": \"427\",\n",
      "    \"428\": \"428\",\n",
      "    \"429\": \"429\",\n",
      "    \"430\": \"430\",\n",
      "    \"431\": \"431\",\n",
      "    \"432\": \"432\",\n",
      "    \"433\": \"433\",\n",
      "    \"434\": \"434\",\n",
      "    \"435\": \"435\",\n",
      "    \"436\": \"436\",\n",
      "    \"437\": \"437\",\n",
      "    \"438\": \"438\",\n",
      "    \"439\": \"439\",\n",
      "    \"440\": \"440\",\n",
      "    \"441\": \"441\",\n",
      "    \"442\": \"442\",\n",
      "    \"443\": \"443\",\n",
      "    \"444\": \"444\",\n",
      "    \"445\": \"445\",\n",
      "    \"446\": \"446\",\n",
      "    \"447\": \"447\",\n",
      "    \"448\": \"448\",\n",
      "    \"449\": \"449\",\n",
      "    \"450\": \"450\",\n",
      "    \"451\": \"451\",\n",
      "    \"452\": \"452\",\n",
      "    \"453\": \"453\",\n",
      "    \"454\": \"454\",\n",
      "    \"455\": \"455\",\n",
      "    \"456\": \"456\",\n",
      "    \"457\": \"457\",\n",
      "    \"458\": \"458\",\n",
      "    \"459\": \"459\",\n",
      "    \"460\": \"460\",\n",
      "    \"461\": \"461\",\n",
      "    \"462\": \"462\",\n",
      "    \"463\": \"463\",\n",
      "    \"464\": \"464\",\n",
      "    \"465\": \"465\",\n",
      "    \"466\": \"466\",\n",
      "    \"467\": \"467\",\n",
      "    \"468\": \"468\",\n",
      "    \"469\": \"469\",\n",
      "    \"470\": \"470\",\n",
      "    \"471\": \"471\",\n",
      "    \"472\": \"472\",\n",
      "    \"473\": \"473\",\n",
      "    \"474\": \"474\",\n",
      "    \"475\": \"475\",\n",
      "    \"476\": \"476\",\n",
      "    \"477\": \"477\",\n",
      "    \"478\": \"478\",\n",
      "    \"479\": \"479\",\n",
      "    \"480\": \"480\",\n",
      "    \"481\": \"481\",\n",
      "    \"482\": \"482\",\n",
      "    \"483\": \"483\",\n",
      "    \"484\": \"484\",\n",
      "    \"485\": \"485\",\n",
      "    \"486\": \"486\",\n",
      "    \"487\": \"487\",\n",
      "    \"488\": \"488\",\n",
      "    \"489\": \"489\",\n",
      "    \"490\": \"490\",\n",
      "    \"491\": \"491\",\n",
      "    \"492\": \"492\",\n",
      "    \"493\": \"493\",\n",
      "    \"494\": \"494\",\n",
      "    \"495\": \"495\",\n",
      "    \"496\": \"496\",\n",
      "    \"497\": \"497\",\n",
      "    \"498\": \"498\",\n",
      "    \"499\": \"499\",\n",
      "    \"500\": \"500\",\n",
      "    \"501\": \"501\",\n",
      "    \"502\": \"502\",\n",
      "    \"503\": \"503\",\n",
      "    \"504\": \"504\",\n",
      "    \"505\": \"505\",\n",
      "    \"506\": \"506\",\n",
      "    \"507\": \"507\",\n",
      "    \"508\": \"508\",\n",
      "    \"509\": \"509\",\n",
      "    \"510\": \"510\",\n",
      "    \"511\": \"511\",\n",
      "    \"512\": \"512\",\n",
      "    \"513\": \"513\",\n",
      "    \"514\": \"514\",\n",
      "    \"515\": \"515\",\n",
      "    \"516\": \"516\",\n",
      "    \"517\": \"517\",\n",
      "    \"518\": \"518\",\n",
      "    \"519\": \"519\",\n",
      "    \"520\": \"520\",\n",
      "    \"521\": \"521\",\n",
      "    \"522\": \"522\",\n",
      "    \"523\": \"523\",\n",
      "    \"524\": \"524\",\n",
      "    \"525\": \"525\",\n",
      "    \"526\": \"526\",\n",
      "    \"527\": \"527\",\n",
      "    \"528\": \"528\",\n",
      "    \"529\": \"529\",\n",
      "    \"530\": \"530\",\n",
      "    \"531\": \"531\",\n",
      "    \"532\": \"532\",\n",
      "    \"533\": \"533\",\n",
      "    \"534\": \"534\",\n",
      "    \"535\": \"535\",\n",
      "    \"536\": \"536\",\n",
      "    \"537\": \"537\",\n",
      "    \"538\": \"538\",\n",
      "    \"539\": \"539\",\n",
      "    \"540\": \"540\",\n",
      "    \"541\": \"541\",\n",
      "    \"542\": \"542\",\n",
      "    \"543\": \"543\",\n",
      "    \"544\": \"544\",\n",
      "    \"545\": \"545\",\n",
      "    \"546\": \"546\",\n",
      "    \"547\": \"547\",\n",
      "    \"548\": \"548\",\n",
      "    \"549\": \"549\",\n",
      "    \"550\": \"550\",\n",
      "    \"551\": \"551\",\n",
      "    \"552\": \"552\",\n",
      "    \"553\": \"553\",\n",
      "    \"554\": \"554\",\n",
      "    \"555\": \"555\",\n",
      "    \"556\": \"556\",\n",
      "    \"557\": \"557\",\n",
      "    \"558\": \"558\",\n",
      "    \"559\": \"559\",\n",
      "    \"560\": \"560\",\n",
      "    \"561\": \"561\",\n",
      "    \"562\": \"562\",\n",
      "    \"563\": \"563\",\n",
      "    \"564\": \"564\",\n",
      "    \"565\": \"565\",\n",
      "    \"566\": \"566\",\n",
      "    \"567\": \"567\",\n",
      "    \"568\": \"568\",\n",
      "    \"569\": \"569\",\n",
      "    \"570\": \"570\",\n",
      "    \"571\": \"571\",\n",
      "    \"572\": \"572\",\n",
      "    \"573\": \"573\",\n",
      "    \"574\": \"574\",\n",
      "    \"575\": \"575\",\n",
      "    \"576\": \"576\",\n",
      "    \"577\": \"577\",\n",
      "    \"578\": \"578\",\n",
      "    \"579\": \"579\",\n",
      "    \"580\": \"580\",\n",
      "    \"581\": \"581\",\n",
      "    \"582\": \"582\",\n",
      "    \"583\": \"583\",\n",
      "    \"584\": \"584\",\n",
      "    \"585\": \"585\",\n",
      "    \"586\": \"586\",\n",
      "    \"587\": \"587\",\n",
      "    \"588\": \"588\",\n",
      "    \"589\": \"589\",\n",
      "    \"590\": \"590\",\n",
      "    \"591\": \"591\",\n",
      "    \"592\": \"592\",\n",
      "    \"593\": \"593\",\n",
      "    \"594\": \"594\",\n",
      "    \"595\": \"595\",\n",
      "    \"596\": \"596\",\n",
      "    \"597\": \"597\",\n",
      "    \"598\": \"598\",\n",
      "    \"599\": \"599\",\n",
      "    \"600\": \"600\",\n",
      "    \"601\": \"601\",\n",
      "    \"602\": \"602\",\n",
      "    \"603\": \"603\",\n",
      "    \"604\": \"604\",\n",
      "    \"605\": \"605\",\n",
      "    \"606\": \"606\",\n",
      "    \"607\": \"607\",\n",
      "    \"608\": \"608\",\n",
      "    \"609\": \"609\",\n",
      "    \"610\": \"610\",\n",
      "    \"611\": \"611\",\n",
      "    \"612\": \"612\",\n",
      "    \"613\": \"613\",\n",
      "    \"614\": \"614\",\n",
      "    \"615\": \"615\",\n",
      "    \"616\": \"616\",\n",
      "    \"617\": \"617\",\n",
      "    \"618\": \"618\",\n",
      "    \"619\": \"619\",\n",
      "    \"620\": \"620\",\n",
      "    \"621\": \"621\",\n",
      "    \"622\": \"622\",\n",
      "    \"623\": \"623\",\n",
      "    \"624\": \"624\",\n",
      "    \"625\": \"625\",\n",
      "    \"626\": \"626\",\n",
      "    \"627\": \"627\",\n",
      "    \"628\": \"628\",\n",
      "    \"629\": \"629\",\n",
      "    \"630\": \"630\",\n",
      "    \"631\": \"631\",\n",
      "    \"632\": \"632\",\n",
      "    \"633\": \"633\",\n",
      "    \"634\": \"634\",\n",
      "    \"635\": \"635\",\n",
      "    \"636\": \"636\",\n",
      "    \"637\": \"637\",\n",
      "    \"638\": \"638\",\n",
      "    \"639\": \"639\",\n",
      "    \"640\": \"640\",\n",
      "    \"641\": \"641\",\n",
      "    \"642\": \"642\",\n",
      "    \"643\": \"643\",\n",
      "    \"644\": \"644\",\n",
      "    \"645\": \"645\",\n",
      "    \"646\": \"646\",\n",
      "    \"647\": \"647\",\n",
      "    \"648\": \"648\",\n",
      "    \"649\": \"649\",\n",
      "    \"650\": \"650\",\n",
      "    \"651\": \"651\",\n",
      "    \"652\": \"652\",\n",
      "    \"653\": \"653\",\n",
      "    \"654\": \"654\",\n",
      "    \"655\": \"655\",\n",
      "    \"656\": \"656\",\n",
      "    \"657\": \"657\",\n",
      "    \"658\": \"658\",\n",
      "    \"659\": \"659\",\n",
      "    \"660\": \"660\",\n",
      "    \"661\": \"661\",\n",
      "    \"662\": \"662\",\n",
      "    \"663\": \"663\",\n",
      "    \"664\": \"664\",\n",
      "    \"665\": \"665\",\n",
      "    \"666\": \"666\",\n",
      "    \"667\": \"667\",\n",
      "    \"668\": \"668\",\n",
      "    \"669\": \"669\",\n",
      "    \"670\": \"670\",\n",
      "    \"671\": \"671\",\n",
      "    \"672\": \"672\",\n",
      "    \"673\": \"673\",\n",
      "    \"674\": \"674\",\n",
      "    \"675\": \"675\",\n",
      "    \"676\": \"676\",\n",
      "    \"677\": \"677\",\n",
      "    \"678\": \"678\",\n",
      "    \"679\": \"679\",\n",
      "    \"680\": \"680\",\n",
      "    \"681\": \"681\",\n",
      "    \"682\": \"682\",\n",
      "    \"683\": \"683\",\n",
      "    \"684\": \"684\",\n",
      "    \"685\": \"685\",\n",
      "    \"686\": \"686\",\n",
      "    \"687\": \"687\",\n",
      "    \"688\": \"688\",\n",
      "    \"689\": \"689\",\n",
      "    \"690\": \"690\",\n",
      "    \"691\": \"691\",\n",
      "    \"692\": \"692\",\n",
      "    \"693\": \"693\",\n",
      "    \"694\": \"694\",\n",
      "    \"695\": \"695\",\n",
      "    \"696\": \"696\",\n",
      "    \"697\": \"697\",\n",
      "    \"698\": \"698\",\n",
      "    \"699\": \"699\",\n",
      "    \"700\": \"700\",\n",
      "    \"701\": \"701\",\n",
      "    \"702\": \"702\",\n",
      "    \"703\": \"703\",\n",
      "    \"704\": \"704\",\n",
      "    \"705\": \"705\",\n",
      "    \"706\": \"706\",\n",
      "    \"707\": \"707\",\n",
      "    \"708\": \"708\",\n",
      "    \"709\": \"709\",\n",
      "    \"710\": \"710\",\n",
      "    \"711\": \"711\",\n",
      "    \"712\": \"712\",\n",
      "    \"713\": \"713\",\n",
      "    \"714\": \"714\",\n",
      "    \"715\": \"715\",\n",
      "    \"716\": \"716\",\n",
      "    \"717\": \"717\",\n",
      "    \"718\": \"718\",\n",
      "    \"719\": \"719\",\n",
      "    \"720\": \"720\",\n",
      "    \"721\": \"721\",\n",
      "    \"722\": \"722\",\n",
      "    \"723\": \"723\",\n",
      "    \"724\": \"724\",\n",
      "    \"725\": \"725\",\n",
      "    \"726\": \"726\",\n",
      "    \"727\": \"727\",\n",
      "    \"728\": \"728\",\n",
      "    \"729\": \"729\",\n",
      "    \"730\": \"730\",\n",
      "    \"731\": \"731\",\n",
      "    \"732\": \"732\",\n",
      "    \"733\": \"733\",\n",
      "    \"734\": \"734\",\n",
      "    \"735\": \"735\",\n",
      "    \"736\": \"736\",\n",
      "    \"737\": \"737\",\n",
      "    \"738\": \"738\",\n",
      "    \"739\": \"739\",\n",
      "    \"740\": \"740\",\n",
      "    \"741\": \"741\",\n",
      "    \"742\": \"742\",\n",
      "    \"743\": \"743\",\n",
      "    \"744\": \"744\",\n",
      "    \"745\": \"745\",\n",
      "    \"746\": \"746\",\n",
      "    \"747\": \"747\",\n",
      "    \"748\": \"748\",\n",
      "    \"749\": \"749\",\n",
      "    \"750\": \"750\",\n",
      "    \"751\": \"751\",\n",
      "    \"752\": \"752\",\n",
      "    \"753\": \"753\",\n",
      "    \"754\": \"754\",\n",
      "    \"755\": \"755\",\n",
      "    \"756\": \"756\",\n",
      "    \"757\": \"757\",\n",
      "    \"758\": \"758\",\n",
      "    \"759\": \"759\",\n",
      "    \"760\": \"760\",\n",
      "    \"761\": \"761\",\n",
      "    \"762\": \"762\",\n",
      "    \"763\": \"763\",\n",
      "    \"764\": \"764\",\n",
      "    \"765\": \"765\",\n",
      "    \"766\": \"766\",\n",
      "    \"767\": \"767\",\n",
      "    \"768\": \"768\",\n",
      "    \"769\": \"769\",\n",
      "    \"770\": \"770\",\n",
      "    \"771\": \"771\",\n",
      "    \"772\": \"772\",\n",
      "    \"773\": \"773\",\n",
      "    \"774\": \"774\",\n",
      "    \"775\": \"775\",\n",
      "    \"776\": \"776\",\n",
      "    \"777\": \"777\",\n",
      "    \"778\": \"778\",\n",
      "    \"779\": \"779\",\n",
      "    \"780\": \"780\",\n",
      "    \"781\": \"781\",\n",
      "    \"782\": \"782\",\n",
      "    \"783\": \"783\",\n",
      "    \"784\": \"784\",\n",
      "    \"785\": \"785\",\n",
      "    \"786\": \"786\",\n",
      "    \"787\": \"787\",\n",
      "    \"788\": \"788\",\n",
      "    \"789\": \"789\",\n",
      "    \"790\": \"790\",\n",
      "    \"791\": \"791\",\n",
      "    \"792\": \"792\",\n",
      "    \"793\": \"793\",\n",
      "    \"794\": \"794\",\n",
      "    \"795\": \"795\",\n",
      "    \"796\": \"796\",\n",
      "    \"797\": \"797\",\n",
      "    \"798\": \"798\",\n",
      "    \"799\": \"799\",\n",
      "    \"800\": \"800\",\n",
      "    \"801\": \"801\",\n",
      "    \"802\": \"802\",\n",
      "    \"803\": \"803\",\n",
      "    \"804\": \"804\",\n",
      "    \"805\": \"805\",\n",
      "    \"806\": \"806\",\n",
      "    \"807\": \"807\",\n",
      "    \"808\": \"808\",\n",
      "    \"809\": \"809\",\n",
      "    \"810\": \"810\",\n",
      "    \"811\": \"811\",\n",
      "    \"812\": \"812\",\n",
      "    \"813\": \"813\",\n",
      "    \"814\": \"814\",\n",
      "    \"815\": \"815\",\n",
      "    \"816\": \"816\",\n",
      "    \"817\": \"817\",\n",
      "    \"818\": \"818\",\n",
      "    \"819\": \"819\",\n",
      "    \"820\": \"820\",\n",
      "    \"821\": \"821\",\n",
      "    \"822\": \"822\",\n",
      "    \"823\": \"823\",\n",
      "    \"824\": \"824\",\n",
      "    \"825\": \"825\",\n",
      "    \"826\": \"826\",\n",
      "    \"827\": \"827\",\n",
      "    \"828\": \"828\",\n",
      "    \"829\": \"829\",\n",
      "    \"830\": \"830\",\n",
      "    \"831\": \"831\",\n",
      "    \"832\": \"832\",\n",
      "    \"833\": \"833\",\n",
      "    \"834\": \"834\",\n",
      "    \"835\": \"835\",\n",
      "    \"836\": \"836\",\n",
      "    \"837\": \"837\",\n",
      "    \"838\": \"838\",\n",
      "    \"839\": \"839\",\n",
      "    \"840\": \"840\",\n",
      "    \"841\": \"841\",\n",
      "    \"842\": \"842\",\n",
      "    \"843\": \"843\",\n",
      "    \"844\": \"844\",\n",
      "    \"845\": \"845\",\n",
      "    \"846\": \"846\",\n",
      "    \"847\": \"847\",\n",
      "    \"848\": \"848\",\n",
      "    \"849\": \"849\",\n",
      "    \"850\": \"850\",\n",
      "    \"851\": \"851\",\n",
      "    \"852\": \"852\",\n",
      "    \"853\": \"853\",\n",
      "    \"854\": \"854\",\n",
      "    \"855\": \"855\",\n",
      "    \"856\": \"856\",\n",
      "    \"857\": \"857\",\n",
      "    \"858\": \"858\",\n",
      "    \"859\": \"859\",\n",
      "    \"860\": \"860\",\n",
      "    \"861\": \"861\",\n",
      "    \"862\": \"862\",\n",
      "    \"863\": \"863\",\n",
      "    \"864\": \"864\",\n",
      "    \"865\": \"865\",\n",
      "    \"866\": \"866\",\n",
      "    \"867\": \"867\",\n",
      "    \"868\": \"868\",\n",
      "    \"869\": \"869\",\n",
      "    \"870\": \"870\",\n",
      "    \"871\": \"871\",\n",
      "    \"872\": \"872\",\n",
      "    \"873\": \"873\",\n",
      "    \"874\": \"874\",\n",
      "    \"875\": \"875\",\n",
      "    \"876\": \"876\",\n",
      "    \"877\": \"877\",\n",
      "    \"878\": \"878\",\n",
      "    \"879\": \"879\",\n",
      "    \"880\": \"880\",\n",
      "    \"881\": \"881\",\n",
      "    \"882\": \"882\",\n",
      "    \"883\": \"883\",\n",
      "    \"884\": \"884\",\n",
      "    \"885\": \"885\",\n",
      "    \"886\": \"886\",\n",
      "    \"887\": \"887\",\n",
      "    \"888\": \"888\",\n",
      "    \"889\": \"889\",\n",
      "    \"890\": \"890\",\n",
      "    \"891\": \"891\",\n",
      "    \"892\": \"892\",\n",
      "    \"893\": \"893\",\n",
      "    \"894\": \"894\",\n",
      "    \"895\": \"895\",\n",
      "    \"896\": \"896\",\n",
      "    \"897\": \"897\",\n",
      "    \"898\": \"898\",\n",
      "    \"899\": \"899\",\n",
      "    \"900\": \"900\",\n",
      "    \"901\": \"901\",\n",
      "    \"902\": \"902\",\n",
      "    \"903\": \"903\",\n",
      "    \"904\": \"904\",\n",
      "    \"905\": \"905\",\n",
      "    \"906\": \"906\",\n",
      "    \"907\": \"907\",\n",
      "    \"908\": \"908\",\n",
      "    \"909\": \"909\",\n",
      "    \"910\": \"910\",\n",
      "    \"911\": \"911\",\n",
      "    \"912\": \"912\",\n",
      "    \"913\": \"913\",\n",
      "    \"914\": \"914\",\n",
      "    \"915\": \"915\",\n",
      "    \"916\": \"916\",\n",
      "    \"917\": \"917\",\n",
      "    \"918\": \"918\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"100\": 100,\n",
      "    \"101\": 101,\n",
      "    \"102\": 102,\n",
      "    \"103\": 103,\n",
      "    \"104\": 104,\n",
      "    \"105\": 105,\n",
      "    \"106\": 106,\n",
      "    \"107\": 107,\n",
      "    \"108\": 108,\n",
      "    \"109\": 109,\n",
      "    \"11\": 11,\n",
      "    \"110\": 110,\n",
      "    \"111\": 111,\n",
      "    \"112\": 112,\n",
      "    \"113\": 113,\n",
      "    \"114\": 114,\n",
      "    \"115\": 115,\n",
      "    \"116\": 116,\n",
      "    \"117\": 117,\n",
      "    \"118\": 118,\n",
      "    \"119\": 119,\n",
      "    \"12\": 12,\n",
      "    \"120\": 120,\n",
      "    \"121\": 121,\n",
      "    \"122\": 122,\n",
      "    \"123\": 123,\n",
      "    \"124\": 124,\n",
      "    \"125\": 125,\n",
      "    \"126\": 126,\n",
      "    \"127\": 127,\n",
      "    \"128\": 128,\n",
      "    \"129\": 129,\n",
      "    \"13\": 13,\n",
      "    \"130\": 130,\n",
      "    \"131\": 131,\n",
      "    \"132\": 132,\n",
      "    \"133\": 133,\n",
      "    \"134\": 134,\n",
      "    \"135\": 135,\n",
      "    \"136\": 136,\n",
      "    \"137\": 137,\n",
      "    \"138\": 138,\n",
      "    \"139\": 139,\n",
      "    \"14\": 14,\n",
      "    \"140\": 140,\n",
      "    \"141\": 141,\n",
      "    \"142\": 142,\n",
      "    \"143\": 143,\n",
      "    \"144\": 144,\n",
      "    \"145\": 145,\n",
      "    \"146\": 146,\n",
      "    \"147\": 147,\n",
      "    \"148\": 148,\n",
      "    \"149\": 149,\n",
      "    \"15\": 15,\n",
      "    \"150\": 150,\n",
      "    \"151\": 151,\n",
      "    \"152\": 152,\n",
      "    \"153\": 153,\n",
      "    \"154\": 154,\n",
      "    \"155\": 155,\n",
      "    \"156\": 156,\n",
      "    \"157\": 157,\n",
      "    \"158\": 158,\n",
      "    \"159\": 159,\n",
      "    \"16\": 16,\n",
      "    \"160\": 160,\n",
      "    \"161\": 161,\n",
      "    \"162\": 162,\n",
      "    \"163\": 163,\n",
      "    \"164\": 164,\n",
      "    \"165\": 165,\n",
      "    \"166\": 166,\n",
      "    \"167\": 167,\n",
      "    \"168\": 168,\n",
      "    \"169\": 169,\n",
      "    \"17\": 17,\n",
      "    \"170\": 170,\n",
      "    \"171\": 171,\n",
      "    \"172\": 172,\n",
      "    \"173\": 173,\n",
      "    \"174\": 174,\n",
      "    \"175\": 175,\n",
      "    \"176\": 176,\n",
      "    \"177\": 177,\n",
      "    \"178\": 178,\n",
      "    \"179\": 179,\n",
      "    \"18\": 18,\n",
      "    \"180\": 180,\n",
      "    \"181\": 181,\n",
      "    \"182\": 182,\n",
      "    \"183\": 183,\n",
      "    \"184\": 184,\n",
      "    \"185\": 185,\n",
      "    \"186\": 186,\n",
      "    \"187\": 187,\n",
      "    \"188\": 188,\n",
      "    \"189\": 189,\n",
      "    \"19\": 19,\n",
      "    \"190\": 190,\n",
      "    \"191\": 191,\n",
      "    \"192\": 192,\n",
      "    \"193\": 193,\n",
      "    \"194\": 194,\n",
      "    \"195\": 195,\n",
      "    \"196\": 196,\n",
      "    \"197\": 197,\n",
      "    \"198\": 198,\n",
      "    \"199\": 199,\n",
      "    \"2\": 2,\n",
      "    \"20\": 20,\n",
      "    \"200\": 200,\n",
      "    \"201\": 201,\n",
      "    \"202\": 202,\n",
      "    \"203\": 203,\n",
      "    \"204\": 204,\n",
      "    \"205\": 205,\n",
      "    \"206\": 206,\n",
      "    \"207\": 207,\n",
      "    \"208\": 208,\n",
      "    \"209\": 209,\n",
      "    \"21\": 21,\n",
      "    \"210\": 210,\n",
      "    \"211\": 211,\n",
      "    \"212\": 212,\n",
      "    \"213\": 213,\n",
      "    \"214\": 214,\n",
      "    \"215\": 215,\n",
      "    \"216\": 216,\n",
      "    \"217\": 217,\n",
      "    \"218\": 218,\n",
      "    \"219\": 219,\n",
      "    \"22\": 22,\n",
      "    \"220\": 220,\n",
      "    \"221\": 221,\n",
      "    \"222\": 222,\n",
      "    \"223\": 223,\n",
      "    \"224\": 224,\n",
      "    \"225\": 225,\n",
      "    \"226\": 226,\n",
      "    \"227\": 227,\n",
      "    \"228\": 228,\n",
      "    \"229\": 229,\n",
      "    \"23\": 23,\n",
      "    \"230\": 230,\n",
      "    \"231\": 231,\n",
      "    \"232\": 232,\n",
      "    \"233\": 233,\n",
      "    \"234\": 234,\n",
      "    \"235\": 235,\n",
      "    \"236\": 236,\n",
      "    \"237\": 237,\n",
      "    \"238\": 238,\n",
      "    \"239\": 239,\n",
      "    \"24\": 24,\n",
      "    \"240\": 240,\n",
      "    \"241\": 241,\n",
      "    \"242\": 242,\n",
      "    \"243\": 243,\n",
      "    \"244\": 244,\n",
      "    \"245\": 245,\n",
      "    \"246\": 246,\n",
      "    \"247\": 247,\n",
      "    \"248\": 248,\n",
      "    \"249\": 249,\n",
      "    \"25\": 25,\n",
      "    \"250\": 250,\n",
      "    \"251\": 251,\n",
      "    \"252\": 252,\n",
      "    \"253\": 253,\n",
      "    \"254\": 254,\n",
      "    \"255\": 255,\n",
      "    \"256\": 256,\n",
      "    \"257\": 257,\n",
      "    \"258\": 258,\n",
      "    \"259\": 259,\n",
      "    \"26\": 26,\n",
      "    \"260\": 260,\n",
      "    \"261\": 261,\n",
      "    \"262\": 262,\n",
      "    \"263\": 263,\n",
      "    \"264\": 264,\n",
      "    \"265\": 265,\n",
      "    \"266\": 266,\n",
      "    \"267\": 267,\n",
      "    \"268\": 268,\n",
      "    \"269\": 269,\n",
      "    \"27\": 27,\n",
      "    \"270\": 270,\n",
      "    \"271\": 271,\n",
      "    \"272\": 272,\n",
      "    \"273\": 273,\n",
      "    \"274\": 274,\n",
      "    \"275\": 275,\n",
      "    \"276\": 276,\n",
      "    \"277\": 277,\n",
      "    \"278\": 278,\n",
      "    \"279\": 279,\n",
      "    \"28\": 28,\n",
      "    \"280\": 280,\n",
      "    \"281\": 281,\n",
      "    \"282\": 282,\n",
      "    \"283\": 283,\n",
      "    \"284\": 284,\n",
      "    \"285\": 285,\n",
      "    \"286\": 286,\n",
      "    \"287\": 287,\n",
      "    \"288\": 288,\n",
      "    \"289\": 289,\n",
      "    \"29\": 29,\n",
      "    \"290\": 290,\n",
      "    \"291\": 291,\n",
      "    \"292\": 292,\n",
      "    \"293\": 293,\n",
      "    \"294\": 294,\n",
      "    \"295\": 295,\n",
      "    \"296\": 296,\n",
      "    \"297\": 297,\n",
      "    \"298\": 298,\n",
      "    \"299\": 299,\n",
      "    \"3\": 3,\n",
      "    \"30\": 30,\n",
      "    \"300\": 300,\n",
      "    \"301\": 301,\n",
      "    \"302\": 302,\n",
      "    \"303\": 303,\n",
      "    \"304\": 304,\n",
      "    \"305\": 305,\n",
      "    \"306\": 306,\n",
      "    \"307\": 307,\n",
      "    \"308\": 308,\n",
      "    \"309\": 309,\n",
      "    \"31\": 31,\n",
      "    \"310\": 310,\n",
      "    \"311\": 311,\n",
      "    \"312\": 312,\n",
      "    \"313\": 313,\n",
      "    \"314\": 314,\n",
      "    \"315\": 315,\n",
      "    \"316\": 316,\n",
      "    \"317\": 317,\n",
      "    \"318\": 318,\n",
      "    \"319\": 319,\n",
      "    \"32\": 32,\n",
      "    \"320\": 320,\n",
      "    \"321\": 321,\n",
      "    \"322\": 322,\n",
      "    \"323\": 323,\n",
      "    \"324\": 324,\n",
      "    \"325\": 325,\n",
      "    \"326\": 326,\n",
      "    \"327\": 327,\n",
      "    \"328\": 328,\n",
      "    \"329\": 329,\n",
      "    \"33\": 33,\n",
      "    \"330\": 330,\n",
      "    \"331\": 331,\n",
      "    \"332\": 332,\n",
      "    \"333\": 333,\n",
      "    \"334\": 334,\n",
      "    \"335\": 335,\n",
      "    \"336\": 336,\n",
      "    \"337\": 337,\n",
      "    \"338\": 338,\n",
      "    \"339\": 339,\n",
      "    \"34\": 34,\n",
      "    \"340\": 340,\n",
      "    \"341\": 341,\n",
      "    \"342\": 342,\n",
      "    \"343\": 343,\n",
      "    \"344\": 344,\n",
      "    \"345\": 345,\n",
      "    \"346\": 346,\n",
      "    \"347\": 347,\n",
      "    \"348\": 348,\n",
      "    \"349\": 349,\n",
      "    \"35\": 35,\n",
      "    \"350\": 350,\n",
      "    \"351\": 351,\n",
      "    \"352\": 352,\n",
      "    \"353\": 353,\n",
      "    \"354\": 354,\n",
      "    \"355\": 355,\n",
      "    \"356\": 356,\n",
      "    \"357\": 357,\n",
      "    \"358\": 358,\n",
      "    \"359\": 359,\n",
      "    \"36\": 36,\n",
      "    \"360\": 360,\n",
      "    \"361\": 361,\n",
      "    \"362\": 362,\n",
      "    \"363\": 363,\n",
      "    \"364\": 364,\n",
      "    \"365\": 365,\n",
      "    \"366\": 366,\n",
      "    \"367\": 367,\n",
      "    \"368\": 368,\n",
      "    \"369\": 369,\n",
      "    \"37\": 37,\n",
      "    \"370\": 370,\n",
      "    \"371\": 371,\n",
      "    \"372\": 372,\n",
      "    \"373\": 373,\n",
      "    \"374\": 374,\n",
      "    \"375\": 375,\n",
      "    \"376\": 376,\n",
      "    \"377\": 377,\n",
      "    \"378\": 378,\n",
      "    \"379\": 379,\n",
      "    \"38\": 38,\n",
      "    \"380\": 380,\n",
      "    \"381\": 381,\n",
      "    \"382\": 382,\n",
      "    \"383\": 383,\n",
      "    \"384\": 384,\n",
      "    \"385\": 385,\n",
      "    \"386\": 386,\n",
      "    \"387\": 387,\n",
      "    \"388\": 388,\n",
      "    \"389\": 389,\n",
      "    \"39\": 39,\n",
      "    \"390\": 390,\n",
      "    \"391\": 391,\n",
      "    \"392\": 392,\n",
      "    \"393\": 393,\n",
      "    \"394\": 394,\n",
      "    \"395\": 395,\n",
      "    \"396\": 396,\n",
      "    \"397\": 397,\n",
      "    \"398\": 398,\n",
      "    \"399\": 399,\n",
      "    \"4\": 4,\n",
      "    \"40\": 40,\n",
      "    \"400\": 400,\n",
      "    \"401\": 401,\n",
      "    \"402\": 402,\n",
      "    \"403\": 403,\n",
      "    \"404\": 404,\n",
      "    \"405\": 405,\n",
      "    \"406\": 406,\n",
      "    \"407\": 407,\n",
      "    \"408\": 408,\n",
      "    \"409\": 409,\n",
      "    \"41\": 41,\n",
      "    \"410\": 410,\n",
      "    \"411\": 411,\n",
      "    \"412\": 412,\n",
      "    \"413\": 413,\n",
      "    \"414\": 414,\n",
      "    \"415\": 415,\n",
      "    \"416\": 416,\n",
      "    \"417\": 417,\n",
      "    \"418\": 418,\n",
      "    \"419\": 419,\n",
      "    \"42\": 42,\n",
      "    \"420\": 420,\n",
      "    \"421\": 421,\n",
      "    \"422\": 422,\n",
      "    \"423\": 423,\n",
      "    \"424\": 424,\n",
      "    \"425\": 425,\n",
      "    \"426\": 426,\n",
      "    \"427\": 427,\n",
      "    \"428\": 428,\n",
      "    \"429\": 429,\n",
      "    \"43\": 43,\n",
      "    \"430\": 430,\n",
      "    \"431\": 431,\n",
      "    \"432\": 432,\n",
      "    \"433\": 433,\n",
      "    \"434\": 434,\n",
      "    \"435\": 435,\n",
      "    \"436\": 436,\n",
      "    \"437\": 437,\n",
      "    \"438\": 438,\n",
      "    \"439\": 439,\n",
      "    \"44\": 44,\n",
      "    \"440\": 440,\n",
      "    \"441\": 441,\n",
      "    \"442\": 442,\n",
      "    \"443\": 443,\n",
      "    \"444\": 444,\n",
      "    \"445\": 445,\n",
      "    \"446\": 446,\n",
      "    \"447\": 447,\n",
      "    \"448\": 448,\n",
      "    \"449\": 449,\n",
      "    \"45\": 45,\n",
      "    \"450\": 450,\n",
      "    \"451\": 451,\n",
      "    \"452\": 452,\n",
      "    \"453\": 453,\n",
      "    \"454\": 454,\n",
      "    \"455\": 455,\n",
      "    \"456\": 456,\n",
      "    \"457\": 457,\n",
      "    \"458\": 458,\n",
      "    \"459\": 459,\n",
      "    \"46\": 46,\n",
      "    \"460\": 460,\n",
      "    \"461\": 461,\n",
      "    \"462\": 462,\n",
      "    \"463\": 463,\n",
      "    \"464\": 464,\n",
      "    \"465\": 465,\n",
      "    \"466\": 466,\n",
      "    \"467\": 467,\n",
      "    \"468\": 468,\n",
      "    \"469\": 469,\n",
      "    \"47\": 47,\n",
      "    \"470\": 470,\n",
      "    \"471\": 471,\n",
      "    \"472\": 472,\n",
      "    \"473\": 473,\n",
      "    \"474\": 474,\n",
      "    \"475\": 475,\n",
      "    \"476\": 476,\n",
      "    \"477\": 477,\n",
      "    \"478\": 478,\n",
      "    \"479\": 479,\n",
      "    \"48\": 48,\n",
      "    \"480\": 480,\n",
      "    \"481\": 481,\n",
      "    \"482\": 482,\n",
      "    \"483\": 483,\n",
      "    \"484\": 484,\n",
      "    \"485\": 485,\n",
      "    \"486\": 486,\n",
      "    \"487\": 487,\n",
      "    \"488\": 488,\n",
      "    \"489\": 489,\n",
      "    \"49\": 49,\n",
      "    \"490\": 490,\n",
      "    \"491\": 491,\n",
      "    \"492\": 492,\n",
      "    \"493\": 493,\n",
      "    \"494\": 494,\n",
      "    \"495\": 495,\n",
      "    \"496\": 496,\n",
      "    \"497\": 497,\n",
      "    \"498\": 498,\n",
      "    \"499\": 499,\n",
      "    \"5\": 5,\n",
      "    \"50\": 50,\n",
      "    \"500\": 500,\n",
      "    \"501\": 501,\n",
      "    \"502\": 502,\n",
      "    \"503\": 503,\n",
      "    \"504\": 504,\n",
      "    \"505\": 505,\n",
      "    \"506\": 506,\n",
      "    \"507\": 507,\n",
      "    \"508\": 508,\n",
      "    \"509\": 509,\n",
      "    \"51\": 51,\n",
      "    \"510\": 510,\n",
      "    \"511\": 511,\n",
      "    \"512\": 512,\n",
      "    \"513\": 513,\n",
      "    \"514\": 514,\n",
      "    \"515\": 515,\n",
      "    \"516\": 516,\n",
      "    \"517\": 517,\n",
      "    \"518\": 518,\n",
      "    \"519\": 519,\n",
      "    \"52\": 52,\n",
      "    \"520\": 520,\n",
      "    \"521\": 521,\n",
      "    \"522\": 522,\n",
      "    \"523\": 523,\n",
      "    \"524\": 524,\n",
      "    \"525\": 525,\n",
      "    \"526\": 526,\n",
      "    \"527\": 527,\n",
      "    \"528\": 528,\n",
      "    \"529\": 529,\n",
      "    \"53\": 53,\n",
      "    \"530\": 530,\n",
      "    \"531\": 531,\n",
      "    \"532\": 532,\n",
      "    \"533\": 533,\n",
      "    \"534\": 534,\n",
      "    \"535\": 535,\n",
      "    \"536\": 536,\n",
      "    \"537\": 537,\n",
      "    \"538\": 538,\n",
      "    \"539\": 539,\n",
      "    \"54\": 54,\n",
      "    \"540\": 540,\n",
      "    \"541\": 541,\n",
      "    \"542\": 542,\n",
      "    \"543\": 543,\n",
      "    \"544\": 544,\n",
      "    \"545\": 545,\n",
      "    \"546\": 546,\n",
      "    \"547\": 547,\n",
      "    \"548\": 548,\n",
      "    \"549\": 549,\n",
      "    \"55\": 55,\n",
      "    \"550\": 550,\n",
      "    \"551\": 551,\n",
      "    \"552\": 552,\n",
      "    \"553\": 553,\n",
      "    \"554\": 554,\n",
      "    \"555\": 555,\n",
      "    \"556\": 556,\n",
      "    \"557\": 557,\n",
      "    \"558\": 558,\n",
      "    \"559\": 559,\n",
      "    \"56\": 56,\n",
      "    \"560\": 560,\n",
      "    \"561\": 561,\n",
      "    \"562\": 562,\n",
      "    \"563\": 563,\n",
      "    \"564\": 564,\n",
      "    \"565\": 565,\n",
      "    \"566\": 566,\n",
      "    \"567\": 567,\n",
      "    \"568\": 568,\n",
      "    \"569\": 569,\n",
      "    \"57\": 57,\n",
      "    \"570\": 570,\n",
      "    \"571\": 571,\n",
      "    \"572\": 572,\n",
      "    \"573\": 573,\n",
      "    \"574\": 574,\n",
      "    \"575\": 575,\n",
      "    \"576\": 576,\n",
      "    \"577\": 577,\n",
      "    \"578\": 578,\n",
      "    \"579\": 579,\n",
      "    \"58\": 58,\n",
      "    \"580\": 580,\n",
      "    \"581\": 581,\n",
      "    \"582\": 582,\n",
      "    \"583\": 583,\n",
      "    \"584\": 584,\n",
      "    \"585\": 585,\n",
      "    \"586\": 586,\n",
      "    \"587\": 587,\n",
      "    \"588\": 588,\n",
      "    \"589\": 589,\n",
      "    \"59\": 59,\n",
      "    \"590\": 590,\n",
      "    \"591\": 591,\n",
      "    \"592\": 592,\n",
      "    \"593\": 593,\n",
      "    \"594\": 594,\n",
      "    \"595\": 595,\n",
      "    \"596\": 596,\n",
      "    \"597\": 597,\n",
      "    \"598\": 598,\n",
      "    \"599\": 599,\n",
      "    \"6\": 6,\n",
      "    \"60\": 60,\n",
      "    \"600\": 600,\n",
      "    \"601\": 601,\n",
      "    \"602\": 602,\n",
      "    \"603\": 603,\n",
      "    \"604\": 604,\n",
      "    \"605\": 605,\n",
      "    \"606\": 606,\n",
      "    \"607\": 607,\n",
      "    \"608\": 608,\n",
      "    \"609\": 609,\n",
      "    \"61\": 61,\n",
      "    \"610\": 610,\n",
      "    \"611\": 611,\n",
      "    \"612\": 612,\n",
      "    \"613\": 613,\n",
      "    \"614\": 614,\n",
      "    \"615\": 615,\n",
      "    \"616\": 616,\n",
      "    \"617\": 617,\n",
      "    \"618\": 618,\n",
      "    \"619\": 619,\n",
      "    \"62\": 62,\n",
      "    \"620\": 620,\n",
      "    \"621\": 621,\n",
      "    \"622\": 622,\n",
      "    \"623\": 623,\n",
      "    \"624\": 624,\n",
      "    \"625\": 625,\n",
      "    \"626\": 626,\n",
      "    \"627\": 627,\n",
      "    \"628\": 628,\n",
      "    \"629\": 629,\n",
      "    \"63\": 63,\n",
      "    \"630\": 630,\n",
      "    \"631\": 631,\n",
      "    \"632\": 632,\n",
      "    \"633\": 633,\n",
      "    \"634\": 634,\n",
      "    \"635\": 635,\n",
      "    \"636\": 636,\n",
      "    \"637\": 637,\n",
      "    \"638\": 638,\n",
      "    \"639\": 639,\n",
      "    \"64\": 64,\n",
      "    \"640\": 640,\n",
      "    \"641\": 641,\n",
      "    \"642\": 642,\n",
      "    \"643\": 643,\n",
      "    \"644\": 644,\n",
      "    \"645\": 645,\n",
      "    \"646\": 646,\n",
      "    \"647\": 647,\n",
      "    \"648\": 648,\n",
      "    \"649\": 649,\n",
      "    \"65\": 65,\n",
      "    \"650\": 650,\n",
      "    \"651\": 651,\n",
      "    \"652\": 652,\n",
      "    \"653\": 653,\n",
      "    \"654\": 654,\n",
      "    \"655\": 655,\n",
      "    \"656\": 656,\n",
      "    \"657\": 657,\n",
      "    \"658\": 658,\n",
      "    \"659\": 659,\n",
      "    \"66\": 66,\n",
      "    \"660\": 660,\n",
      "    \"661\": 661,\n",
      "    \"662\": 662,\n",
      "    \"663\": 663,\n",
      "    \"664\": 664,\n",
      "    \"665\": 665,\n",
      "    \"666\": 666,\n",
      "    \"667\": 667,\n",
      "    \"668\": 668,\n",
      "    \"669\": 669,\n",
      "    \"67\": 67,\n",
      "    \"670\": 670,\n",
      "    \"671\": 671,\n",
      "    \"672\": 672,\n",
      "    \"673\": 673,\n",
      "    \"674\": 674,\n",
      "    \"675\": 675,\n",
      "    \"676\": 676,\n",
      "    \"677\": 677,\n",
      "    \"678\": 678,\n",
      "    \"679\": 679,\n",
      "    \"68\": 68,\n",
      "    \"680\": 680,\n",
      "    \"681\": 681,\n",
      "    \"682\": 682,\n",
      "    \"683\": 683,\n",
      "    \"684\": 684,\n",
      "    \"685\": 685,\n",
      "    \"686\": 686,\n",
      "    \"687\": 687,\n",
      "    \"688\": 688,\n",
      "    \"689\": 689,\n",
      "    \"69\": 69,\n",
      "    \"690\": 690,\n",
      "    \"691\": 691,\n",
      "    \"692\": 692,\n",
      "    \"693\": 693,\n",
      "    \"694\": 694,\n",
      "    \"695\": 695,\n",
      "    \"696\": 696,\n",
      "    \"697\": 697,\n",
      "    \"698\": 698,\n",
      "    \"699\": 699,\n",
      "    \"7\": 7,\n",
      "    \"70\": 70,\n",
      "    \"700\": 700,\n",
      "    \"701\": 701,\n",
      "    \"702\": 702,\n",
      "    \"703\": 703,\n",
      "    \"704\": 704,\n",
      "    \"705\": 705,\n",
      "    \"706\": 706,\n",
      "    \"707\": 707,\n",
      "    \"708\": 708,\n",
      "    \"709\": 709,\n",
      "    \"71\": 71,\n",
      "    \"710\": 710,\n",
      "    \"711\": 711,\n",
      "    \"712\": 712,\n",
      "    \"713\": 713,\n",
      "    \"714\": 714,\n",
      "    \"715\": 715,\n",
      "    \"716\": 716,\n",
      "    \"717\": 717,\n",
      "    \"718\": 718,\n",
      "    \"719\": 719,\n",
      "    \"72\": 72,\n",
      "    \"720\": 720,\n",
      "    \"721\": 721,\n",
      "    \"722\": 722,\n",
      "    \"723\": 723,\n",
      "    \"724\": 724,\n",
      "    \"725\": 725,\n",
      "    \"726\": 726,\n",
      "    \"727\": 727,\n",
      "    \"728\": 728,\n",
      "    \"729\": 729,\n",
      "    \"73\": 73,\n",
      "    \"730\": 730,\n",
      "    \"731\": 731,\n",
      "    \"732\": 732,\n",
      "    \"733\": 733,\n",
      "    \"734\": 734,\n",
      "    \"735\": 735,\n",
      "    \"736\": 736,\n",
      "    \"737\": 737,\n",
      "    \"738\": 738,\n",
      "    \"739\": 739,\n",
      "    \"74\": 74,\n",
      "    \"740\": 740,\n",
      "    \"741\": 741,\n",
      "    \"742\": 742,\n",
      "    \"743\": 743,\n",
      "    \"744\": 744,\n",
      "    \"745\": 745,\n",
      "    \"746\": 746,\n",
      "    \"747\": 747,\n",
      "    \"748\": 748,\n",
      "    \"749\": 749,\n",
      "    \"75\": 75,\n",
      "    \"750\": 750,\n",
      "    \"751\": 751,\n",
      "    \"752\": 752,\n",
      "    \"753\": 753,\n",
      "    \"754\": 754,\n",
      "    \"755\": 755,\n",
      "    \"756\": 756,\n",
      "    \"757\": 757,\n",
      "    \"758\": 758,\n",
      "    \"759\": 759,\n",
      "    \"76\": 76,\n",
      "    \"760\": 760,\n",
      "    \"761\": 761,\n",
      "    \"762\": 762,\n",
      "    \"763\": 763,\n",
      "    \"764\": 764,\n",
      "    \"765\": 765,\n",
      "    \"766\": 766,\n",
      "    \"767\": 767,\n",
      "    \"768\": 768,\n",
      "    \"769\": 769,\n",
      "    \"77\": 77,\n",
      "    \"770\": 770,\n",
      "    \"771\": 771,\n",
      "    \"772\": 772,\n",
      "    \"773\": 773,\n",
      "    \"774\": 774,\n",
      "    \"775\": 775,\n",
      "    \"776\": 776,\n",
      "    \"777\": 777,\n",
      "    \"778\": 778,\n",
      "    \"779\": 779,\n",
      "    \"78\": 78,\n",
      "    \"780\": 780,\n",
      "    \"781\": 781,\n",
      "    \"782\": 782,\n",
      "    \"783\": 783,\n",
      "    \"784\": 784,\n",
      "    \"785\": 785,\n",
      "    \"786\": 786,\n",
      "    \"787\": 787,\n",
      "    \"788\": 788,\n",
      "    \"789\": 789,\n",
      "    \"79\": 79,\n",
      "    \"790\": 790,\n",
      "    \"791\": 791,\n",
      "    \"792\": 792,\n",
      "    \"793\": 793,\n",
      "    \"794\": 794,\n",
      "    \"795\": 795,\n",
      "    \"796\": 796,\n",
      "    \"797\": 797,\n",
      "    \"798\": 798,\n",
      "    \"799\": 799,\n",
      "    \"8\": 8,\n",
      "    \"80\": 80,\n",
      "    \"800\": 800,\n",
      "    \"801\": 801,\n",
      "    \"802\": 802,\n",
      "    \"803\": 803,\n",
      "    \"804\": 804,\n",
      "    \"805\": 805,\n",
      "    \"806\": 806,\n",
      "    \"807\": 807,\n",
      "    \"808\": 808,\n",
      "    \"809\": 809,\n",
      "    \"81\": 81,\n",
      "    \"810\": 810,\n",
      "    \"811\": 811,\n",
      "    \"812\": 812,\n",
      "    \"813\": 813,\n",
      "    \"814\": 814,\n",
      "    \"815\": 815,\n",
      "    \"816\": 816,\n",
      "    \"817\": 817,\n",
      "    \"818\": 818,\n",
      "    \"819\": 819,\n",
      "    \"82\": 82,\n",
      "    \"820\": 820,\n",
      "    \"821\": 821,\n",
      "    \"822\": 822,\n",
      "    \"823\": 823,\n",
      "    \"824\": 824,\n",
      "    \"825\": 825,\n",
      "    \"826\": 826,\n",
      "    \"827\": 827,\n",
      "    \"828\": 828,\n",
      "    \"829\": 829,\n",
      "    \"83\": 83,\n",
      "    \"830\": 830,\n",
      "    \"831\": 831,\n",
      "    \"832\": 832,\n",
      "    \"833\": 833,\n",
      "    \"834\": 834,\n",
      "    \"835\": 835,\n",
      "    \"836\": 836,\n",
      "    \"837\": 837,\n",
      "    \"838\": 838,\n",
      "    \"839\": 839,\n",
      "    \"84\": 84,\n",
      "    \"840\": 840,\n",
      "    \"841\": 841,\n",
      "    \"842\": 842,\n",
      "    \"843\": 843,\n",
      "    \"844\": 844,\n",
      "    \"845\": 845,\n",
      "    \"846\": 846,\n",
      "    \"847\": 847,\n",
      "    \"848\": 848,\n",
      "    \"849\": 849,\n",
      "    \"85\": 85,\n",
      "    \"850\": 850,\n",
      "    \"851\": 851,\n",
      "    \"852\": 852,\n",
      "    \"853\": 853,\n",
      "    \"854\": 854,\n",
      "    \"855\": 855,\n",
      "    \"856\": 856,\n",
      "    \"857\": 857,\n",
      "    \"858\": 858,\n",
      "    \"859\": 859,\n",
      "    \"86\": 86,\n",
      "    \"860\": 860,\n",
      "    \"861\": 861,\n",
      "    \"862\": 862,\n",
      "    \"863\": 863,\n",
      "    \"864\": 864,\n",
      "    \"865\": 865,\n",
      "    \"866\": 866,\n",
      "    \"867\": 867,\n",
      "    \"868\": 868,\n",
      "    \"869\": 869,\n",
      "    \"87\": 87,\n",
      "    \"870\": 870,\n",
      "    \"871\": 871,\n",
      "    \"872\": 872,\n",
      "    \"873\": 873,\n",
      "    \"874\": 874,\n",
      "    \"875\": 875,\n",
      "    \"876\": 876,\n",
      "    \"877\": 877,\n",
      "    \"878\": 878,\n",
      "    \"879\": 879,\n",
      "    \"88\": 88,\n",
      "    \"880\": 880,\n",
      "    \"881\": 881,\n",
      "    \"882\": 882,\n",
      "    \"883\": 883,\n",
      "    \"884\": 884,\n",
      "    \"885\": 885,\n",
      "    \"886\": 886,\n",
      "    \"887\": 887,\n",
      "    \"888\": 888,\n",
      "    \"889\": 889,\n",
      "    \"89\": 89,\n",
      "    \"890\": 890,\n",
      "    \"891\": 891,\n",
      "    \"892\": 892,\n",
      "    \"893\": 893,\n",
      "    \"894\": 894,\n",
      "    \"895\": 895,\n",
      "    \"896\": 896,\n",
      "    \"897\": 897,\n",
      "    \"898\": 898,\n",
      "    \"899\": 899,\n",
      "    \"9\": 9,\n",
      "    \"90\": 90,\n",
      "    \"900\": 900,\n",
      "    \"901\": 901,\n",
      "    \"902\": 902,\n",
      "    \"903\": 903,\n",
      "    \"904\": 904,\n",
      "    \"905\": 905,\n",
      "    \"906\": 906,\n",
      "    \"907\": 907,\n",
      "    \"908\": 908,\n",
      "    \"909\": 909,\n",
      "    \"91\": 91,\n",
      "    \"910\": 910,\n",
      "    \"911\": 911,\n",
      "    \"912\": 912,\n",
      "    \"913\": 913,\n",
      "    \"914\": 914,\n",
      "    \"915\": 915,\n",
      "    \"916\": 916,\n",
      "    \"917\": 917,\n",
      "    \"918\": 918,\n",
      "    \"92\": 92,\n",
      "    \"93\": 93,\n",
      "    \"94\": 94,\n",
      "    \"95\": 95,\n",
      "    \"96\": 96,\n",
      "    \"97\": 97,\n",
      "    \"98\": 98,\n",
      "    \"99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_type\": \"standard\",\n",
      "  \"lm_head\": null,\n",
      "  \"mask_token_id\": 4,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"max_tokens_per_msa\": 16384,\n",
      "  \"model_type\": \"rnamsm\",\n",
      "  \"null_token_id\": 5,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"num_labels\": 919,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.3\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 26\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "--- Creating Datasets ---\n",
      "Detected max_length=200 in the dataset, using it as the max_length.\n",
      "Loading data from tfb_prediction_dataset\\train_tfb.npy...\n",
      "Loaded 4400000 examples from tfb_prediction_dataset\\train_tfb.npy\n",
      "Detected shuffle=True, shuffling the examples...\n",
      "Detected max_examples=100000, truncating the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:07<00:00, 534.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys have consistent sequence lengths, skipping padding and truncation.\n",
      "Detected max_length=200 in the dataset, using it as the max_length.\n",
      "Loading data from tfb_prediction_dataset\\test_tfb.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "C:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tfb_prediction_dataset\\\\test_tfb.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Creating Datasets ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m train_set = DeepSEADataset(\n\u001B[32m     21\u001B[39m     data_source=TRAIN_FILE,\n\u001B[32m     22\u001B[39m     tokenizer=tokenizer,\n\u001B[32m   (...)\u001B[39m\u001B[32m     25\u001B[39m     force_padding=\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# DeepSEA does not require padding\u001B[39;00m\n\u001B[32m     26\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m test_set = DeepSEADataset(\n\u001B[32m     28\u001B[39m     data_source=TEST_FILE,\n\u001B[32m     29\u001B[39m     tokenizer=tokenizer,\n\u001B[32m     30\u001B[39m     max_length=MAX_LENGTH,\n\u001B[32m     31\u001B[39m     max_examples=MAX_EXAMPLES,\n\u001B[32m     32\u001B[39m     force_padding=\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# DeepSEA does not require padding\u001B[39;00m\n\u001B[32m     33\u001B[39m )\n\u001B[32m     34\u001B[39m valid_set = DeepSEADataset(\n\u001B[32m     35\u001B[39m     data_source=VALID_FILE,\n\u001B[32m     36\u001B[39m     tokenizer=tokenizer,\n\u001B[32m   (...)\u001B[39m\u001B[32m     39\u001B[39m     force_padding=\u001B[38;5;28;01mFalse\u001B[39;00m,  \u001B[38;5;66;03m# DeepSEA does not require padding\u001B[39;00m\n\u001B[32m     40\u001B[39m ) \u001B[38;5;28;01mif\u001B[39;00m os.path.exists(VALID_FILE) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     42\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Initialization Complete ---\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mDeepSEADataset.__init__\u001B[39m\u001B[34m(self, data_source, tokenizer, max_length, **kwargs)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_source, tokenizer, max_length=\u001B[38;5;28;01mNone\u001B[39;00m, **kwargs):\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(data_source, tokenizer, max_length, **kwargs)\n\u001B[32m      8\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs.items():\n\u001B[32m      9\u001B[39m         \u001B[38;5;28mself\u001B[39m.metadata[key] = value\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\OneDrive - University of Exeter\\AIProjects\\OmniGenomeBench\\omnigenbench\\src\\abc\\abstract_dataset.py:171\u001B[39m, in \u001B[36mOmniDataset.__init__\u001B[39m\u001B[34m(self, data_source, tokenizer, max_length, **kwargs)\u001B[39m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data_source \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    170\u001B[39m     fprint(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLoading data from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_source\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m     \u001B[38;5;28mself\u001B[39m.load_data_source(data_source, **kwargs)\n\u001B[32m    172\u001B[39m     \u001B[38;5;28mself\u001B[39m._preprocessing()\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m tqdm.tqdm(\u001B[38;5;28mself\u001B[39m.examples):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\OneDrive - University of Exeter\\AIProjects\\OmniGenomeBench\\omnigenbench\\src\\abc\\abstract_dataset.py:439\u001B[39m, in \u001B[36mOmniDataset.load_data_source\u001B[39m\u001B[34m(self, data_source, **kwargs)\u001B[39m\n\u001B[32m    436\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m    438\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data_source.endswith(\u001B[33m\"\u001B[39m\u001B[33m.npy\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m439\u001B[39m     data = np.load(data_source, allow_pickle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    440\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, np.ndarray):\n\u001B[32m    441\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\py312\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[39m\n\u001B[32m    425\u001B[39m     own_fid = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m427\u001B[39m     fid = stack.enter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m    428\u001B[39m     own_fid = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    430\u001B[39m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'tfb_prediction_dataset\\\\test_tfb.npy'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Training the Model\n\nWith everything set up, we can now train the model. We'll use the `AccelerateTrainer` for a streamlined and efficient training loop. The trainer handles the training loop, evaluation, early stopping, and device placement automatically."
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-08-06T15:29:12.437384700Z",
     "start_time": "2025-08-06T09:06:49.185621Z"
    }
   },
   "source": [
    "# Set random seed for reproducibility across all libraries\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE) if valid_set else None\n",
    "\n",
    "# Define the metric for evaluation. For DeepSEA, ROC AUC is a standard metric.\n",
    "metrics = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=valid_loader, # Use validation set for early stopping and checkpointing\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    compute_metrics=metrics,\n",
    "    patience=PATIENCE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"--- Starting Training ---\")\n",
    "metrics = trainer.train()\n",
    "print(\"--- Training Finished ---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\multimolecule\\models\\rnamsm\\modeling_rnamsm.py:166: UserWarning: Single sequence input detected, RNA-MSM works best with MSA inputs.\n",
      "  warn(\"Single sequence input detected, RNA-MSM works best with MSA inputs.\")\n",
      "Evaluating: 100%|██████████| 500/500 [00:16<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.5128501615091636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 Loss:   0%|          | 0/6250 [00:00<?, ?it/s]C:\\Users\\hengu\\miniconda3\\envs\\py312\\Lib\\site-packages\\multimolecule\\models\\rnamsm\\modeling_rnamsm.py:166: UserWarning: Single sequence input detected, RNA-MSM works best with MSA inputs.\n",
      "  warn(\"Single sequence input detected, RNA-MSM works best with MSA inputs.\")\n",
      "Epoch 1/30 Loss: 0.6747:  84%|████████▎ | 5232/6250 [08:22<01:36, 10.59it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Evaluation\n\nAfter training is complete, the `AccelerateTrainer` automatically loads the best performing model (based on the validation set). We can now evaluate this model on the held-out test set to get a final, unbiased measure of its performance."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:29:12.437384700Z",
     "start_time": "2025-08-06T08:43:19.050860Z"
    }
   },
   "source": [
    "print(\"--- Evaluating on Test Set ---\")\n",
    "# The evaluation is automated by the trainer\n",
    "print(f\"All metrics:\", metrics)\n",
    "for metric in metrics['test']:\n",
    "    print(f\"Test metric:  {metric}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating on Test Set ---\n",
      "All metrics: {'valid': [{'roc_auc_score': 0.66544658242437}, {'roc_auc_score': 0.6510533985521894}, {'roc_auc_score': 0.6531804294068921}, {'roc_auc_score': 0.6499433313733695}, {'roc_auc_score': 0.6428444822852545}], 'best_valid': {'roc_auc_score': 0.66544658242437}, 'test': [{'roc_auc_score': 0.6833502729163198}]}\n",
      "Test metric:  {'roc_auc_score': 0.6833502729163198}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Inference Example\n\nFinally, let's see how to use the fine-tuned model to make a prediction on a new, unseen DNA sequence. This demonstrates the practical application of the trained model."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T15:29:12.438891900Z",
     "start_time": "2025-08-06T00:09:41.994583Z"
    }
   },
   "source": [
    "# Create a sample DNA sequence (must be at least MAX_LENGTH base pairs long)\n",
    "sample_sequence = \"AGCT\" * (MAX_LENGTH // 4) # Create a sequence of the required length\n",
    "\n",
    "# Prepare the sequence for the model (add spaces between characters)\n",
    "spaced_sequence = ' '.join(list(sample_sequence))\n",
    "inputs = tokenizer(spaced_sequence, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model.predict(inputs)\n",
    "\n",
    "# Get the predictions and probabilities\n",
    "predictions = outputs['predictions'].cpu().numpy().flatten()\n",
    "probabilities = outputs['probabilities'].cpu().numpy().flatten()\n",
    "\n",
    "print(f\"Input sequence length: {len(sample_sequence)} bp\")\n",
    "print(f\"Number of predicted labels: {len(predictions)}\")\n",
    "\n",
    "# Display predictions for the first 10 transcription factors\n",
    "print(\"\\n--- Predictions for the first 10 TFs ---\")\n",
    "for i in range(10):\n",
    "    pred_label = 'Binds' if predictions[i] == 1 else 'Does not bind'\n",
    "    print(f\"Label {i+1}: Prediction={pred_label}, Probability={probabilities[i]:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence length: 200 bp\n",
      "Number of predicted labels: 919\n",
      "\n",
      "--- Predictions for the first 10 TFs ---\n",
      "Label 1: Prediction=Does not bind, Probability=0.3310\n",
      "Label 2: Prediction=Does not bind, Probability=0.3695\n",
      "Label 3: Prediction=Does not bind, Probability=0.3184\n",
      "Label 4: Prediction=Does not bind, Probability=0.2379\n",
      "Label 5: Prediction=Binds, Probability=0.5397\n",
      "Label 6: Prediction=Does not bind, Probability=0.4741\n",
      "Label 7: Prediction=Does not bind, Probability=0.4068\n",
      "Label 8: Prediction=Does not bind, Probability=0.3112\n",
      "Label 9: Prediction=Does not bind, Probability=0.3325\n",
      "Label 10: Prediction=Does not bind, Probability=0.3347\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
