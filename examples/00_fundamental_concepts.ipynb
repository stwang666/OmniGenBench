{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f416cbc",
   "metadata": {},
   "source": [
    "# 🧬 OmniGenBench Fundamental Concepts Tutorial\n",
    "\n",
    "Welcome to the **OmniGenBench Fundamental Concepts Tutorial**! This is a comprehensive tutorial designed to help you understand the core concepts of genomic deep learning and the design philosophy of the OmniGenBench framework. We recommend learning this foundational tutorial before tackling any specific genomic prediction problems.\n",
    "\n",
    "> 🎯 **Learning Objectives**: Master core concepts of genomic deep learning, understand machine learning task classification, learn genomic foundation model principles, and familiarize yourself with OmniGenBench workflows\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Tutorial Outline\n",
    "\n",
    "1. **From Language Models to Genomic Foundation Models** - Understanding AI principles in biological applications\n",
    "2. **Machine Learning Task Classification** - How to formulate biological problems as ML tasks\n",
    "3. **Data, Tasks, and Model Relationships** - Guidelines for selecting appropriate tools\n",
    "4. **OmniGenBench Workflow** - Standardized four-step methodology\n",
    "5. **Practical Guidelines** - How to apply concepts to specific research problems\n",
    "\n",
    "This tutorial provides the theoretical foundation for all specific biological prediction tasks (such as translation efficiency prediction, transcription factor binding prediction, variant effect prediction, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cef18",
   "metadata": {},
   "source": [
    "## 🚀 Part 1: From Language Models to Genomic Foundation Models\n",
    "\n",
    "### 1.1 The Language Model Revolution\n",
    "\n",
    "In recent years, **Language Models (LMs)** such as BERT and GPT have revolutionized the field of natural language processing. These models learn intrinsic patterns of language—grammar, context, and even semantics—through pre-training on vast amounts of text data. This enables them to be \"fine-tuned\" for a wide range of specific tasks (e.g., translation, summarization, question answering).\n",
    "\n",
    "### 1.2 A New Paradigm in Biology: Genomic Foundation Models (GFMs)\n",
    "\n",
    "The same principles can be applied to biology. The \"language of life\" is written in RNA and DNA sequences using nucleotides (A, C, G, U/T) as \"letters.\" **Genomic Foundation Models (GFMs)**, such as **OmniGenome** (Yang et al., 2025), are large-scale models pre-trained on massive genomic sequence datasets.\n",
    "\n",
    "### 1.3 Analogical Comparison Table\n",
    "\n",
    "| Concept | Language Models | Genomic Foundation Models |\n",
    "|---------|-----------------|---------------------------|\n",
    "| **\"Letters\"** | English alphabet (A-Z) | Nucleotides (A, C, G, T/U) |\n",
    "| **\"Words\"** | English words | k-mers, functional domains |\n",
    "| **\"Sentences\"** | Sentences | Genes, transcripts |\n",
    "| **\"Documents\"** | Articles, books | Genomes, chromosomes |\n",
    "| **\"Grammar\"** | Grammatical rules | Biological laws (e.g., codon usage bias) |\n",
    "| **\"Semantics\"** | Meaning, intent | Biological function |\n",
    "| **Pre-training data** | Wikipedia, books | Genomic databases (NCBI, Ensembl) |\n",
    "| **Downstream tasks** | Translation, QA | Function prediction, variant effects |\n",
    "\n",
    "### 1.4 Why Are Genomic Foundation Models So Powerful?\n",
    "\n",
    "**OmniGenome** models have been pre-trained to learn:\n",
    "- 🧬 **Sequence patterns**: Conserved sequences, repetitive elements, functional domains\n",
    "- 🔗 **Contextual relationships**: Promoter-gene relationships, exon-intron structures\n",
    "- 📊 **Statistical features**: Nucleotide composition preferences, codon usage frequencies\n",
    "- 🎯 **Functional associations**: Latent relationships between sequence and function\n",
    "\n",
    "This pre-trained knowledge provides a powerful foundation for various downstream tasks, enabling excellent performance even on small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ed418",
   "metadata": {},
   "source": [
    "## 🎯 Part 2: Machine Learning Task Classification\n",
    "\n",
    "In genomics, we can categorize various biological problems into different machine learning task types. Understanding this classification is crucial for selecting appropriate model architectures and evaluation metrics.\n",
    "\n",
    "### 2.1 Four Main Task Types\n",
    "\n",
    "| Task Type | Description | Biological Example | Output Format |\n",
    "|-----------|-------------|-------------------|---------------|\n",
    "| **Sequence Classification** | Assign one or more labels to an entire sequence | Is this DNA sequence a promoter? (Yes/No) | Single label or label set |\n",
    "| **Sequence Regression** | Predict continuous numerical values for a sequence | What is this protein's stability score? (0.0-1.0) | Continuous values |\n",
    "| **Token Classification** | Assign a label to each nucleotide in a sequence | Which nucleotides in this gene are binding sites? | One label per position |\n",
    "| **Sequence-to-Sequence** | Transform input sequence into different output sequence | What is the secondary structure of this RNA? | Output sequence |\n",
    "\n",
    "### 2.2 Specific Task Mapping Examples\n",
    "\n",
    "#### 🔍 Sequence Classification Tasks\n",
    "- **Translation efficiency prediction**: High efficiency vs Low efficiency\n",
    "- **Transcription factor binding**: Binding vs Non-binding\n",
    "- **Promoter identification**: Promoter vs Background sequence\n",
    "- **Subcellular localization**: Nucleus vs Cytoplasm vs Mitochondria\n",
    "\n",
    "#### 📊 Sequence Regression Tasks\n",
    "- **Gene expression levels**: FPKM value prediction\n",
    "- **Protein stability**: ΔΔG value prediction\n",
    "- **Binding affinity**: Kd value prediction\n",
    "- **Enzyme activity**: kcat value prediction\n",
    "\n",
    "#### 🎯 Token Classification Tasks\n",
    "- **Transcription factor binding sites**: Per-nucleotide binding probability\n",
    "- **Splice site prediction**: Splice site boundary annotation\n",
    "- **RNA modification sites**: m6A modification position identification\n",
    "- **Protein secondary structure**: α-helix, β-sheet, random coil\n",
    "\n",
    "#### 🔄 Sequence-to-Sequence Tasks\n",
    "- **RNA secondary structure prediction**: Sequence → Structure notation\n",
    "- **Protein design**: Function description → Amino acid sequence\n",
    "- **Sequence optimization**: Wild-type → Optimized sequence\n",
    "- **Reverse translation**: Protein → DNA codon sequence\n",
    "\n",
    "### 2.3 Task Selection Guidelines\n",
    "\n",
    "**How to determine which task type your biological problem belongs to?**\n",
    "\n",
    "1. **Ask yourself**: What do I want to predict?\n",
    "   - Properties of entire sequence → **Sequence classification/regression**\n",
    "   - Properties of specific positions in sequence → **Token classification**\n",
    "   - New sequences → **Sequence-to-sequence**\n",
    "\n",
    "2. **Output type**: What format is my target output?\n",
    "   - Category labels (High/Low, Yes/No) → **Classification**\n",
    "   - Numerical values (0.1, 2.5, etc.) → **Regression**\n",
    "   - Multiple labels at sequence level → **Multi-label classification**\n",
    "   - Labels for each position → **Token classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e81e0",
   "metadata": {},
   "source": [
    "## 🔗 Part 3: Data, Task, and Model Relationships\n",
    "\n",
    "Successful genomic deep learning projects require correctly matching data formats, task types, and model architectures. OmniGenBench provides a systematic solution for this matching.\n",
    "\n",
    "### 3.1 OmniGenBench Model Family\n",
    "\n",
    "| Task Type | OmniGenBench Model | Data Format Requirements | Application Scenarios |\n",
    "|-----------|-------------------|-------------------------|----------------------|\n",
    "| **Sequence Classification** | `OmniModelForSequenceClassification` | `{sequence, label}` | Promoter identification, functional classification |\n",
    "| **Sequence Regression** | `OmniModelForSequenceRegression` | `{sequence, value}` | Expression levels, stability prediction |\n",
    "| **Token Classification** | `OmniModelForTokenClassification` | `{sequence, labels_per_position}` | Binding sites, modification sites |\n",
    "| **Multi-label Classification** | `OmniModelForMultiLabelClassification` | `{sequence, label_vector}` | Multi-functional, multi-localization prediction |\n",
    "\n",
    "### 3.2 Standard Dataset Formats\n",
    "\n",
    "#### Sequence Classification Data Format\n",
    "```csv\n",
    "sequence,label,id,split\n",
    "ATCGATCGATCG,1,seq_001,train\n",
    "GCGCGCGCGCGC,0,seq_002,valid\n",
    "```\n",
    "\n",
    "#### Sequence Regression Data Format\n",
    "```csv\n",
    "sequence,value,id,split\n",
    "ATCGATCGATCG,8.5,seq_001,train\n",
    "GCGCGCGCGCGC,2.3,seq_002,valid\n",
    "```\n",
    "\n",
    "#### Token Classification Data Format\n",
    "```csv\n",
    "sequence,labels,id,split\n",
    "ATCGATCGATCG,\"0,0,1,1,0,0,0,0,0,0,0,0\",seq_001,train\n",
    "```\n",
    "\n",
    "### 3.3 Model Selection Decision Tree\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"🧬 I have genomic sequence data\"] --> B{\"What do I want to predict?\"}\n",
    "    B -->|\"Category of entire sequence\"| C[\"📊 Sequence Classification\"]\n",
    "    B -->|\"Numerical value of entire sequence\"| D[\"📈 Sequence Regression\"]\n",
    "    B -->|\"Properties of each position in sequence\"| E[\"🎯 Token Classification\"]\n",
    "    B -->|\"Multiple simultaneous properties\"| F[\"🏷️ Multi-label Classification\"]\n",
    "    \n",
    "    C --> C1[\"OmniModelForSequenceClassification\"]\n",
    "    D --> D1[\"OmniModelForSequenceRegression\"]\n",
    "    E --> E1[\"OmniModelForTokenClassification\"]\n",
    "    F --> F1[\"OmniModelForMultiLabelClassification\"]\n",
    "    \n",
    "    style C1 fill:#e1f5fe\n",
    "    style D1 fill:#f3e5f5\n",
    "    style E1 fill:#e8f5e8\n",
    "    style F1 fill:#fff3e0\n",
    "```\n",
    "\n",
    "### 3.4 Practical Application Examples\n",
    "\n",
    "Let's look at some specific matching examples:\n",
    "\n",
    "**Case 1: Translation Efficiency Prediction**\n",
    "- 📝 **Question**: \"Is this mRNA sequence's translation efficiency high or low?\"\n",
    "- 🎯 **Task Type**: Sequence classification (binary classification)\n",
    "- 🤖 **Model Choice**: `OmniModelForSequenceClassification`\n",
    "- 📊 **Data Format**: `{sequence: \"AUGCCC...\", label: 1}`\n",
    "\n",
    "**Case 2: Gene Expression Level Prediction**\n",
    "- 📝 **Question**: \"What expression level will this promoter sequence produce?\"\n",
    "- 🎯 **Task Type**: Sequence regression\n",
    "- 🤖 **Model Choice**: `OmniModelForSequenceRegression`\n",
    "- 📊 **Data Format**: `{sequence: \"ATGCCC...\", value: 8.5}`\n",
    "\n",
    "**Case 3: Transcription Factor Binding Site Prediction**\n",
    "- 📝 **Question**: \"Which positions in this DNA sequence will bind specific transcription factors?\"\n",
    "- 🎯 **Task Type**: Token classification\n",
    "- 🤖 **Model Choice**: `OmniModelForTokenClassification`\n",
    "- 📊 **Data Format**: `{sequence: \"ATGCCC...\", labels: [0,0,1,1,0,0...]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08258f",
   "metadata": {},
   "source": [
    "## ⚙️ Part 4: OmniGenBench Standardized Workflow\n",
    "\n",
    "OmniGenBench follows a standardized four-step workflow that applies to all types of genomic prediction tasks. This consistency makes the learning curve gentler and code more reusable.\n",
    "\n",
    "### 4.1 Universal Four-Step Methodology\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"🧬 OmniGenBench Standard Workflow\"\n",
    "        A[\"📥 Step 1: Data Preparation<br/>Download and process datasets\"] --> B[\"🔧 Step 2: Model Initialization<br/>Load pre-trained genomic foundation models\"]\n",
    "        B --> C[\"🎓 Step 3: Model Training<br/>Fine-tune models on specific tasks\"]\n",
    "        C --> D[\"🔮 Step 4: Model Inference<br/>Use trained models for predictions\"]\n",
    "    end\n",
    "    \n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "### 4.2 Detailed Content for Each Step\n",
    "\n",
    "#### 📥 Step 1: Data Preparation\n",
    "- **Environment Setup**: Install necessary Python packages\n",
    "- **Data Acquisition**: Download or load datasets\n",
    "- **Data Preprocessing**: Sequence tokenization, label encoding\n",
    "- **Data Loading**: Create PyTorch DataLoaders\n",
    "\n",
    "```python\n",
    "# Typical data preparation code\n",
    "from omnigenbench import OmniTokenizer, OmniDatasetForSequenceClassification\n",
    "\n",
    "tokenizer = OmniTokenizer.from_pretrained(\"yangheng/OmniGenome-52M\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_huggingface(\n",
    "    dataset_name=\"your_dataset_name\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "```\n",
    "\n",
    "#### 🔧 Step 2: Model Initialization\n",
    "- **Tokenizer Loading**: Use the same tokenizer as data preparation\n",
    "- **Base Model Loading**: Load pre-trained OmniGenome models\n",
    "- **Task Adaptation**: Add task-specific output layers\n",
    "- **Model Configuration**: Set model parameters\n",
    "\n",
    "```python\n",
    "# Typical model initialization code\n",
    "from omnigenbench import OmniModelForSequenceClassification\n",
    "\n",
    "model = OmniModelForSequenceClassification(\n",
    "    model_name_or_path=\"yangheng/OmniGenome-52M\",\n",
    "    tokenizer=tokenizer,\n",
    "    num_labels=2  # Adjust based on task\n",
    ")\n",
    "```\n",
    "\n",
    "#### 🎓 Step 3: Model Training\n",
    "- **Training Configuration**: Set learning rate, batch size, and other hyperparameters\n",
    "- **Evaluation Metrics**: Choose appropriate performance evaluation metrics\n",
    "- **Training Loop**: Execute model fine-tuning\n",
    "- **Model Saving**: Save best-performing models\n",
    "\n",
    "```python\n",
    "# Typical model training code\n",
    "from omnigenbench import AccelerateTrainer, ClassificationMetric\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    compute_metrics=[ClassificationMetric().f1_score]\n",
    ")\n",
    "\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"my_finetuned_model\")\n",
    "```\n",
    "\n",
    "#### 🔮 Step 4: Model Inference\n",
    "- **Model Loading**: Load trained models\n",
    "- **Input Preprocessing**: Prepare new sequence data\n",
    "- **Prediction Generation**: Obtain model outputs\n",
    "- **Result Interpretation**: Parse and visualize prediction results\n",
    "\n",
    "```python\n",
    "# Typical model inference code\n",
    "from omnigenbench import ModelHub\n",
    "\n",
    "inference_model = ModelHub.load(\"path/to/my_finetuned_model\")\n",
    "outputs = inference_model.inference(new_sequence)\n",
    "prediction = outputs['predictions'][0]\n",
    "```\n",
    "\n",
    "### 4.3 Workflow Advantages\n",
    "\n",
    "This standardized workflow brings multiple advantages:\n",
    "\n",
    "✅ **Consistency**: All tasks use the same pattern, reducing learning costs  \n",
    "✅ **Reproducibility**: Standardized steps ensure reproducible results  \n",
    "✅ **Scalability**: Easy to adapt to new tasks and datasets  \n",
    "✅ **Best Practices**: Integrates proven machine learning best practices  \n",
    "✅ **Error Reduction**: Standardization reduces common errors  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb915c07",
   "metadata": {},
   "source": [
    "## 🧭 Part 5: Practical Application Guidelines\n",
    "\n",
    "Now that you've mastered the theoretical foundations, let's see how to apply these concepts to actual research projects.\n",
    "\n",
    "### 5.1 New Project Checklist\n",
    "\n",
    "Before starting any new genomic deep learning project, please answer the following questions:\n",
    "\n",
    "#### 🎯 Problem Definition\n",
    "- [ ] **Biological Question**: What specific biological problem do I want to solve?\n",
    "- [ ] **Prediction Target**: What output do I want to predict? (categories, numerical values, sequences, etc.)\n",
    "- [ ] **Application Scenario**: How will this model be used in practice?\n",
    "\n",
    "#### 📊 Data Assessment\n",
    "- [ ] **Data Source**: What type of data do I have? (DNA, RNA, proteins)\n",
    "- [ ] **Data Scale**: How many samples do I have? Is it sufficient for training deep learning models?\n",
    "- [ ] **Label Quality**: How were the labels generated? How reliable are they?\n",
    "- [ ] **Data Balance**: Is the distribution of different categories balanced?\n",
    "\n",
    "#### 🤖 Technical Choices\n",
    "- [ ] **Task Type**: Determine ML task type based on output format\n",
    "- [ ] **Model Selection**: Choose appropriate OmniGenBench model\n",
    "- [ ] **Evaluation Metrics**: Select suitable performance evaluation metrics\n",
    "- [ ] **Computational Resources**: Assess required computational resources\n",
    "\n",
    "### 5.2 Common Application Scenario Templates\n",
    "\n",
    "#### 🧬 Function Prediction Projects\n",
    "```\n",
    "Problem: Predict biological functions of DNA/RNA/protein sequences\n",
    "Task Type: Sequence classification\n",
    "Data Format: {sequence, functional_class}\n",
    "Model: OmniModelForSequenceClassification\n",
    "Evaluation: F1-score, accuracy, AUC\n",
    "```\n",
    "\n",
    "#### 📊 Quantitative Prediction Projects\n",
    "```\n",
    "Problem: Predict gene expression, protein activity, and other quantitative indicators\n",
    "Task Type: Sequence regression\n",
    "Data Format: {sequence, quantitative_value}\n",
    "Model: OmniModelForSequenceRegression\n",
    "Evaluation: MSE, MAE, Pearson correlation coefficient\n",
    "```\n",
    "\n",
    "#### 🎯 Site Identification Projects\n",
    "```\n",
    "Problem: Identify specific functional sites in sequences\n",
    "Task Type: Token classification\n",
    "Data Format: {sequence, position_labels}\n",
    "Model: OmniModelForTokenClassification\n",
    "Evaluation: Precision, recall, F1-score (site-level)\n",
    "```\n",
    "\n",
    "### 5.3 Performance Optimization Strategies\n",
    "\n",
    "#### 🚀 Techniques to Improve Model Performance\n",
    "\n",
    "1. **Data Quality Optimization**\n",
    "   - Clean low-quality sequences\n",
    "   - Remove duplicate samples\n",
    "   - Balance category distributions\n",
    "\n",
    "2. **Model Selection Strategy**\n",
    "   - Small datasets: Use OmniGenome-52M\n",
    "   - Large datasets/complex tasks: Use OmniGenome-186M\n",
    "   - Compare multiple pre-trained models\n",
    "\n",
    "3. **Hyperparameter Tuning**\n",
    "   - Learning rate: Usually start from 1e-5\n",
    "   - Batch size: Adjust based on GPU memory\n",
    "   - Sequence length: Balance information completeness and computational efficiency\n",
    "\n",
    "4. **Evaluation Strategy**\n",
    "   - Use biologically meaningful data splits\n",
    "   - Avoid data leakage (separate homologous sequences)\n",
    "   - Combine multiple evaluation metrics\n",
    "\n",
    "### 5.4 Troubleshooting Guide\n",
    "\n",
    "#### ❌ Common Problems and Solutions\n",
    "\n",
    "**Problem 1: Poor Model Performance**\n",
    "- ✅ Check data quality and label correctness\n",
    "- ✅ Try different pre-trained models\n",
    "- ✅ Adjust sequence length and batch size\n",
    "- ✅ Increase training data or use data augmentation\n",
    "\n",
    "**Problem 2: Out of Memory**\n",
    "- ✅ Reduce batch size\n",
    "- ✅ Shorten input sequence length\n",
    "- ✅ Use gradient accumulation\n",
    "- ✅ Choose smaller models\n",
    "\n",
    "**Problem 3: Slow Training**\n",
    "- ✅ Use GPU acceleration\n",
    "- ✅ Optimize data loading pipeline\n",
    "- ✅ Use mixed precision training\n",
    "- ✅ Reduce validation frequency\n",
    "\n",
    "**Problem 4: Overfitting**\n",
    "- ✅ Increase regularization\n",
    "- ✅ Use early stopping mechanism\n",
    "- ✅ Data augmentation\n",
    "- ✅ Reduce model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9fbc",
   "metadata": {},
   "source": [
    "## 🎓 Summary and Next Steps\n",
    "\n",
    "Congratulations! You have mastered the core concepts of OmniGenBench and genomic deep learning.\n",
    "\n",
    "### ✅ What You've Learned\n",
    "\n",
    "1. **Theoretical Foundation**: The evolution from language models to genomic foundation models\n",
    "2. **Task Classification**: Four main machine learning task types and their biological applications\n",
    "3. **Tool Mapping**: How to select appropriate data formats, task types, and model architectures\n",
    "4. **Standard Process**: OmniGenBench's four-step workflow\n",
    "5. **Practical Skills**: Project planning, performance optimization, and troubleshooting\n",
    "\n",
    "### 🚀 Next Learning Pathways\n",
    "\n",
    "Now you can choose to learn specific application tutorials:\n",
    "\n",
    "#### 🧬 Sequence Classification Tasks\n",
    "- **Translation Efficiency Prediction**: [Translation Efficiency Prediction Tutorial](../translation_efficiency_prediction/tutorials/)\n",
    "- **Transcription Factor Binding Prediction**: [Transcription Factor Binding Tutorial](../tfb_prediction/tutorials/)\n",
    "- **Promoter Recognition**: [Promoter Recognition Tutorial](../promoter_prediction/tutorials/)\n",
    "\n",
    "#### 📊 Sequence Regression Tasks\n",
    "- **Gene Expression Prediction**: [Gene Expression Prediction Tutorial](../gene_expression/tutorials/)\n",
    "- **Protein Stability**: [Protein Stability Tutorial](../protein_stability/tutorials/)\n",
    "\n",
    "#### 🎯 Token Classification Tasks\n",
    "- **Transcription Factor Binding Sites**: [TF Binding Site Tutorial](../tf_binding_sites/tutorials/)\n",
    "- **RNA Modification Sites**: [RNA Modification Tutorial](../rna_modification/tutorials/)\n",
    "\n",
    "#### 🔬 Variant Effect Prediction\n",
    "- **Variant Effect Analysis**: [Variant Effect Prediction Tutorial](../variant_effect_prediction/tutorials/)\n",
    "\n",
    "### 💡 Learning Recommendations\n",
    "\n",
    "1. **Start Simple**: Choose a binary classification task as your starting point\n",
    "2. **Understand Principles**: Think about why each step is done this way\n",
    "3. **Hands-on Practice**: Try to reproduce tutorials with your own data\n",
    "4. **Progressive Advancement**: Master basics before attempting complex tasks\n",
    "5. **Community Engagement**: Participate in GitHub discussions and Q&A\n",
    "\n",
    "### 🆘 Getting Help\n",
    "\n",
    "If you encounter problems during learning:\n",
    "- 📖 Consult [OmniGenBench Documentation](https://omnigenbench.readthedocs.io/)\n",
    "- 💬 Ask questions in [GitHub Issues](https://github.com/yangheng95/OmniGenBench/issues)\n",
    "- 👥 Participate in community discussions\n",
    "- 📧 Contact the development team\n",
    "\n",
    "**Start your genomic deep learning journey now!** 🚀"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
