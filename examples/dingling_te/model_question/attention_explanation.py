# -*- coding: utf-8 -*-
# æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£ä¸å¯è§†åŒ–

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("="*70)
print("ğŸ‘ï¸ æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰è¯¦è§£")
print("="*70)

# ============= 1. ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›ï¼Ÿ =============
print("\nğŸ“Œ 1. ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ")
print("-"*70)

print("""
æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³ï¼š
  ğŸ’¡ è®©æ¨¡å‹å­¦ä¼š"å…³æ³¨"è¾“å…¥åºåˆ—ä¸­çš„é‡è¦éƒ¨åˆ†

ç±»æ¯”äººç±»é˜…è¯»ï¼š
  "The cat sat on the mat because it was tired"
  
  å½“ç†è§£"it"æŒ‡ä»£ä»€ä¹ˆæ—¶ï¼Œäººç±»ä¼šï¼š
  1. å›çœ‹å‰é¢çš„å•è¯
  2. é‡ç‚¹å…³æ³¨"cat"è€Œä¸æ˜¯"mat"æˆ–"on"
  3. æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­"it"="cat"
  
  è¿™å°±æ˜¯æ³¨æ„åŠ›ï¼æ¨¡å‹å­¦ä¹ å“ªäº›ä½ç½®æ›´é‡è¦ã€‚
""")

# ============= 2. æ³¨æ„åŠ›çš„æ•°å­¦åŸç† =============
print("\nğŸ“Œ 2. æ³¨æ„åŠ›çš„æ•°å­¦åŸç†")
print("-"*70)

print("""
Self-Attentionè®¡ç®—æ­¥éª¤ï¼š

æ­¥éª¤1: è®¡ç®—Query, Key, Value
  Q = X @ W_q    # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
  K = X @ W_k    # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
  V = X @ W_v    # [seq_len, d_model] @ [d_model, d_v] = [seq_len, d_v]

æ­¥éª¤2: è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆç›¸ä¼¼åº¦ï¼‰
  scores = Q @ K^T / sqrt(d_k)    # [seq_len, seq_len]
  
æ­¥éª¤3: å½’ä¸€åŒ–ï¼ˆSoftmaxï¼‰
  attention_weights = softmax(scores)    # [seq_len, seq_len]
  
æ­¥éª¤4: åŠ æƒæ±‚å’Œ
  output = attention_weights @ V    # [seq_len, d_v]

å…¶ä¸­:
  - Q (Query): "æˆ‘æƒ³æ‰¾ä»€ä¹ˆï¼Ÿ"
  - K (Key): "æˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ"
  - V (Value): "æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
  - attention_weights: "æ¯ä¸ªä½ç½®çš„é‡è¦æ€§"
""")

# ============= 3. DNAåºåˆ—çš„æ³¨æ„åŠ›ç¤ºä¾‹ =============
print("\nğŸ“Œ 3. DNAåºåˆ—çš„æ³¨æ„åŠ›ç¤ºä¾‹")
print("-"*70)

# ç®€åŒ–çš„DNAåºåˆ—
sequence = "ATCGATCGTAGC"
seq_len = len(sequence)

print(f"DNAåºåˆ—: {sequence}")
print(f"åºåˆ—é•¿åº¦: {seq_len}\n")

# æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼ˆæ¯ä¸ªä½ç½®å¯¹å…¶ä»–ä½ç½®çš„å…³æ³¨ç¨‹åº¦ï¼‰
np.random.seed(42)
# åˆ›å»ºä¸€äº›ç»“æ„åŒ–çš„æ³¨æ„åŠ›æ¨¡å¼
attention_matrix = np.random.rand(seq_len, seq_len)

# è®©æ³¨æ„åŠ›é›†ä¸­åœ¨é™„è¿‘çš„ä½ç½®ï¼ˆå±€éƒ¨æ³¨æ„åŠ›æ¨¡å¼ï¼‰
for i in range(seq_len):
    for j in range(seq_len):
        dist = abs(i - j)
        attention_matrix[i, j] = np.exp(-dist / 2.0)  # è·ç¦»è¶Šè¿‘ï¼Œæ³¨æ„åŠ›è¶Šå¤§

# å½’ä¸€åŒ–ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)

print("æ³¨æ„åŠ›çŸ©é˜µç¤ºä¾‹:")
print("  è¡Œ: æŸ¥è¯¢ä½ç½® (Query)")
print("  åˆ—: è¢«å…³æ³¨çš„ä½ç½® (Key)")
print("  å€¼: æ³¨æ„åŠ›æƒé‡ (0-1ä¹‹é—´)")
print(f"  å½¢çŠ¶: [{seq_len}, {seq_len}]\n")

# å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µ
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# å·¦å›¾ï¼šæ³¨æ„åŠ›çƒ­åŠ›å›¾
ax1 = axes[0]
im = ax1.imshow(attention_matrix, cmap='YlOrRd', aspect='auto')
ax1.set_xticks(range(seq_len))
ax1.set_yticks(range(seq_len))
ax1.set_xticklabels(list(sequence))
ax1.set_yticklabels(list(sequence))
ax1.set_xlabel('è¢«å…³æ³¨çš„ä½ç½® (Key/Value)', fontsize=11)
ax1.set_ylabel('æŸ¥è¯¢ä½ç½® (Query)', fontsize=11)
ax1.set_title('DNAåºåˆ—çš„æ³¨æ„åŠ›æƒé‡çŸ©é˜µ\nï¼ˆæ¯ä¸ªç¢±åŸºå¯¹å…¶ä»–ç¢±åŸºçš„å…³æ³¨ç¨‹åº¦ï¼‰', 
             fontsize=12, fontweight='bold')

# æ·»åŠ colorbar
cbar = plt.colorbar(im, ax=ax1)
cbar.set_label('æ³¨æ„åŠ›æƒé‡', rotation=270, labelpad=20)

# å³å›¾ï¼šæŸä¸€ä¸ªä½ç½®çš„æ³¨æ„åŠ›åˆ†å¸ƒ
query_pos = 5  # é€‰æ‹©ä½ç½®5ï¼ˆGï¼‰
ax2 = axes[1]
attention_dist = attention_matrix[query_pos]
colors = ['#FF6B6B' if nt=='A' else '#4ECDC4' if nt=='T' else 
          '#45B7D1' if nt=='C' else '#95E1D3' for nt in sequence]

bars = ax2.bar(range(seq_len), attention_dist, color=colors, alpha=0.7, edgecolor='black')
ax2.set_xticks(range(seq_len))
ax2.set_xticklabels(list(sequence))
ax2.set_xlabel('åºåˆ—ä½ç½®', fontsize=11)
ax2.set_ylabel('æ³¨æ„åŠ›æƒé‡', fontsize=11)
ax2.set_title(f'ä½ç½®{query_pos}çš„ç¢±åŸº"{sequence[query_pos]}"å¯¹å…¶ä»–ä½ç½®çš„æ³¨æ„åŠ›\n'
             f'ï¼ˆé«˜åº¦è¡¨ç¤ºå…³æ³¨ç¨‹åº¦ï¼‰', 
             fontsize=12, fontweight='bold')
ax2.grid(axis='y', alpha=0.3)

# æ ‡æ³¨æœ€å…³æ³¨çš„3ä¸ªä½ç½®
top_3_idx = np.argsort(attention_dist)[-3:]
for idx in top_3_idx:
    ax2.text(idx, attention_dist[idx], f'{attention_dist[idx]:.2f}',
            ha='center', va='bottom', fontweight='bold', fontsize=9)

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/attention_visualization.png', 
           dpi=300, bbox_inches='tight')
print("ğŸ’¾ æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: attention_visualization.png\n")

# ============= 4. Multi-Head Attention =============
print("\nğŸ“Œ 4. Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰")
print("-"*70)

print("""
ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ
  ğŸ¤” å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½åªå…³æ³¨ä¸€ç§æ¨¡å¼
  ğŸ’¡ å¤šä¸ªæ³¨æ„åŠ›å¤´å¯ä»¥åŒæ—¶å…³æ³¨å¤šç§æ¨¡å¼

ä¾‹å¦‚åœ¨DNAåºåˆ—ä¸­ï¼š
  å¤´1: å…³æ³¨å±€éƒ¨motifï¼ˆå¦‚TATA boxï¼‰
  å¤´2: å…³æ³¨è¿œè·ç¦»ç›¸äº’ä½œç”¨
  å¤´3: å…³æ³¨é‡å¤åºåˆ—
  å¤´4: å…³æ³¨GCå«é‡å˜åŒ–
  ...

å®ç°:
  1. å°†è¾“å…¥åˆ†æˆhä¸ªå¤´
  2. æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
  3. æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
  4. é€šè¿‡çº¿æ€§å±‚èåˆ
""")

num_heads = 8
d_model = 768
d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

print(f"\nOmniGenome-52Mçš„å¤šå¤´æ³¨æ„åŠ›é…ç½®:")
print(f"  æ€»ç»´åº¦ (d_model): {d_model}")
print(f"  æ³¨æ„åŠ›å¤´æ•° (num_heads): {num_heads}")
print(f"  æ¯ä¸ªå¤´çš„ç»´åº¦ (d_k): {d_k}")
print(f"\nè®¡ç®—æµç¨‹:")
print(f"  è¾“å…¥: [batch, seq_len, {d_model}]")
print(f"  â†’ Splitæˆ{num_heads}ä¸ªå¤´: [batch, {num_heads}, seq_len, {d_k}]")
print(f"  â†’ æ¯ä¸ªå¤´è®¡ç®—attention: [batch, {num_heads}, seq_len, {d_k}]")
print(f"  â†’ Concatæ‰€æœ‰å¤´: [batch, seq_len, {d_model}]")

# ============= 5. æ³¨æ„åŠ›çš„ä½œç”¨ =============
print("\n\nğŸ“Œ 5. æ³¨æ„åŠ›æœºåˆ¶çš„ä½œç”¨")
print("-"*70)

print("""
åœ¨è½¬åº§å­è¡¨è¾¾é¢„æµ‹ä»»åŠ¡ä¸­çš„ä½œç”¨ï¼š

1ï¸âƒ£ è¯†åˆ«é‡è¦çš„è°ƒæ§å…ƒä»¶
   - å¯åŠ¨å­åŒºåŸŸï¼ˆPromoterï¼‰
   - å¢å¼ºå­ï¼ˆEnhancerï¼‰
   - è½¬å½•å› å­ç»“åˆä½ç‚¹ï¼ˆTFBSï¼‰
   
2ï¸âƒ£ æ•æ‰é•¿è·ç¦»ä¾èµ–
   - è¿œç«¯è°ƒæ§å…ƒä»¶ä¸åŸºå› çš„äº’ä½œ
   - æŸ“è‰²è´¨ç¯ï¼ˆChromatin Loopï¼‰
   
3ï¸âƒ£ å…³æ³¨åºåˆ—ç‰¹å¼‚æ€§motif
   - TATA box
   - CAAT box
   - GC-richåŒºåŸŸ
   
4ï¸âƒ£ å»ºæ¨¡ç¢±åŸºé—´ç›¸äº’ä½œç”¨
   - é…å¯¹ç¢±åŸº
   - äºŒçº§ç»“æ„
   - å¯†ç å­ä½¿ç”¨åå¥½
""")

# ============= 6. å¦‚ä½•å¯è§†åŒ–æ³¨æ„åŠ›ï¼Ÿ =============
print("\n\nğŸ“Œ 6. å¦‚ä½•å¯è§†åŒ–æ³¨æ„åŠ›ï¼Ÿ")
print("-"*70)

print("""
æ–¹æ³•1: æå–Transformerçš„æ³¨æ„åŠ›æƒé‡
  
  # åœ¨æ¨¡å‹å‰å‘ä¼ æ’­æ—¶
  outputs = model(input_ids, output_attentions=True)
  attentions = outputs.attentions  # Tuple of attention matrices
  
  # attentions[layer][batch, num_heads, seq_len, seq_len]
  
  # å¯è§†åŒ–ç¬¬12å±‚çš„ç¬¬1ä¸ªå¤´
  layer_12_head_1 = attentions[11][0, 0, :, :]  # [seq_len, seq_len]
  
  # ç»˜åˆ¶çƒ­åŠ›å›¾
  sns.heatmap(layer_12_head_1, cmap='viridis')

æ–¹æ³•2: å¹³å‡å¤šä¸ªå¤´çš„æ³¨æ„åŠ›
  
  # å¹³å‡æ‰€æœ‰å¤´
  avg_attention = attentions[layer].mean(dim=1)  # [batch, seq_len, seq_len]
  
  # å¯è§†åŒ–å¹³å‡æ³¨æ„åŠ›
  plt.imshow(avg_attention[0].cpu().numpy())

æ–¹æ³•3: å…³æ³¨ç‰¹å®šä½ç½®çš„æ³¨æ„åŠ›
  
  # ä¾‹å¦‚ï¼šçœ‹ä½ç½®50å¯¹å…¶ä»–ä½ç½®çš„æ³¨æ„åŠ›
  attention_from_pos_50 = avg_attention[0, 50, :]
  
  # æ˜ å°„å›åºåˆ—
  for i, (nt, att) in enumerate(zip(sequence, attention_from_pos_50)):
      print(f"{i:3d} {nt}  {'â–ˆ' * int(att*50)}")
""")

# ============= 7. å®é™…åº”ç”¨ç¤ºä¾‹ä»£ç  =============
print("\n\nğŸ“Œ 7. å®é™…åº”ç”¨ä»£ç ç¤ºä¾‹")
print("-"*70)

code_example = '''
def visualize_attention_for_sequence(model, sequence, tokenizer):
    """å¯è§†åŒ–åºåˆ—çš„æ³¨æ„åŠ›æ¨¡å¼"""
    
    # 1. Tokenize
    inputs = tokenizer(sequence, return_tensors="pt")
    
    # 2. å‰å‘ä¼ æ’­å¹¶è·å–æ³¨æ„åŠ›
    with torch.no_grad():
        outputs = model.model(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            output_attentions=True
        )
    
    # 3. æå–æ³¨æ„åŠ›æƒé‡
    attentions = outputs.attentions  # Tuple: (layer1, layer2, ...)
    
    # 4. é€‰æ‹©æœ€åä¸€å±‚ï¼Œå¹³å‡æ‰€æœ‰å¤´
    last_layer_attention = attentions[-1]  # [1, num_heads, seq_len, seq_len]
    avg_attention = last_layer_attention.mean(dim=1)[0]  # [seq_len, seq_len]
    
    # 5. å¯è§†åŒ–
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # è·å–tokenåºåˆ—ï¼ˆç”¨äºæ ‡ç­¾ï¼‰
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    
    sns.heatmap(
        avg_attention.cpu().numpy(),
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='viridis',
        ax=ax
    )
    
    ax.set_title('Attention Pattern (Average of All Heads)')
    plt.tight_layout()
    plt.savefig('attention_pattern.png', dpi=300)
    
    return avg_attention

# ä½¿ç”¨ç¤ºä¾‹
sequence = "ATCGATCGTAGCTAGCTAGC"
attention = visualize_attention_for_sequence(model, sequence, tokenizer)
'''

print(code_example)

# ============= 8. æ³¨æ„åŠ›çš„è§£é‡Š =============
print("\nğŸ“Œ 8. æ³¨æ„åŠ›æƒé‡çš„ç”Ÿç‰©å­¦è§£é‡Š")
print("-"*70)

print("""
é«˜æ³¨æ„åŠ›æƒé‡å¯èƒ½è¡¨ç¤ºï¼š

âœ… åŠŸèƒ½ç›¸å…³çš„motif
   å¦‚æœä¸¤ä¸ªä½ç½®ä¹‹é—´æœ‰é«˜æ³¨æ„åŠ›ï¼Œå®ƒä»¬å¯èƒ½ï¼š
   - å½¢æˆè½¬å½•å› å­ç»“åˆä½ç‚¹
   - å‚ä¸äºŒçº§ç»“æ„å½¢æˆ
   - å…±åŒå½±å“åŸºå› è¡¨è¾¾

âœ… ä¿å®ˆåŒºåŸŸ
   é‡è¦çš„åŠŸèƒ½å…ƒä»¶é€šå¸¸è¿›åŒ–ä¿å®ˆ
   æ¨¡å‹å­¦ä¼šå…³æ³¨è¿™äº›ä¿å®ˆä½ç½®

âœ… ç»„ç»‡ç‰¹å¼‚æ€§è°ƒæ§
   ä¸åŒæ³¨æ„åŠ›å¤´å¯èƒ½å¯¹åº”ä¸åŒç»„ç»‡
   å¤´1å…³æ³¨æ ¹ç‰¹å¼‚æ€§å…ƒä»¶
   å¤´2å…³æ³¨å¶ç‰¹å¼‚æ€§å…ƒä»¶

âš ï¸ æ³¨æ„ï¼š
   æ³¨æ„åŠ›æƒé‡â‰ å› æœå…³ç³»
   åªæ˜¯æ¨¡å‹è®¤ä¸º"é‡è¦"çš„ä½ç½®
   éœ€è¦ç»“åˆç”Ÿç‰©å­¦çŸ¥è¯†è§£é‡Š
""")

plt.show()

print("\n" + "="*70)
print("âœ… æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£å®Œæˆï¼")
print("="*70)



