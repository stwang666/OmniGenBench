# -*- coding: utf-8 -*-
# file: triclass_te_improved.py
# æ”¹è¿›ç‰ˆè®­ç»ƒè„šæœ¬ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
# time: 20/10/2025

import torch
import torch.nn as nn
import math
import numpy as np

from omnigenbench import (
    ClassificationMetric,
    AccelerateTrainer,
    ModelHub,
    OmniTokenizer,
    OmniDatasetForMultiLabelClassification,
    OmniModelForMultiLabelSequenceClassification,
    OmniPooling,
)

model_name_or_path = "yangheng/OmniGenome-52M"

# Load tokenizer
tokenizer = OmniTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)


class TriClassTEDataset(OmniDatasetForMultiLabelClassification):
    """Dataset for 3-class (Low/Medium/High) multi-label TE classification"""

    def __init__(self, augment=False, **kwargs):
        super().__init__(**kwargs)
        self.augment = augment

    def prepare_input(self, instance, **kwargs):
        # Map labels to indices: Low=0, Medium=1, High=2
        label2idx = {'Low': 0, 'Medium': 1, 'High': 2, 'nan': -100}

        # Extract labels for all 9 tissues
        root_TE_label = label2idx[str(instance["root_TE_label"])]
        seedling_TE_label = label2idx[str(instance["seedling_TE_label"])]
        leaf_TE_label = label2idx[str(instance["leaf_TE_label"])]
        FMI_TE_label = label2idx[str(instance["FMI_TE_label"])]
        FOD_TE_label = label2idx[str(instance["FOD_TE_label"])]
        Prophase_I_pollen_TE_label = label2idx[str(instance["Prophase-I-pollen_TE_label"])]
        Tricellular_pollen_TE_label = label2idx[str(instance["Tricellular-pollen_TE_label"])]
        flag_TE_label = label2idx[str(instance["flag_TE_label"])]
        grain_TE_label = label2idx[str(instance["grain_TE_label"])]
        sequence = instance["sequence"]

        # ğŸ†• æ•°æ®å¢å¼ºï¼šéšæœºåå‘äº’è¡¥
        if self.augment and torch.rand(1).item() > 0.5:
            complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}
            sequence = ''.join([complement.get(base, base) for base in sequence[::-1]])

        # Tokenize sequence
        tokenized_inputs = self.tokenizer(
            sequence,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Stack all labels
        labels = torch.tensor([
            root_TE_label,
            seedling_TE_label,
            leaf_TE_label,
            FMI_TE_label,
            FOD_TE_label,
            Prophase_I_pollen_TE_label,
            Tricellular_pollen_TE_label,
            flag_TE_label,
            grain_TE_label,
        ], dtype=torch.long)

        tokenized_inputs["labels"] = labels

        return tokenized_inputs


class ImprovedOmniModelForTriClassTE(OmniModelForMultiLabelSequenceClassification):
    """æ”¹è¿›çš„æ¨¡å‹ï¼šæ·»åŠ æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ"""

    def __init__(self, config_or_model, tokenizer, num_labels=9, num_classes=3, 
                 dropout_rate=0.3, freeze_layers=0, *args, **kwargs):
        super().__init__(config_or_model, tokenizer, num_labels=num_labels * num_classes, *args, **kwargs)
        self.metadata["model_name"] = self.__class__.__name__
        self.num_labels = num_labels  # 9 tissues
        self.num_classes = num_classes  # 3 classes (Low/Medium/High)
        
        # ğŸ†• å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„åº•å±‚ï¼ˆå¯é€‰ï¼‰
        if freeze_layers > 0:
            print(f"ğŸ”’ å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‰ {freeze_layers} å±‚")
            for i, layer in enumerate(self.model.encoder.layer):
                if i < freeze_layers:
                    for param in layer.parameters():
                        param.requires_grad = False
        
        self.pooler = OmniPooling(self.config)
        
        # ğŸ†• æ·»åŠ Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(dropout_rate)
        
        # ğŸ†• ä½¿ç”¨å¤šå±‚åˆ†ç±»å™¨è€Œéå•å±‚
        self.classifier = nn.Sequential(
            nn.Linear(self.config.hidden_size, self.config.hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(self.config.hidden_size // 2, self.num_classes * self.num_labels)
        )
        
        # ğŸ†• ä½¿ç”¨Label Smoothingå‡å°‘è¿‡æ‹Ÿåˆ
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=-100, reduction="mean", label_smoothing=0.1)

        self.dataset_class = kwargs.pop('dataset_class', TriClassTEDataset)

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        """Forward pass with dropout"""
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

        # Get the logits from classifier head
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        
        # ğŸ†• åº”ç”¨dropout
        pooled = self.pooler(input_ids, logits)
        pooled = self.dropout(pooled)
        logits = self.classifier(pooled)
        
        # Reshape logits
        batch_size = logits.shape[0]
        logits = logits.view(batch_size, self.num_labels, self.num_classes)

        loss = None
        if labels is not None:
            logits_flat = logits.view(-1, self.num_classes)
            labels_flat = labels.view(-1)
            loss = self.loss_fn(logits_flat, labels_flat)

        return {
            "loss": loss,
            "logits": logits,
            "last_hidden_state": outputs.last_hidden_state if hasattr(outputs, 'last_hidden_state') else None,
        }

    def predict(self, sequence_or_inputs, **kwargs):
        """Prediction with softmax for multi-class"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        probabilities = torch.softmax(logits, dim=-1)
        predictions = torch.argmax(probabilities, dim=-1)

        outputs = {
            "predictions": predictions,
            "logits": logits,
            "probabilities": probabilities,
            "last_hidden_state": last_hidden_state,
        }

        return outputs

    def inference(self, sequence_or_inputs, **kwargs):
        """Inference wrapper"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        probabilities = torch.softmax(logits, dim=-1)
        predictions = torch.argmax(probabilities, dim=-1)
        confidence, _ = torch.max(probabilities, dim=-1)

        if not isinstance(sequence_or_inputs, list):
            outputs = {
                "predictions": predictions[0],
                "logits": logits[0],
                "probabilities": probabilities[0],
                "confidence": confidence[0],
                "last_hidden_state": last_hidden_state[0] if last_hidden_state is not None else None,
            }
        else:
            outputs = {
                "predictions": predictions,
                "logits": logits,
                "probabilities": probabilities,
                "confidence": confidence,
                "last_hidden_state": last_hidden_state,
            }

        return outputs


if __name__ == "__main__":
    # Load datasets
    print("ğŸ“Š Loading datasets...")
    datasets = TriClassTEDataset.from_hub(
        dataset_name_or_path="examples/dingling_te/",
        tokenizer=tokenizer,
        max_length=512,
        force_padding=False,
        augment=True  # ğŸ†• å¯ç”¨æ•°æ®å¢å¼º
    )

    print("ğŸ“ Data loading completed!")
    print(f"ğŸ“Š Loaded datasets: {list(datasets.keys())}")
    for split, dataset in datasets.items():
        print(f"  - {split}: {len(dataset)} samples")

    # ğŸ†• æ•°æ®é‡æ£€æŸ¥
    train_size = len(datasets['train'])
    if train_size < 500:
        print("âš ï¸  è­¦å‘Š: è®­ç»ƒé›†æ ·æœ¬è¾ƒå°‘ï¼Œå»ºè®®ä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–æˆ–æ•°æ®å¢å¼º")

    # Initialize improved model
    print("\nğŸš€ Initializing improved model...")
    model = ImprovedOmniModelForTriClassTE(
        model_name_or_path,
        tokenizer,
        num_labels=9,
        num_classes=3,
        dropout_rate=0.4,  # ğŸ†• è¾ƒå¤§çš„dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        freeze_layers=6,   # ğŸ†• å†»ç»“åº•å±‚6å±‚ï¼Œåªå¾®è°ƒé¡¶å±‚
        trust_remote_code=True,
        dataset_class=TriClassTEDataset
    )

    # Define metrics
    metric_functions = [
        ClassificationMetric(ignore_y=-100).accuracy_score,
        ClassificationMetric(ignore_y=-100, average='macro').f1_score,
    ]

    # ğŸ†• æ”¹è¿›çš„è®­ç»ƒé…ç½®
    trainer = AccelerateTrainer(
        model=model,
        epochs=30,  # ğŸ†• å‡å°‘epochæ•°é‡ï¼ˆä»50é™åˆ°30ï¼‰
        learning_rate=1e-5,  # ğŸ†• é™ä½å­¦ä¹ ç‡ï¼ˆä»2e-5é™åˆ°1e-5ï¼‰
        batch_size=8,  # ğŸ†• å‡å°batch sizeå¢åŠ æ­£åˆ™åŒ–æ•ˆæœ
        train_dataset=datasets["train"],
        eval_dataset=datasets["valid"],
        test_dataset=datasets["test"],
        compute_metrics=metric_functions,
        gradient_accumulation_steps=8,  # ğŸ†• å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆbatch=64ï¼‰
        weight_decay=0.01,  # ğŸ†• æ·»åŠ æƒé‡è¡°å‡
        warmup_steps=100,  # ğŸ†• å­¦ä¹ ç‡é¢„çƒ­
        eval_steps=50,  # ğŸ†• æ›´é¢‘ç¹çš„éªŒè¯
        save_strategy="steps",  # ğŸ†• æŒ‰æ­¥æ•°ä¿å­˜
        save_steps=50,
        load_best_model_at_end=True,  # ğŸ†• è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="accuracy_score",  # ğŸ†• åŸºäºéªŒè¯é›†accuracyé€‰æ‹©æœ€ä½³æ¨¡å‹
        greater_is_better=True,
        save_total_limit=3,  # ğŸ†• åªä¿ç•™æœ€å¥½çš„3ä¸ªcheckpoint
    )

    print("\nğŸ¯ å¼€å§‹è®­ç»ƒï¼ˆé˜²è¿‡æ‹Ÿåˆé…ç½®ï¼‰")
    print("="*70)
    print("é˜²è¿‡æ‹Ÿåˆç­–ç•¥:")
    print("  âœ… 1. Dropout rate = 0.4")
    print("  âœ… 2. å†»ç»“åº•å±‚6å±‚")
    print("  âœ… 3. Label smoothing = 0.1")
    print("  âœ… 4. Weight decay = 0.01")
    print("  âœ… 5. é™ä½å­¦ä¹ ç‡åˆ°1e-5")
    print("  âœ… 6. å‡å°‘epochåˆ°30")
    print("  âœ… 7. æ•°æ®å¢å¼ºï¼ˆåå‘äº’è¡¥ï¼‰")
    print("  âœ… 8. Early stoppingï¼ˆè‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹ï¼‰")
    print("  âœ… 9. å¤šå±‚åˆ†ç±»å™¨")
    print("="*70)

    metrics = trainer.train(
        path_to_save="ogb_te_3class_improved",
        dataset_class=TriClassTEDataset
    )
    
    print('\nğŸ“Š Final Metrics:', metrics)
    print("\nâœ… è®­ç»ƒå®Œæˆï¼æ£€æŸ¥éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ€§èƒ½å·®å¼‚ã€‚")
    print("å¦‚æœæµ‹è¯•é›†æ€§èƒ½ä»ç„¶å¾ˆä½ï¼Œè¿è¡Œ diagnose_overfitting.py è¿›è¡Œè¯Šæ–­ã€‚")



