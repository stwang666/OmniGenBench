# -*- coding: utf-8 -*-
# æ­£åˆ™åŒ–å’ŒDropoutè¯¦è§£

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

print("="*70)
print("ğŸ›¡ï¸ æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰å’Œ Dropout è¯¦è§£")
print("="*70)

# ============= 1. ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼Ÿ =============
print("\nğŸ“Œ 1. ä»€ä¹ˆæ˜¯æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰ï¼Ÿ")
print("-"*70)

print("""
æ­£åˆ™åŒ–çš„æ ¸å¿ƒç›®çš„ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ

è¿‡æ‹Ÿåˆé—®é¢˜ï¼š
  è®­ç»ƒé›†ï¼š99% âœ…
  æµ‹è¯•é›†ï¼š40% âŒ
  
  æ¨¡å‹æŠŠè®­ç»ƒæ•°æ®"è®°ä½"äº†ï¼Œè€Œä¸æ˜¯å­¦åˆ°è§„å¾‹

æ­£åˆ™åŒ–çš„æ€æƒ³ï¼š
  ç»™æ¨¡å‹æ·»åŠ çº¦æŸï¼Œè®©å®ƒä¸è¦å¤ª"å¤æ‚"
  
ç±»æ¯”ï¼š
  å­¦ç”Ÿåšé¢˜ï¼š
    æ— çº¦æŸï¼šæ­»è®°ç¡¬èƒŒæ¯é“é¢˜çš„ç­”æ¡ˆï¼ˆè¿‡æ‹Ÿåˆï¼‰
    æœ‰çº¦æŸï¼šç†è§£åŸç†ï¼Œå­¦ä¼šä¸¾ä¸€åä¸‰ï¼ˆæ³›åŒ–ï¼‰
""")

# ============= 2. L2æ­£åˆ™åŒ–ï¼ˆWeight Decayï¼‰ =============
print("\nğŸ“Œ 2. L2æ­£åˆ™åŒ–ï¼ˆWeight Decayï¼‰")
print("-"*70)

print("""
åŸç†ï¼šæƒ©ç½šå¤§çš„æƒé‡å€¼

æŸå¤±å‡½æ•°å˜åŒ–ï¼š
  åŸå§‹æŸå¤±ï¼š
    Loss = CrossEntropyLoss(predictions, labels)
  
  æ·»åŠ L2æ­£åˆ™åŒ–ï¼š
    Loss = CrossEntropyLoss(predictions, labels) + Î» * Î£(wÂ²)
           â””â”€â”€â”€â”€â”€â”€â”€ ä»»åŠ¡æŸå¤± â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€ æ­£åˆ™åŒ–é¡¹ â”€â”€â”˜
  
  å…¶ä¸­ï¼š
    - Î» (lambda): æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆå¦‚0.01ï¼‰
    - w: æ¨¡å‹çš„æƒé‡
    - Î£(wÂ²): æ‰€æœ‰æƒé‡çš„å¹³æ–¹å’Œ

æ•ˆæœï¼š
  âœ… è®©æƒé‡å˜å°
  âœ… æ¨¡å‹æ›´"å¹³æ»‘"
  âœ… é™ä½è¿‡æ‹Ÿåˆé£é™©
  
PyTorchå®ç°ï¼š
  optimizer = torch.optim.Adam(
      model.parameters(),
      lr=1e-5,
      weight_decay=0.01  # ğŸ”‘ è¿™å°±æ˜¯L2æ­£åˆ™åŒ–ï¼
  )
""")

# å¯è§†åŒ–L2æ­£åˆ™åŒ–æ•ˆæœ
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# ç”Ÿæˆç®€å•æ•°æ®
np.random.seed(42)
x = np.linspace(-3, 3, 100)
y_true = 2 * x + 1  # çœŸå®å…³ç³»

# æ·»åŠ å™ªå£°
y_noisy = y_true + np.random.randn(100) * 2

# ä¸‰ç§æ¨¡å‹ï¼šæ— æ­£åˆ™åŒ–ã€è½»åº¦æ­£åˆ™åŒ–ã€é‡åº¦æ­£åˆ™åŒ–
for ax, decay, title in zip(axes, [0, 0.01, 0.1], 
                            ['æ— æ­£åˆ™åŒ–ï¼ˆè¿‡æ‹Ÿåˆï¼‰', 'L2=0.01ï¼ˆé€‚åº¦ï¼‰', 'L2=0.1ï¼ˆæ¬ æ‹Ÿåˆï¼‰']):
    # æ¨¡æ‹Ÿä¸åŒæ­£åˆ™åŒ–å¼ºåº¦ä¸‹çš„æ‹Ÿåˆ
    if decay == 0:
        # è¿‡æ‹Ÿåˆï¼šä½¿ç”¨é«˜æ¬¡å¤šé¡¹å¼
        coeffs = np.polyfit(x, y_noisy, 9)
        y_pred = np.polyval(coeffs, x)
        color = 'red'
    elif decay == 0.01:
        # é€‚åº¦ï¼šç®€å•çº¿æ€§
        coeffs = np.polyfit(x, y_noisy, 1)
        y_pred = np.polyval(coeffs, x)
        color = 'green'
    else:
        # è¿‡åº¦æ­£åˆ™åŒ–ï¼šæ¥è¿‘å¸¸æ•°
        y_pred = np.ones_like(x) * np.mean(y_noisy)
        color = 'orange'
    
    ax.scatter(x, y_noisy, alpha=0.3, s=20, label='æ•°æ®ç‚¹')
    ax.plot(x, y_true, 'b--', linewidth=2, label='çœŸå®å…³ç³»', alpha=0.7)
    ax.plot(x, y_pred, color=color, linewidth=3, label='æ¨¡å‹æ‹Ÿåˆ')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title(title, fontweight='bold', fontsize=11)
    ax.legend()
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/l2_regularization.png', 
           dpi=300, bbox_inches='tight')
print("ğŸ’¾ L2æ­£åˆ™åŒ–æ•ˆæœå›¾å·²ä¿å­˜: l2_regularization.png\n")

# ============= 3. Dropout =============
print("\nğŸ“Œ 3. Dropout")
print("-"*70)

print("""
åŸç†ï¼šè®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€äº›ç¥ç»å…ƒ

å·¥ä½œæœºåˆ¶ï¼š
  è®­ç»ƒé˜¶æ®µï¼ˆTrainingï¼‰ï¼š
    1. å¯¹æ¯ä¸ªç¥ç»å…ƒï¼Œä»¥æ¦‚ç‡péšæœºå°†å…¶è¾“å‡ºè®¾ä¸º0
    2. å…¶ä½™ç¥ç»å…ƒçš„è¾“å‡º Ã— (1/(1-p)) æ¥ä¿æŒæœŸæœ›ä¸å˜
    3. æ¯ä¸ªbatchéƒ½é‡æ–°éšæœº
  
  æµ‹è¯•é˜¶æ®µï¼ˆInferenceï¼‰ï¼š
    - æ‰€æœ‰ç¥ç»å…ƒéƒ½å·¥ä½œ
    - ä¸è¿›è¡Œdropoutæ“ä½œ

ç¤ºä¾‹ï¼ˆDropout=0.4ï¼‰ï¼š
  åŸå§‹ç¥ç»å…ƒè¾“å‡º:
    [0.5, 0.8, 0.3, 0.6, 0.9, 0.2, 0.7, 0.4, 0.1, 0.5]
  
  è®­ç»ƒæ—¶åº”ç”¨Dropoutï¼ˆéšæœºå…³é—­40%ï¼‰:
    [0.0, 1.3, 0.0, 1.0, 1.5, 0.0, 0.0, 0.7, 0.2, 0.0]
    â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜
     âŒ   âœ…   âŒ   âœ…   âœ…   âŒ   âŒ   âœ…   âœ…   âŒ
  
  æµ‹è¯•æ—¶ï¼ˆæ— Dropoutï¼‰:
    [0.5, 0.8, 0.3, 0.6, 0.9, 0.2, 0.7, 0.4, 0.1, 0.5]
    â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜  â””â”€â”˜
     âœ…   âœ…   âœ…   âœ…   âœ…   âœ…   âœ…   âœ…   âœ…   âœ…

ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
  1. é˜²æ­¢ç¥ç»å…ƒä¹‹é—´çš„"åˆè°‹"ï¼ˆco-adaptationï¼‰
  2. ç±»ä¼¼äºè®­ç»ƒå¤šä¸ªæ¨¡å‹çš„é›†æˆ
  3. å¼ºåˆ¶ç½‘ç»œå­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾
  
ç±»æ¯”ï¼š
  è¶³çƒé˜Ÿè®­ç»ƒï¼š
    æ— Dropout: å›ºå®š11äººç»„åˆè®­ç»ƒ
      â†’ é˜Ÿå‘˜ä¹‹é—´é…åˆå¥½ï¼Œä½†ä¾èµ–æ€§å¼º
      â†’ ç¼ºä¸€ä¸ªäººå°±ç©ä¸è½¬
    
    æœ‰Dropout: æ¯æ¬¡è®­ç»ƒéšæœºç¼ºäºº
      â†’ æ¯ä¸ªäººå­¦ä¼šç‹¬ç«‹èƒ½åŠ›
      â†’ ä»»ä½•ç»„åˆéƒ½èƒ½å‘æŒ¥
""")

# å¯è§†åŒ–Dropout
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå±‚
layer_size = 100
dropout_rate = 0.4

# åŸå§‹æ¿€æ´»
original_activation = np.random.randn(layer_size)

# ç»˜åˆ¶6æ¬¡ä¸åŒçš„dropout mask
for idx, ax in enumerate(axes.flat):
    # ç”Ÿæˆdropout mask
    mask = np.random.binomial(1, 1-dropout_rate, layer_size)
    dropped_activation = original_activation * mask / (1 - dropout_rate)
    
    # å¯è§†åŒ–
    ax.bar(range(layer_size), original_activation, alpha=0.3, color='blue', label='åŸå§‹')
    ax.bar(range(layer_size), dropped_activation, alpha=0.7, color='red', label='Dropoutå')
    ax.set_title(f'Dropoutç¤ºä¾‹ {idx+1}\nï¼ˆçº¢è‰²=0è¡¨ç¤ºè¢«å…³é—­ï¼‰', fontsize=10, fontweight='bold')
    ax.set_xlabel('ç¥ç»å…ƒç´¢å¼•')
    ax.set_ylabel('æ¿€æ´»å€¼')
    ax.set_ylim(-3, 3)
    if idx == 0:
        ax.legend()
    
    # ç»Ÿè®¡è¢«dropoutçš„æ•°é‡
    n_dropped = np.sum(mask == 0)
    ax.text(0.5, 0.95, f'å…³é—­: {n_dropped}/{layer_size} ({n_dropped/layer_size*100:.1f}%)',
           transform=ax.transAxes, ha='center', va='top',
           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),
           fontsize=9)

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/dropout_visualization.png', 
           dpi=300, bbox_inches='tight')
print("ğŸ’¾ Dropoutå¯è§†åŒ–å·²ä¿å­˜: dropout_visualization.png\n")

# ============= 4. PyTorchä»£ç ç¤ºä¾‹ =============
print("\nğŸ“Œ 4. PyTorchä»£ç ç¤ºä¾‹")
print("-"*70)

print("""
ç¤ºä¾‹1: æ·»åŠ Dropoutå±‚

class ModelWithoutDropout(nn.Module):
    def __init__(self):
        self.fc1 = nn.Linear(768, 384)
        self.fc2 = nn.Linear(384, 27)
    
    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)  # âŒ å®¹æ˜“è¿‡æ‹Ÿåˆ
        return x

class ModelWithDropout(nn.Module):
    def __init__(self):
        self.fc1 = nn.Linear(768, 384)
        self.dropout = nn.Dropout(0.4)  # âœ… æ·»åŠ Dropout
        self.fc2 = nn.Linear(384, 27)
    
    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)  # âœ… è®­ç»ƒæ—¶éšæœºdropout
        x = self.fc2(x)
        return x

ç¤ºä¾‹2: ä½¿ç”¨Weight Decay

# æ— æ­£åˆ™åŒ–
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

# æœ‰L2æ­£åˆ™åŒ–ï¼ˆWeight Decayï¼‰
optimizer = torch.optim.Adam(
    model.parameters(), 
    lr=1e-5,
    weight_decay=0.01  # âœ… Î»=0.01
)

ç¤ºä¾‹3: è®­ç»ƒ/æµ‹è¯•æ¨¡å¼åˆ‡æ¢

# è®­ç»ƒæ¨¡å¼ï¼ˆDropoutç”Ÿæ•ˆï¼‰
model.train()
for batch in train_loader:
    loss = model(batch)
    loss.backward()
    optimizer.step()

# æµ‹è¯•æ¨¡å¼ï¼ˆDropoutå…³é—­ï¼‰
model.eval()
with torch.no_grad():
    for batch in test_loader:
        predictions = model(batch)
""")

# ============= 5. å®é™…è®­ç»ƒå¯¹æ¯” =============
print("\nğŸ“Œ 5. æœ‰æ— æ­£åˆ™åŒ–çš„å¯¹æ¯”")
print("-"*70)

# æ¨¡æ‹Ÿè®­ç»ƒæ›²çº¿
epochs = np.arange(1, 51)

# æ— æ­£åˆ™åŒ–ï¼šå¿«é€Ÿè¿‡æ‹Ÿåˆ
train_loss_no_reg = 1.0 * np.exp(-epochs / 5) + 0.05
test_loss_no_reg = 0.8 * np.exp(-epochs / 10) + 0.4 + 0.1 * epochs / 50

# æœ‰æ­£åˆ™åŒ–ï¼šç¨³å®šæ”¶æ•›
train_loss_reg = 1.0 * np.exp(-epochs / 8) + 0.15
test_loss_reg = 0.8 * np.exp(-epochs / 8) + 0.2

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# æŸå¤±æ›²çº¿
ax1 = axes[0]
ax1.plot(epochs, train_loss_no_reg, 'r-', linewidth=2, label='è®­ç»ƒé›†ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰')
ax1.plot(epochs, test_loss_no_reg, 'r--', linewidth=2, label='æµ‹è¯•é›†ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰')
ax1.plot(epochs, train_loss_reg, 'g-', linewidth=2, label='è®­ç»ƒé›†ï¼ˆæœ‰æ­£åˆ™åŒ–ï¼‰')
ax1.plot(epochs, test_loss_reg, 'g--', linewidth=2, label='æµ‹è¯•é›†ï¼ˆæœ‰æ­£åˆ™åŒ–ï¼‰')
ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('è®­ç»ƒæ›²çº¿å¯¹æ¯”\nï¼ˆç»¿è‰²=æœ‰æ­£åˆ™åŒ–ï¼Œçº¢è‰²=æ— æ­£åˆ™åŒ–ï¼‰', fontweight='bold', fontsize=12)
ax1.legend()
ax1.grid(alpha=0.3)

# æ ‡æ³¨è¿‡æ‹ŸåˆåŒºåŸŸ
ax1.axvspan(15, 50, alpha=0.2, color='red')
ax1.text(32, 0.9, 'æ— æ­£åˆ™åŒ–ï¼š\nä¸¥é‡è¿‡æ‹Ÿåˆ', ha='center', fontsize=10,
        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))
ax1.text(32, 0.3, 'æœ‰æ­£åˆ™åŒ–ï¼š\næ³›åŒ–è‰¯å¥½', ha='center', fontsize=10,
        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))

# æ³›åŒ–gap
ax2 = axes[1]
gap_no_reg = test_loss_no_reg - train_loss_no_reg
gap_reg = test_loss_reg - train_loss_reg

ax2.plot(epochs, gap_no_reg, 'r-', linewidth=3, label='æ— æ­£åˆ™åŒ–')
ax2.plot(epochs, gap_reg, 'g-', linewidth=3, label='æœ‰æ­£åˆ™åŒ–')
ax2.axhline(y=0.1, color='orange', linestyle='--', label='å¯æ¥å—é˜ˆå€¼')
ax2.fill_between(epochs, 0, gap_no_reg, alpha=0.3, color='red')
ax2.fill_between(epochs, 0, gap_reg, alpha=0.3, color='green')
ax2.set_xlabel('Epoch', fontsize=11)
ax2.set_ylabel('æ³›åŒ–Gapï¼ˆæµ‹è¯•Loss - è®­ç»ƒLossï¼‰', fontsize=11)
ax2.set_title('æ³›åŒ–èƒ½åŠ›å¯¹æ¯”\nï¼ˆGapè¶Šå°è¶Šå¥½ï¼‰', fontweight='bold', fontsize=12)
ax2.legend()
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/regularization_comparison.png', 
           dpi=300, bbox_inches='tight')
print("ğŸ’¾ æ­£åˆ™åŒ–å¯¹æ¯”å›¾å·²ä¿å­˜: regularization_comparison.png\n")

# ============= 6. å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯ =============
print("\nğŸ“Œ 6. å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯")
print("-"*70)

print("""
1ï¸âƒ£ L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
   Loss = Task_Loss + Î» * Î£|w|
   
   ç‰¹ç‚¹ï¼š
   - å€¾å‘äºäº§ç”Ÿç¨€ç–æƒé‡ï¼ˆå¾ˆå¤šæƒé‡=0ï¼‰
   - å¯ç”¨äºç‰¹å¾é€‰æ‹©
   - PyTorchä¸­éœ€è¦æ‰‹åŠ¨å®ç°

2ï¸âƒ£ Label Smoothing
   åŸå§‹æ ‡ç­¾: [0, 0, 1]  # one-hot
   å¹³æ»‘å:    [0.05, 0.05, 0.9]  # æ›´è½¯çš„æ ‡ç­¾
   
   æ•ˆæœï¼š
   - é˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡
   - æé«˜æ³›åŒ–èƒ½åŠ›
   
   ä»£ç ï¼š
   loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)

3ï¸âƒ£ Early Stopping
   ç›‘æ§éªŒè¯é›†æ€§èƒ½ï¼Œå¦‚æœä¸å†æå‡å°±åœæ­¢è®­ç»ƒ
   
   å®ç°ï¼š
   best_val_loss = float('inf')
   patience = 5
   counter = 0
   
   for epoch in range(max_epochs):
       val_loss = validate(model)
       if val_loss < best_val_loss:
           best_val_loss = val_loss
           save_model(model)
           counter = 0
       else:
           counter += 1
           if counter >= patience:
               print("Early stopping!")
               break

4ï¸âƒ£ Batch Normalization
   æ ‡å‡†åŒ–æ¯å±‚çš„è¾“å…¥
   
   class ModelWithBN(nn.Module):
       def __init__(self):
           self.fc1 = nn.Linear(768, 384)
           self.bn = nn.BatchNorm1d(384)  # âœ… æ‰¹å½’ä¸€åŒ–
           self.fc2 = nn.Linear(384, 27)
       
       def forward(self, x):
           x = self.fc1(x)
           x = self.bn(x)  # å½’ä¸€åŒ–
           x = F.relu(x)
           x = self.fc2(x)
           return x

5ï¸âƒ£ Data Augmentationï¼ˆæ•°æ®å¢å¼ºï¼‰
   å¯¹DNAåºåˆ—ï¼š
   - åå‘äº’è¡¥
   - éšæœºçªå˜ï¼ˆæ…ç”¨ï¼‰
   - æ»‘åŠ¨çª—å£
   
   æ•ˆæœï¼š
   - å¢åŠ æ•°æ®å¤šæ ·æ€§
   - æé«˜æ¨¡å‹é²æ£’æ€§

6ï¸âƒ£ å†»ç»“å±‚ï¼ˆLayer Freezingï¼‰
   åªè®­ç»ƒéƒ¨åˆ†å±‚ï¼Œå†»ç»“å…¶ä»–å±‚
   
   # å†»ç»“å‰6å±‚
   for i, layer in enumerate(model.encoder.layer):
       if i < 6:
           for param in layer.parameters():
               param.requires_grad = False
""")

# ============= 7. å¦‚ä½•é€‰æ‹©æ­£åˆ™åŒ–å¼ºåº¦ï¼Ÿ =============
print("\nğŸ“Œ 7. å¦‚ä½•é€‰æ‹©æ­£åˆ™åŒ–å¼ºåº¦ï¼Ÿ")
print("-"*70)

print("""
Weight Decayï¼ˆÎ»ï¼‰çš„é€‰æ‹©ï¼š

å¤ªå°ï¼ˆÎ» < 0.0001ï¼‰:
  âŒ æ­£åˆ™åŒ–æ•ˆæœä¸æ˜æ˜¾
  âŒ ä»ç„¶å®¹æ˜“è¿‡æ‹Ÿåˆ

é€‚ä¸­ï¼ˆÎ» = 0.001 - 0.1ï¼‰:
  âœ… å¹³è¡¡æ‹Ÿåˆå’Œæ³›åŒ–
  âœ… å¤§å¤šæ•°æƒ…å†µçš„æ¨èå€¼

å¤ªå¤§ï¼ˆÎ» > 0.5ï¼‰:
  âŒ æ¬ æ‹Ÿåˆ
  âŒ æ¨¡å‹å­¦ä¸åˆ°ä¸œè¥¿

å¸¸ç”¨å€¼ï¼š
  - å›¾åƒåˆ†ç±»: 0.0001 - 0.001
  - NLP: 0.01 - 0.1
  - åŸºå› ç»„å­¦: 0.01ï¼ˆä½ çš„ä»»åŠ¡æ¨èå€¼ï¼‰

Dropout Rateçš„é€‰æ‹©ï¼š

å¤ªå°ï¼ˆ< 0.1ï¼‰:
  âŒ æ•ˆæœä¸æ˜æ˜¾

é€‚ä¸­ï¼ˆ0.3 - 0.5ï¼‰:
  âœ… æ¨èèŒƒå›´
  âœ… 0.4æ˜¯å¸¸ç”¨å€¼ï¼ˆä½ çš„æ¨¡å‹ä½¿ç”¨ï¼‰

å¤ªå¤§ï¼ˆ> 0.7ï¼‰:
  âŒ æ‰”æ‰å¤ªå¤šä¿¡æ¯
  âŒ è®­ç»ƒä¸ç¨³å®š

ç»éªŒæ³•åˆ™ï¼š
  - å…¨è¿æ¥å±‚: 0.5
  - RNN: 0.2 - 0.5
  - Transformer: 0.1 - 0.3
  - ä»»åŠ¡å¤´: 0.3 - 0.5ï¼ˆä½ çš„ä»»åŠ¡ï¼‰

è°ƒå‚å»ºè®®ï¼š
  1. ä»å°å¼€å§‹ï¼ˆÎ»=0.01, dropout=0.1ï¼‰
  2. è§‚å¯Ÿè®­ç»ƒ/éªŒè¯æ›²çº¿
  3. å¦‚æœè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œå¢å¤§æ­£åˆ™åŒ–
  4. å¦‚æœæ¬ æ‹Ÿåˆï¼Œå‡å°æ­£åˆ™åŒ–
  5. ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–
""")

plt.show()

print("\n" + "="*70)
print("âœ… æ­£åˆ™åŒ–å’ŒDropoutè¯¦è§£å®Œæˆï¼")
print("="*70)

print("\nğŸ’¡ å…³é”®è¦ç‚¹ï¼š")
print("  1. æ­£åˆ™åŒ– = é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯")
print("  2. Weight Decay = æƒ©ç½šå¤§æƒé‡")
print("  3. Dropout = éšæœºå…³é—­ç¥ç»å…ƒ")
print("  4. ä¸¤è€…å¯ä»¥åŒæ—¶ä½¿ç”¨")
print("  5. åˆç†é€‰æ‹©å¼ºåº¦å¾ˆé‡è¦")



