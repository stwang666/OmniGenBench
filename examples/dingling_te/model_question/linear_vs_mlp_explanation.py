# -*- coding: utf-8 -*-
# çº¿æ€§åˆ†ç±»å™¨ vs MLPåˆ†ç±»å™¨å¯¹æ¯”ç¤ºä¾‹

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

print("="*70)
print("ğŸ”¢ çº¿æ€§åˆ†ç±»å™¨ vs MLPåˆ†ç±»å™¨è¯¦è§£")
print("="*70)

# ============= 1. çº¿æ€§åˆ†ç±»å™¨ï¼ˆå•å±‚ï¼‰ =============
print("\nğŸ“Œ 1. çº¿æ€§åˆ†ç±»å™¨ï¼ˆLinear Classifierï¼‰")
print("-"*70)

hidden_size = 768  # å‡è®¾transformerè¾“å‡º768ç»´
num_classes = 27   # 9ä¸ªç»„ç»‡ Ã— 3ä¸ªç±»åˆ« = 27

# å•å±‚çº¿æ€§åˆ†ç±»å™¨
linear_classifier = nn.Linear(hidden_size, num_classes)

print(f"è¾“å…¥ç»´åº¦: {hidden_size}")
print(f"è¾“å‡ºç»´åº¦: {num_classes}")
print(f"å‚æ•°æ•°é‡: {hidden_size * num_classes + num_classes:,}")
print(f"  = æƒé‡çŸ©é˜µ W: {hidden_size} Ã— {num_classes} = {hidden_size * num_classes:,}")
print(f"  + åç½®å‘é‡ b: {num_classes}")

# ç¤ºä¾‹å‰å‘ä¼ æ’­
batch_size = 2
input_features = torch.randn(batch_size, hidden_size)
output = linear_classifier(input_features)

print(f"\nå‰å‘ä¼ æ’­ç¤ºä¾‹:")
print(f"  è¾“å…¥: {input_features.shape}  # [batch_size, hidden_size]")
print(f"  è¾“å‡º: {output.shape}  # [batch_size, num_classes]")

print(f"\næ•°å­¦å…¬å¼:")
print(f"  output = input @ W^T + b")
print(f"  å…¶ä¸­:")
print(f"    input: [{batch_size}, {hidden_size}]")
print(f"    W: [{num_classes}, {hidden_size}]")
print(f"    b: [{num_classes}]")
print(f"    output: [{batch_size}, {num_classes}]")

# ============= 2. MLPåˆ†ç±»å™¨ï¼ˆå¤šå±‚ï¼‰ =============
print("\n\nğŸ“Œ 2. MLPåˆ†ç±»å™¨ï¼ˆMulti-Layer Perceptronï¼‰")
print("-"*70)

# å¤šå±‚MLPåˆ†ç±»å™¨
mlp_classifier = nn.Sequential(
    nn.Linear(hidden_size, hidden_size // 2),  # ç¬¬1å±‚: 768 â†’ 384
    nn.ReLU(),                                  # æ¿€æ´»å‡½æ•°
    nn.Dropout(0.4),                           # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    nn.Linear(hidden_size // 2, num_classes)   # ç¬¬2å±‚: 384 â†’ 27
)

# è®¡ç®—å‚æ•°é‡
mlp_params = (hidden_size * (hidden_size // 2) + hidden_size // 2) + \
             ((hidden_size // 2) * num_classes + num_classes)

print(f"å±‚ç»“æ„:")
print(f"  ç¬¬1å±‚: Linear({hidden_size} â†’ {hidden_size // 2})")
print(f"  ç¬¬2å±‚: ReLU()")
print(f"  ç¬¬3å±‚: Dropout(0.4)")
print(f"  ç¬¬4å±‚: Linear({hidden_size // 2} â†’ {num_classes})")

print(f"\nå‚æ•°æ•°é‡: {mlp_params:,}")
print(f"  = ç¬¬1å±‚: {hidden_size * (hidden_size // 2) + hidden_size // 2:,}")
print(f"  + ç¬¬2å±‚: {(hidden_size // 2) * num_classes + num_classes:,}")

# ç¤ºä¾‹å‰å‘ä¼ æ’­
output_mlp = mlp_classifier(input_features)
print(f"\nå‰å‘ä¼ æ’­ç¤ºä¾‹:")
print(f"  è¾“å…¥: {input_features.shape}  # [batch_size, hidden_size]")
print(f"  â†’ ç¬¬1å±‚: [{batch_size}, {hidden_size // 2}]")
print(f"  â†’ ReLUæ¿€æ´»")
print(f"  â†’ Dropout")
print(f"  â†’ ç¬¬2å±‚: [{batch_size}, {num_classes}]")
print(f"  è¾“å‡º: {output_mlp.shape}")

# ============= 3. å¯¹æ¯”åˆ†æ =============
print("\n\nğŸ“Œ 3. çº¿æ€§ vs MLP å¯¹æ¯”")
print("-"*70)

linear_params = hidden_size * num_classes + num_classes
comparison = [
    ["ç‰¹æ€§", "çº¿æ€§åˆ†ç±»å™¨", "MLPåˆ†ç±»å™¨"],
    ["å±‚æ•°", "1å±‚", "2+å±‚"],
    ["å‚æ•°é‡", f"{linear_params:,}", f"{mlp_params:,}"],
    ["éçº¿æ€§", "âŒ æ— ", "âœ… æœ‰ï¼ˆReLUï¼‰"],
    ["è¡¨è¾¾èƒ½åŠ›", "è¾ƒå¼±ï¼ˆçº¿æ€§å˜æ¢ï¼‰", "è¾ƒå¼ºï¼ˆå¯æ‹Ÿåˆå¤æ‚å‡½æ•°ï¼‰"],
    ["è¿‡æ‹Ÿåˆé£é™©", "è¾ƒä½", "è¾ƒé«˜ï¼ˆä½†å¯ç”¨Dropoutæ§åˆ¶ï¼‰"],
    ["è®­ç»ƒé€Ÿåº¦", "å¿«", "ç¨æ…¢"],
    ["é€‚ç”¨åœºæ™¯", "ç®€å•ä»»åŠ¡ã€ç‰¹å¾å·²å¥½", "å¤æ‚ä»»åŠ¡ã€éœ€è¦ç‰¹å¾å˜æ¢"],
]

for row in comparison:
    print(f"{row[0]:15s} | {row[1]:25s} | {row[2]:35s}")

# ============= 4. å¯è§†åŒ–å†³ç­–è¾¹ç•Œ =============
print("\n\nğŸ“Œ 4. å†³ç­–è¾¹ç•Œå¯è§†åŒ–ï¼ˆ2Dç¤ºä¾‹ï¼‰")
print("-"*70)

# ä¸ºäº†å¯è§†åŒ–ï¼Œæˆ‘ä»¬ç”¨2Dè¾“å…¥ã€2ç±»è¾“å‡ºçš„ç®€åŒ–ç‰ˆæœ¬
np.random.seed(42)

# ç”Ÿæˆ2Dæ•°æ®
X = np.random.randn(200, 2)
y = (X[:, 0] + X[:, 1] > 0).astype(int)  # ç®€å•çº¿æ€§å¯åˆ†

# çº¿æ€§åˆ†ç±»å™¨ï¼ˆ2D â†’ 2ç±»ï¼‰
linear_2d = nn.Linear(2, 2)
linear_2d.weight.data = torch.tensor([[1.0, 1.0], [-1.0, -1.0]])
linear_2d.bias.data = torch.tensor([0.0, 0.0])

# MLPåˆ†ç±»å™¨ï¼ˆ2D â†’ 2ç±»ï¼‰
mlp_2d = nn.Sequential(
    nn.Linear(2, 10),
    nn.ReLU(),
    nn.Linear(10, 2)
)

# åˆ›å»ºç½‘æ ¼
xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))
grid = np.c_[xx.ravel(), yy.ravel()]
grid_tensor = torch.FloatTensor(grid)

# é¢„æµ‹
with torch.no_grad():
    Z_linear = linear_2d(grid_tensor).argmax(dim=1).numpy().reshape(xx.shape)
    Z_mlp = mlp_2d(grid_tensor).argmax(dim=1).numpy().reshape(xx.shape)

# ç»˜å›¾
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# çº¿æ€§åˆ†ç±»å™¨å†³ç­–è¾¹ç•Œ
axes[0].contourf(xx, yy, Z_linear, alpha=0.3, levels=1, cmap='RdYlBu')
axes[0].scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', label='ç±»åˆ«0', edgecolors='k', alpha=0.7)
axes[0].scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', label='ç±»åˆ«1', edgecolors='k', alpha=0.7)
axes[0].set_title('çº¿æ€§åˆ†ç±»å™¨\nå†³ç­–è¾¹ç•Œæ˜¯ç›´çº¿', fontsize=12, fontweight='bold')
axes[0].set_xlabel('ç‰¹å¾1')
axes[0].set_ylabel('ç‰¹å¾2')
axes[0].legend()
axes[0].grid(alpha=0.3)

# MLPåˆ†ç±»å™¨å†³ç­–è¾¹ç•Œ
axes[1].contourf(xx, yy, Z_mlp, alpha=0.3, levels=1, cmap='RdYlBu')
axes[1].scatter(X[y==0, 0], X[y==0, 1], c='red', marker='o', label='ç±»åˆ«0', edgecolors='k', alpha=0.7)
axes[1].scatter(X[y==1, 0], X[y==1, 1], c='blue', marker='s', label='ç±»åˆ«1', edgecolors='k', alpha=0.7)
axes[1].set_title('MLPåˆ†ç±»å™¨\nå†³ç­–è¾¹ç•Œå¯ä»¥æ˜¯æ›²çº¿', fontsize=12, fontweight='bold')
axes[1].set_xlabel('ç‰¹å¾1')
axes[1].set_ylabel('ç‰¹å¾2')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/linear_vs_mlp.png', dpi=300, bbox_inches='tight')
print("ğŸ’¾ å†³ç­–è¾¹ç•Œå›¾å·²ä¿å­˜: linear_vs_mlp.png")
plt.show()

# ============= 5. ä¸ºä»€ä¹ˆä½ çš„æ¨¡å‹æ˜¯27ç»´è¾“å‡ºï¼Ÿ =============
print("\n\nğŸ“Œ 5. ä¸ºä»€ä¹ˆè¾“å‡ºæ˜¯27ç»´ï¼Ÿ")
print("-"*70)

num_tissues = 9
num_class_per_tissue = 3
total_output = num_tissues * num_class_per_tissue

print(f"ä»»åŠ¡: é¢„æµ‹9ä¸ªç»„ç»‡çš„è¡¨è¾¾æ°´å¹³ï¼ˆLow/Medium/Highï¼‰")
print(f"\næ–¹æ³•1: Flattenè¾“å‡ºï¼ˆåŸå§‹ä»£ç ä½¿ç”¨ï¼‰")
print(f"  è¾“å‡ºç»´åº¦: {num_tissues} Ã— {num_class_per_tissue} = {total_output}")
print(f"  è¾“å‡ºå½¢çŠ¶: [batch, {total_output}]")
print(f"  ç„¶åreshape: [batch, {total_output}] â†’ [batch, {num_tissues}, {num_class_per_tissue}]")
print(f"\nç¤ºä¾‹:")
print(f"  Linearè¾“å‡º: [2, 27]")
print(f"  Reshapeå: [2, 9, 3]")
print(f"    â”œâ”€ batchç»´åº¦: 2ä¸ªæ ·æœ¬")
print(f"    â”œâ”€ tissueç»´åº¦: 9ä¸ªç»„ç»‡")
print(f"    â””â”€ classç»´åº¦: 3ä¸ªç±»åˆ«ï¼ˆLow/Medium/Highï¼‰")

print(f"\nå¯¹äºæ¯ä¸ªç»„ç»‡ï¼Œåº”ç”¨Softmaxå¾—åˆ°3ä¸ªç±»åˆ«çš„æ¦‚ç‡:")
print(f"  logits[i, j, :] â†’ softmax â†’ [P(Low), P(Medium), P(High)]")

# ============= 6. ä»£ç å®ç°å¯¹æ¯” =============
print("\n\nğŸ“Œ 6. å®é™…ä»£ç å¯¹æ¯”")
print("-"*70)

print("\nåŸå§‹ä»£ç ï¼ˆå•å±‚Linearï¼‰:")
print("""
class OriginalModel:
    def __init__(self, hidden_size=768):
        self.classifier = nn.Linear(hidden_size, 27)  # å•å±‚
    
    def forward(self, pooled_output):
        logits = self.classifier(pooled_output)  # [batch, 27]
        logits = logits.view(batch_size, 9, 3)   # reshape
        return logits
""")

print("\næ”¹è¿›ä»£ç ï¼ˆå¤šå±‚MLPï¼‰:")
print("""
class ImprovedModel:
    def __init__(self, hidden_size=768):
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),  # 768 â†’ 384
            nn.ReLU(),                                 # éçº¿æ€§æ¿€æ´»
            nn.Dropout(0.4),                          # æ­£åˆ™åŒ–
            nn.Linear(hidden_size // 2, 27)           # 384 â†’ 27
        )
    
    def forward(self, pooled_output):
        logits = self.classifier(pooled_output)  # [batch, 27]
        logits = logits.view(batch_size, 9, 3)   # reshape
        return logits
""")

print("\nä¼˜åŠ¿:")
print("  âœ… MLPæœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼ˆå¯ä»¥å­¦ä¹ éçº¿æ€§å…³ç³»ï¼‰")
print("  âœ… ä¸­é—´å±‚å¯ä»¥å­¦ä¹ æ›´å¥½çš„ç‰¹å¾è¡¨ç¤º")
print("  âœ… Dropoutå¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆ")

print("\n" + "="*70)
print("âœ… è¯¦è§£å®Œæˆï¼")



