# -*- coding: utf-8 -*-
# æ£€æŸ¥æ¨¡å‹é…ç½®å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

import torch
from omnigenbench import OmniTokenizer, ModelHub

print("="*70)
print("ğŸ” æ£€æŸ¥æ¨¡å‹é…ç½®")
print("="*70)

# åŠ è½½æ¨¡å‹
model_path = "yangheng/OmniGenome-52M"
print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

try:
    tokenizer = OmniTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # æ–¹æ³•1ï¼šç›´æ¥åŠ è½½æ¨¡å‹æŸ¥çœ‹é…ç½®
    from transformers import AutoModel, AutoConfig
    
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    
    print("\nğŸ“Š æ¨¡å‹é…ç½®ä¿¡æ¯ï¼š")
    print("-"*70)
    
    # å…³é”®é…ç½®
    if hasattr(config, 'hidden_size'):
        print(f"âœ… hidden_size (éšè—å±‚ç»´åº¦): {config.hidden_size}")
    
    if hasattr(config, 'num_attention_heads'):
        print(f"âœ… num_attention_heads (æ³¨æ„åŠ›å¤´æ•°): {config.num_attention_heads}")
        if hasattr(config, 'hidden_size'):
            head_dim = config.hidden_size // config.num_attention_heads
            print(f"âœ… æ¯ä¸ªå¤´çš„ç»´åº¦: {head_dim} (= {config.hidden_size} / {config.num_attention_heads})")
    
    if hasattr(config, 'num_hidden_layers'):
        print(f"âœ… num_hidden_layers (Transformerå±‚æ•°): {config.num_hidden_layers}")
    
    if hasattr(config, 'intermediate_size'):
        print(f"âœ… intermediate_size (FFNä¸­é—´å±‚ç»´åº¦): {config.intermediate_size}")
    
    if hasattr(config, 'vocab_size'):
        print(f"âœ… vocab_size (è¯è¡¨å¤§å°): {config.vocab_size}")
    
    if hasattr(config, 'max_position_embeddings'):
        print(f"âœ… max_position_embeddings (æœ€å¤§åºåˆ—é•¿åº¦): {config.max_position_embeddings}")
    
    print("\nå®Œæ•´é…ç½®ï¼š")
    print("-"*70)
    print(config)
    
except Exception as e:
    print(f"âš ï¸ åŠ è½½é…ç½®æ—¶å‡ºé”™: {e}")
    print("\nä½¿ç”¨é»˜è®¤é…ç½®è¿›è¡Œè¯´æ˜...")

print("\n" + "="*70)


