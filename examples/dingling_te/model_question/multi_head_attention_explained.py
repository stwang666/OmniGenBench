# -*- coding: utf-8 -*-
# å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£ï¼ˆé’ˆå¯¹hidden_size=480çš„æƒ…å†µï¼‰

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("="*70)
print("ðŸŽ¯ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰è¯¦è§£")
print("="*70)

# ============= 1. æ˜¯çš„ï¼Transformeréƒ½ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ› =============
print("\nðŸ“Œ 1. ä½ çš„æ¨¡åž‹ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›å—ï¼Ÿ")
print("-"*70)

print("""
ç­”æ¡ˆï¼šâœ… æ˜¯çš„ï¼

æ‰€æœ‰åŸºäºŽTransformerçš„æ¨¡åž‹éƒ½ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

OmniGenomeç³»åˆ—æ¨¡åž‹ï¼š
  - OmniGenome-52M: âœ… ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›
  - OmniGenome-v1.5: âœ… ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›
  - å…¶ä»–Transformeræ¨¡åž‹: âœ… éƒ½ä½¿ç”¨

è¿™æ˜¯Transformeræž¶æž„çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼
""")

# ============= 2. ä»¥hidden_size=480ä¸ºä¾‹ =============
print("\nðŸ“Œ 2. å‡è®¾ä½ çš„æ¨¡åž‹é…ç½®ï¼ˆhidden_size=480ï¼‰")
print("-"*70)

# æ¨¡åž‹é…ç½®
hidden_size = 480
num_heads = 8  # å¸¸è§é…ç½®
head_dim = hidden_size // num_heads

print(f"""
æ¨¡åž‹é…ç½®ç¤ºä¾‹ï¼š
  - hidden_size (d_model): {hidden_size}
  - num_attention_heads: {num_heads}
  - æ¯ä¸ªå¤´çš„ç»´åº¦ (d_k): {head_dim} (= {hidden_size} / {num_heads})

éªŒè¯ï¼š{num_heads} ä¸ªå¤´ Ã— {head_dim} ç»´/å¤´ = {num_heads * head_dim} ç»´ âœ…
""")

# ============= 3. å•å¤´ vs å¤šå¤´å¯¹æ¯” =============
print("\nðŸ“Œ 3. å•å¤´æ³¨æ„åŠ› vs å¤šå¤´æ³¨æ„åŠ›")
print("-"*70)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–¹æ¡ˆA: å•å¤´æ³¨æ„åŠ›ï¼ˆå‡è®¾çš„ï¼Œå®žé™…ä¸ç”¨ï¼‰                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: [batch, seq_len, 480]
  â†“
Q = X @ W_q  â†’  [batch, seq_len, 480]
K = X @ W_k  â†’  [batch, seq_len, 480]
V = X @ W_v  â†’  [batch, seq_len, 480]
  â†“
Attention = softmax(QK^T/âˆš480) @ V
  â†“
è¾“å‡º: [batch, seq_len, 480]

âŒ é—®é¢˜ï¼š
  - åªæœ‰ä¸€ç§æ³¨æ„åŠ›æ¨¡å¼
  - åªèƒ½å­¦ä¹ ä¸€ç§ç‰¹å¾å…³ç³»
  - è¡¨è¾¾èƒ½åŠ›å—é™


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ–¹æ¡ˆB: å¤šå¤´æ³¨æ„åŠ›ï¼ˆå®žé™…ä½¿ç”¨ï¼Œ8ä¸ªå¤´ï¼‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥: [batch, seq_len, 480]
  â†“
Splitæˆ8ä¸ªå¤´
  â†“
å¤´1: [batch, seq_len, 60]  â† å…³æ³¨å±€éƒ¨motif
å¤´2: [batch, seq_len, 60]  â† å…³æ³¨è¿œè·ç¦»äº’ä½œ
å¤´3: [batch, seq_len, 60]  â† å…³æ³¨GCå«é‡
å¤´4: [batch, seq_len, 60]  â† å…³æ³¨é‡å¤åºåˆ—
å¤´5: [batch, seq_len, 60]  â† å…³æ³¨å¯†ç å­
å¤´6: [batch, seq_len, 60]  â† å…³æ³¨å‰ªæŽ¥ä½ç‚¹
å¤´7: [batch, seq_len, 60]  â† å…³æ³¨äºŒçº§ç»“æž„
å¤´8: [batch, seq_len, 60]  â† å…³æ³¨ä¿å®ˆåŒºåŸŸ
  â†“
æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—attention
  â†“
Concatæ‰€æœ‰å¤´: [batch, seq_len, 480]
  â†“
çº¿æ€§å˜æ¢: [batch, seq_len, 480]
  â†“
è¾“å‡º: [batch, seq_len, 480]

âœ… ä¼˜ç‚¹ï¼š
  - 8ç§ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
  - å¯ä»¥åŒæ—¶å…³æ³¨å¤šç§ç‰¹å¾
  - è¡¨è¾¾èƒ½åŠ›å¼ºå¤§
""")

# ============= 4. è¯¦ç»†è®¡ç®—è¿‡ç¨‹ =============
print("\nðŸ“Œ 4. å¤šå¤´æ³¨æ„åŠ›çš„è¯¦ç»†è®¡ç®—è¿‡ç¨‹")
print("-"*70)

print(f"""
å‡è®¾åºåˆ—é•¿åº¦ seq_len = 100

æ­¥éª¤1: çº¿æ€§æŠ•å½±
  è¾“å…¥: X [batch, 100, 480]
  
  Q = X @ W_q  â†’  [batch, 100, 480]
  K = X @ W_k  â†’  [batch, 100, 480]
  V = X @ W_v  â†’  [batch, 100, 480]

æ­¥éª¤2: åˆ†å‰²æˆå¤šä¸ªå¤´
  Q: [batch, 100, 480] â†’ [batch, {num_heads}, 100, {head_dim}]
  K: [batch, 100, 480] â†’ [batch, {num_heads}, 100, {head_dim}]
  V: [batch, 100, 480] â†’ [batch, {num_heads}, 100, {head_dim}]
  
  reshapeæ“ä½œï¼š
    [batch, 100, 480]
    â†’ [batch, 100, {num_heads}, {head_dim}]
    â†’ [batch, {num_heads}, 100, {head_dim}]

æ­¥éª¤3: æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
  å¯¹äºŽæ¯ä¸ªå¤´ h=1,2,...,{num_heads}:
    
    3.1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
      scores_h = Q_h @ K_h^T / âˆš{head_dim}
      [batch, 100, {head_dim}] @ [batch, {head_dim}, 100] = [batch, 100, 100]
    
    3.2: Softmaxå½’ä¸€åŒ–
      attention_weights_h = softmax(scores_h)
      [batch, 100, 100]  # æ¯è¡Œå’Œä¸º1
    
    3.3: åŠ æƒæ±‚å’Œ
      output_h = attention_weights_h @ V_h
      [batch, 100, 100] @ [batch, 100, {head_dim}] = [batch, 100, {head_dim}]

æ­¥éª¤4: æ‹¼æŽ¥æ‰€æœ‰å¤´çš„è¾“å‡º
  outputs = [output_1, output_2, ..., output_{num_heads}]
  concat(outputs) â†’ [batch, 100, {num_heads} Ã— {head_dim}]
                 = [batch, 100, {hidden_size}]

æ­¥éª¤5: æœ€ç»ˆçº¿æ€§å˜æ¢
  final_output = concat(outputs) @ W_o
  [batch, 100, 480] @ [480, 480] = [batch, 100, 480]
""")

# ============= 5. å¯è§†åŒ–å¤šå¤´æ³¨æ„åŠ› =============
print("\nðŸ“Œ 5. å¯è§†åŒ–ä¸åŒå¤´çš„æ³¨æ„åŠ›æ¨¡å¼")
print("-"*70)

# æ¨¡æ‹Ÿä¸€ä¸ªç®€çŸ­çš„DNAåºåˆ—
sequence = "ATCGATCGTAGCTAGC"
seq_len = len(sequence)

# ä¸º8ä¸ªå¤´ç”Ÿæˆä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()

np.random.seed(42)

attention_patterns = [
    "å±€éƒ¨æ³¨æ„åŠ›ï¼ˆè¿‘é‚»ï¼‰",
    "å…¨å±€æ³¨æ„åŠ›ï¼ˆå‡åŒ€ï¼‰",
    "ç¨€ç–æ³¨æ„åŠ›ï¼ˆè¿œè·ç¦»ï¼‰",
    "è‡ªæ³¨æ„åŠ›ï¼ˆå¯¹è§’çº¿ï¼‰",
    "å·¦ä¾§æ³¨æ„åŠ›ï¼ˆå‰æ–‡ï¼‰",
    "å³ä¾§æ³¨æ„åŠ›ï¼ˆåŽæ–‡ï¼‰",
    "å‘¨æœŸæ€§æ³¨æ„åŠ›",
    "éšæœºæ··åˆæ³¨æ„åŠ›"
]

for head_idx, (ax, pattern_name) in enumerate(zip(axes, attention_patterns)):
    # ä¸ºæ¯ä¸ªå¤´ç”Ÿæˆä¸åŒæ¨¡å¼çš„æ³¨æ„åŠ›çŸ©é˜µ
    attention = np.zeros((seq_len, seq_len))
    
    if head_idx == 0:  # å±€éƒ¨æ³¨æ„åŠ›
        for i in range(seq_len):
            for j in range(max(0, i-2), min(seq_len, i+3)):
                attention[i, j] = np.exp(-abs(i-j))
    
    elif head_idx == 1:  # å…¨å±€æ³¨æ„åŠ›
        attention = np.ones((seq_len, seq_len)) * 0.5
        attention += np.random.randn(seq_len, seq_len) * 0.1
    
    elif head_idx == 2:  # ç¨€ç–æ³¨æ„åŠ›
        for i in range(seq_len):
            for j in range(seq_len):
                if abs(i-j) > 3:
                    attention[i, j] = np.random.rand() * 0.5
    
    elif head_idx == 3:  # è‡ªæ³¨æ„åŠ›
        for i in range(seq_len):
            attention[i, i] = 1.0
            if i > 0:
                attention[i, i-1] = 0.3
            if i < seq_len-1:
                attention[i, i+1] = 0.3
    
    elif head_idx == 4:  # å·¦ä¾§æ³¨æ„åŠ›
        for i in range(seq_len):
            for j in range(i+1):
                attention[i, j] = (1.0 - (i-j)/seq_len)
    
    elif head_idx == 5:  # å³ä¾§æ³¨æ„åŠ›
        for i in range(seq_len):
            for j in range(i, seq_len):
                attention[i, j] = (1.0 - (j-i)/seq_len)
    
    elif head_idx == 6:  # å‘¨æœŸæ€§
        for i in range(seq_len):
            for j in range(seq_len):
                if (i - j) % 3 == 0:
                    attention[i, j] = 0.8
    
    else:  # éšæœºæ··åˆ
        attention = np.random.rand(seq_len, seq_len)
    
    # å½’ä¸€åŒ–ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
    attention = attention / (attention.sum(axis=1, keepdims=True) + 1e-9)
    
    # å¯è§†åŒ–
    sns.heatmap(attention, ax=ax, cmap='YlOrRd', cbar=True,
               xticklabels=list(sequence), yticklabels=list(sequence),
               vmin=0, vmax=attention.max())
    ax.set_title(f'å¤´ {head_idx+1}: {pattern_name}', fontsize=11, fontweight='bold')
    ax.set_xlabel('Keyä½ç½®')
    ax.set_ylabel('Queryä½ç½®')

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/multi_head_patterns.png', 
           dpi=300, bbox_inches='tight')
print("ðŸ’¾ å¤šå¤´æ³¨æ„åŠ›æ¨¡å¼å›¾å·²ä¿å­˜: multi_head_patterns.png\n")

# ============= 6. å¤šå¤´çš„ä¼˜ç‚¹ =============
print("\nðŸ“Œ 6. å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜ç‚¹è¯¦è§£")
print("-"*70)

print(f"""
ä¼˜ç‚¹1: ðŸŽ¯ æ•æ‰å¤šç§ç‰¹å¾å…³ç³»
  
  åœ¨DNAåºåˆ—åˆ†æžä¸­ï¼Œä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ï¼š
    å¤´1: TATA boxï¼ˆå¯åŠ¨å­å…ƒä»¶ï¼‰
    å¤´2: å‰ªæŽ¥ä½ç‚¹ï¼ˆGT-AGï¼‰
    å¤´3: CpGå²›ï¼ˆç”²åŸºåŒ–ç›¸å…³ï¼‰
    å¤´4: å¯†ç å­ä½¿ç”¨åå¥½
    å¤´5: é‡å¤åºåˆ—ï¼ˆLINE, SINEï¼‰
    å¤´6: ä¿å®ˆåŸŸ
    å¤´7: äºŒçº§ç»“æž„é…å¯¹
    å¤´8: GCå«é‡å˜åŒ–
  
  â†’ ä¸€ä¸ªå¤´åšä¸åˆ°è¿™ä¹ˆå¤šï¼éœ€è¦å¤šä¸ªå¤´å¹¶è¡Œå·¥ä½œ

ä¼˜ç‚¹2: ðŸ“Š æé«˜æ¨¡åž‹å®¹é‡
  
  å‚æ•°é‡å¯¹æ¯”ï¼ˆå‡è®¾hidden_size={hidden_size}ï¼‰:
  
  å•å¤´ï¼š
    W_q, W_k, W_v: 3 Ã— ({hidden_size} Ã— {hidden_size}) = {3 * hidden_size * hidden_size:,}
    æ€»å‚æ•°: ~{3 * hidden_size * hidden_size:,}
  
  å¤šå¤´ï¼ˆ{num_heads}ä¸ªå¤´ï¼‰ï¼š
    æ¯ä¸ªå¤´: 3 Ã— ({hidden_size} Ã— {head_dim}) = {3 * hidden_size * head_dim:,}
    {num_heads}ä¸ªå¤´: {num_heads} Ã— {3 * hidden_size * head_dim:,} = {num_heads * 3 * hidden_size * head_dim:,}
    è¾“å‡ºæŠ•å½±: {hidden_size} Ã— {hidden_size} = {hidden_size * hidden_size:,}
    æ€»å‚æ•°: ~{num_heads * 3 * hidden_size * head_dim + hidden_size * hidden_size:,}
  
  â†’ å‚æ•°é‡ç›¸è¿‘ï¼Œä½†å¤šå¤´è¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼

ä¼˜ç‚¹3: ðŸ›¡ï¸ é˜²æ­¢è¿‡æ‹Ÿåˆ
  
  åˆ†æˆå¤šä¸ªå°å¤´ï¼ˆæ¯ä¸ªåªæœ‰{head_dim}ç»´ï¼‰ï¼š
    - æ¯ä¸ªå¤´çš„å‚æ•°æ›´å°‘
    - å¼ºåˆ¶å­¦ä¹ ä¸åŒçš„ç‰¹å¾
    - ç±»ä¼¼äºŽé›†æˆå­¦ä¹ çš„æ•ˆæžœ
  
  â†’ æ¯”ä¸€ä¸ªå¤§å¤´æ›´é²æ£’

ä¼˜ç‚¹4: âš¡ å¹¶è¡Œè®¡ç®—æ•ˆçŽ‡
  
  {num_heads}ä¸ªå¤´å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼š
    - GPUå¹¶è¡Œå¤„ç†å¤šä¸ªå¤´
    - è®¡ç®—æ—¶é—´ â‰ˆ å•å¤´çš„æ—¶é—´ï¼ˆç†æƒ³æƒ…å†µï¼‰
    - ä½†èŽ·å¾—äº†{num_heads}å€çš„ç‰¹å¾æå–èƒ½åŠ›
  
  â†’ æ€§èƒ½æå‡æ˜Žæ˜¾ï¼

ä¼˜ç‚¹5: ðŸŽ¨ å¯è§£é‡Šæ€§
  
  å¯ä»¥æŸ¥çœ‹ä¸åŒå¤´å…³æ³¨çš„å†…å®¹ï¼š
    - å¤´1å…³æ³¨ä»€ä¹ˆï¼Ÿâ†’ å¯è§†åŒ–
    - å¤´2å…³æ³¨ä»€ä¹ˆï¼Ÿâ†’ å¯è§†åŒ–
    - å“ªäº›å¤´å¯¹ä»»åŠ¡æœ€é‡è¦ï¼Ÿâ†’ åˆ†æž
  
  â†’ å¸®åŠ©ç†è§£æ¨¡åž‹çš„å·¥ä½œæœºåˆ¶
""")

# ============= 7. ä¸ºä»€ä¹ˆè¦åˆ†æˆ8ä¸ªå¤´ï¼Ÿ =============
print("\nðŸ“Œ 7. ä¸ºä»€ä¹ˆé€šå¸¸æ˜¯8ä¸ªå¤´ï¼Ÿ")
print("-"*70)

print("""
å¤´æ•°é€‰æ‹©çš„è€ƒé‡ï¼š

å¤ªå°‘ï¼ˆ2-4ä¸ªå¤´ï¼‰ï¼š
  âŒ ç‰¹å¾å¤šæ ·æ€§ä¸è¶³
  âŒ è¡¨è¾¾èƒ½åŠ›æœ‰é™
  
é€‚ä¸­ï¼ˆ8-12ä¸ªå¤´ï¼‰ï¼šâœ… å¸¸ç”¨é…ç½®
  âœ… å¹³è¡¡æ€§èƒ½å’Œæ•ˆçŽ‡
  âœ… è¶³å¤Ÿçš„ç‰¹å¾å¤šæ ·æ€§
  âœ… è®¡ç®—å¼€é”€åˆç†
  
å¤ªå¤šï¼ˆ32+ä¸ªå¤´ï¼‰ï¼š
  âŒ è®¡ç®—å¼€é”€å¤§
  âŒ æ¯ä¸ªå¤´ç»´åº¦å¤ªå°
  âŒ æ”¶ç›Šé€’å‡

å¸¸è§é…ç½®ï¼š
  - BERT-base: 12ä¸ªå¤´ (hidden=768, head_dim=64)
  - GPT-2: 12ä¸ªå¤´
  - OmniGenome: 8ä¸ªå¤´ï¼ˆå¯èƒ½ï¼‰
  - ä½ çš„æ¨¡åž‹: å‡è®¾8ä¸ªå¤´ (hidden=480, head_dim=60)

ç»éªŒè§„åˆ™ï¼š
  - head_dimé€šå¸¸æ˜¯64æˆ–æ›´å°
  - æ€»ç»´åº¦hidden_sizeè¦èƒ½è¢«å¤´æ•°æ•´é™¤
  - å¤´æ•°æ˜¯2çš„å¹‚æ¬¡ï¼ˆ8, 16, 32ï¼‰æ–¹ä¾¿è®¡ç®—
""")

# ============= 8. å®žé™…ä»£ç ç¤ºä¾‹ =============
print("\nðŸ“Œ 8. PyTorchå®žçŽ°å¤šå¤´æ³¨æ„åŠ›")
print("-"*70)

code = f'''
class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size={hidden_size}, num_heads={num_heads}):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # {head_dim}
        
        assert hidden_size % num_heads == 0, "hidden_sizeå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        # Q, K, Vçš„æŠ•å½±çŸ©é˜µ
        self.W_q = nn.Linear(hidden_size, hidden_size)
        self.W_k = nn.Linear(hidden_size, hidden_size)
        self.W_v = nn.Linear(hidden_size, hidden_size)
        
        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # æ­¥éª¤1: çº¿æ€§æŠ•å½±
        Q = self.W_q(x)  # [batch, seq_len, {hidden_size}]
        K = self.W_k(x)
        V = self.W_v(x)
        
        # æ­¥éª¤2: åˆ†å‰²æˆå¤šä¸ªå¤´
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        Q = Q.transpose(1, 2)  # [batch, {num_heads}, seq_len, {head_dim}]
        
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.transpose(1, 2)
        
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.transpose(1, 2)
        
        # æ­¥éª¤3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        # scores: [batch, {num_heads}, seq_len, seq_len]
        
        # æ­¥éª¤4: Softmax
        attention_weights = torch.softmax(scores, dim=-1)
        
        # æ­¥éª¤5: åŠ æƒæ±‚å’Œ
        attention_output = torch.matmul(attention_weights, V)
        # [batch, {num_heads}, seq_len, {head_dim}]
        
        # æ­¥éª¤6: æ‹¼æŽ¥æ‰€æœ‰å¤´
        attention_output = attention_output.transpose(1, 2)
        # [batch, seq_len, {num_heads}, {head_dim}]
        
        attention_output = attention_output.contiguous().view(
            batch_size, seq_len, self.hidden_size
        )
        # [batch, seq_len, {hidden_size}]
        
        # æ­¥éª¤7: è¾“å‡ºæŠ•å½±
        output = self.W_o(attention_output)
        
        return output, attention_weights

# ä½¿ç”¨ç¤ºä¾‹
mha = MultiHeadAttention(hidden_size={hidden_size}, num_heads={num_heads})
x = torch.randn(2, 100, {hidden_size})  # [batch=2, seq_len=100, hidden={hidden_size}]
output, attn_weights = mha(x)

print(f"è¾“å…¥å½¢çŠ¶: {{x.shape}}")  # [2, 100, {hidden_size}]
print(f"è¾“å‡ºå½¢çŠ¶: {{output.shape}}")  # [2, 100, {hidden_size}]
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {{attn_weights.shape}}")  # [2, {num_heads}, 100, 100]
'''

print(code)

# ============= 9. ç»´åº¦å˜åŒ–å¯è§†åŒ– =============
print("\nðŸ“Œ 9. å¤šå¤´æ³¨æ„åŠ›ä¸­çš„ç»´åº¦å˜åŒ–")
print("-"*70)

fig, ax = plt.subplots(figsize=(14, 10))
ax.axis('off')

# ç»˜åˆ¶æµç¨‹å›¾
steps = [
    (0.5, 0.95, f"è¾“å…¥: [batch, seq_len, {hidden_size}]", "lightblue"),
    (0.5, 0.85, f"â†“ çº¿æ€§æŠ•å½± (W_q, W_k, W_v)", "white"),
    (0.5, 0.75, f"Q, K, V: [batch, seq_len, {hidden_size}]", "lightgreen"),
    (0.5, 0.65, f"â†“ Reshape + Transpose", "white"),
    (0.5, 0.55, f"Q, K, V: [batch, {num_heads}, seq_len, {head_dim}]", "lightyellow"),
    (0.5, 0.45, f"â†“ æ¯ä¸ªå¤´è®¡ç®— Attention", "white"),
    (0.5, 0.35, f"è¾“å‡º: [batch, {num_heads}, seq_len, {head_dim}]", "lightcoral"),
    (0.5, 0.25, f"â†“ Transpose + Reshape", "white"),
    (0.5, 0.15, f"Concat: [batch, seq_len, {hidden_size}]", "lightpink"),
    (0.5, 0.05, f"â†“ è¾“å‡ºæŠ•å½± (W_o)\næœ€ç»ˆ: [batch, seq_len, {hidden_size}]", "lightblue"),
]

for x, y, text, color in steps:
    if "â†“" in text:
        ax.text(x, y, text, ha='center', va='center', fontsize=12,
               bbox=dict(boxstyle='round', facecolor='white', alpha=0))
    else:
        ax.text(x, y, text, ha='center', va='center', fontsize=11,
               bbox=dict(boxstyle='round', facecolor=color, edgecolor='black', linewidth=2),
               fontweight='bold')

# æ·»åŠ è¯´æ˜Ž
ax.text(0.5, 0.02, 
       f'æ³¨æ„: hidden_sizeä¿æŒ{hidden_size}ä¸å˜ï¼Œåªæ˜¯ä¸­é—´ä¸´æ—¶åˆ†æˆ{num_heads}ä¸ªå¤´å¤„ç†',
       ha='center', fontsize=10, style='italic',
       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))

plt.tight_layout()
plt.savefig('/home/sw1136/OmniGenBench/examples/dingling_te/multi_head_dimensions.png', 
           dpi=300, bbox_inches='tight')
print("ðŸ’¾ ç»´åº¦å˜åŒ–å›¾å·²ä¿å­˜: multi_head_dimensions.png\n")

plt.show()

print("\n" + "="*70)
print("âœ… å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£å®Œæˆï¼")
print("="*70)

print("\nðŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“ï¼š")
print("  1. âœ… ä½ çš„æ¨¡åž‹ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ‰€æœ‰Transformeréƒ½ç”¨ï¼‰")
print(f"  2. hidden_size={hidden_size} é€šå¸¸åˆ†æˆ{num_heads}ä¸ªå¤´")
print(f"  3. æ¯ä¸ªå¤´å¤„ç†{head_dim}ç»´çš„å­ç©ºé—´")
print("  4. å¤šå¤´è®©æ¨¡åž‹åŒæ—¶å…³æ³¨å¤šç§ç‰¹å¾æ¨¡å¼")
print("  5. æé«˜è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆçŽ‡")


