# -*- coding: utf-8 -*-
# file: explainable_triclass.py
# time: 09:35 19/10/2025
# author: Explainability Analysis for TriClass TE Model
# Copyright (C) 2019-2025. All Rights Reserved.

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from omnigenbench import (
    ModelHub,
    OmniTokenizer,
    OmniDatasetForMultiLabelClassification,
    OmniModelForMultiLabelSequenceClassification,
    OmniPooling,
)

label_names = ['Low', 'Medium', 'High']
tissue_names = [
    'root', 'seedling', 'leaf', 'FMI', 'FOD',
    'Prophase-I-pollen', 'Tricellular-pollen', 'flag', 'grain'
]


class TriClassTEDataset(OmniDatasetForMultiLabelClassification):
    """Dataset for 3-class (Low/Medium/High) multi-label TE classification
    
    ç»§æ‰¿è¯´æ˜ï¼š
    - ç»§æ‰¿è‡ª OmniDatasetForMultiLabelClassificationï¼Œè·å¾—å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†çš„åŸºç¡€åŠŸèƒ½
    - é‡å†™ prepare_input() æ–¹æ³•ä»¥é€‚é… TE 3åˆ†ç±»ä»»åŠ¡çš„ç‰¹æ®Šéœ€æ±‚
    """

    def __init__(self, **kwargs):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç»§æ‰¿çˆ¶ç±»çš„å±æ€§å’Œè¡Œä¸º
        super().__init__(**kwargs)

    def prepare_input(self, instance, **kwargs):
        # Map labels to indices: Low=0, Medium=1, High=2
        label2idx = {'Low': 0, 'Medium': 1, 'High': 2, 'nan': -100}

        # Extract labels for all 9 tissues
        root_TE_label = label2idx[str(instance["root_TE_label"])]
        seedling_TE_label = label2idx[str(instance["seedling_TE_label"])]
        leaf_TE_label = label2idx[str(instance["leaf_TE_label"])]
        FMI_TE_label = label2idx[str(instance["FMI_TE_label"])]
        FOD_TE_label = label2idx[str(instance["FOD_TE_label"])]
        Prophase_I_pollen_TE_label = label2idx[str(instance["Prophase-I-pollen_TE_label"])]
        Tricellular_pollen_TE_label = label2idx[str(instance["Tricellular-pollen_TE_label"])]
        flag_TE_label = label2idx[str(instance["flag_TE_label"])]
        grain_TE_label = label2idx[str(instance["grain_TE_label"])]
        sequence = instance["sequence"]

        # Tokenize sequence
        tokenized_inputs = self.tokenizer(
            sequence,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Stack all labels
        labels = torch.tensor([
            root_TE_label,
            seedling_TE_label,
            leaf_TE_label,
            FMI_TE_label,
            FOD_TE_label,
            Prophase_I_pollen_TE_label,
            Tricellular_pollen_TE_label,
            flag_TE_label,
            grain_TE_label,
        ], dtype=torch.long)  # Use long for CrossEntropyLoss

        tokenized_inputs["labels"] = labels

        return tokenized_inputs


class OmniModelForTriClassTESequenceClassification(OmniModelForMultiLabelSequenceClassification):
    """Model for 3-class multi-label TE classification"""

    def __init__(self, config_or_model, tokenizer, num_labels=9, num_classes=3, *args, **kwargs):
        # For multi-label with 3 classes each, output should be [batch, num_labels, num_classes]
        super().__init__(config_or_model, tokenizer, num_labels=num_labels * num_classes, *args, **kwargs)
        self.metadata["model_name"] = self.__class__.__name__
        self.num_labels = num_labels  # 9 tissues
        self.num_classes = num_classes  # 3 classes (Low/Medium/High)
        self.pooler = OmniPooling(self.config)
        self.classifier = torch.nn.Linear(self.config.hidden_size, self.num_classes * self.num_labels)
        # Use CrossEntropyLoss for multi-class classification
        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction="mean")

        # ğŸ”‘ NEW: Store dataset class reference for saving
        self.dataset_class = kwargs.pop('dataset_class', TriClassTEDataset)

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        """Forward pass with proper reshaping for multi-label multi-class"""
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

        # Get the logits from classifier head
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        logits = self.classifier(self.pooler(input_ids, logits))
        # Reshape logits from [batch, num_labels * num_classes] to [batch, num_labels, num_classes]
        batch_size = logits.shape[0]
        logits = logits.view(batch_size, self.num_labels, self.num_classes)

        loss = None
        if labels is not None:
            # labels shape: [batch, num_labels]
            # Flatten for CrossEntropyLoss
            logits_flat = logits.view(-1, self.num_classes)  # [batch * num_labels, num_classes]
            labels_flat = labels.view(-1)  # [batch * num_labels]

            loss = self.loss_fn(logits_flat, labels_flat)

        return {
            "loss": loss,
            "logits": logits,
            "last_hidden_state": outputs.last_hidden_state if hasattr(outputs, 'last_hidden_state') else None,
        }

    def predict(self, sequence_or_inputs, **kwargs):
        """Prediction with softmax for multi-class"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        # Apply softmax to get probabilities for each label
        probabilities = torch.softmax(logits, dim=-1)  # [batch, num_labels, num_classes]

        # Get predicted class for each label
        predictions = torch.argmax(probabilities, dim=-1)  # [batch, num_labels]

        outputs = {
            "predictions": predictions,
            "logits": logits,
            "probabilities": probabilities,
            "last_hidden_state": last_hidden_state,
        }

        return outputs

    def inference(self, sequence_or_inputs, **kwargs):
        """Inference wrapper"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        # Apply softmax
        probabilities = torch.softmax(logits, dim=-1)

        # Get predictions
        predictions = torch.argmax(probabilities, dim=-1)

        # Get confidence (max probability for each label)
        confidence, _ = torch.max(probabilities, dim=-1)

        if not isinstance(sequence_or_inputs, list):
            outputs = {
                "predictions": predictions[0],
                "logits": logits[0],
                "probabilities": probabilities[0],
                "confidence": confidence[0],
                "last_hidden_state": last_hidden_state[0] if last_hidden_state is not None else None,
            }
        else:
            outputs = {
                "predictions": predictions,
                "logits": logits,
                "probabilities": probabilities,
                "confidence": confidence,
                "last_hidden_state": last_hidden_state,
            }

        return outputs



# ==================== è¾…åŠ©å‡½æ•° ====================

def get_prediction_prob(model, sequence, tissue_idx=0, target_class=2):
    """è·å–æ¨¡å‹å¯¹äºç»™å®šåºåˆ—åœ¨ç‰¹å®šç»„ç»‡ä¸­å±äº target_class çš„æ¦‚ç‡"""
    with torch.no_grad():
        outputs = model.inference(sequence)
        probabilities = outputs['probabilities']
        return probabilities[tissue_idx, target_class].cpu().item()


def get_all_predictions(model, sequence):
    """è·å–åºåˆ—åœ¨æ‰€æœ‰ç»„ç»‡ä¸Šçš„é¢„æµ‹ç»“æœ"""
    with torch.no_grad():
        outputs = model.inference(sequence)
        predictions = outputs['predictions'].cpu().numpy()
        probabilities = outputs['probabilities'].cpu().numpy()
        return predictions, probabilities


# ==================== å®šç‚¹çªå˜åˆ†æ ====================

def in_silico_mutagenesis(model, sequence, tissue_idx=0, target_class=2):
    """å¯¹åºåˆ—è¿›è¡Œå®šç‚¹çªå˜ï¼Œå¹¶è®¡ç®—æ¯ä¸ªçªå˜å¯¹æ¨¡å‹é¢„æµ‹æ¦‚ç‡çš„å½±å“"""
    print(f"\nğŸ”¬ æ­£åœ¨å¯¹åºåˆ—è¿›è¡Œè®¡ç®—æœºå†…å®šç‚¹çªå˜åˆ†æ...")
    print(f"ğŸ“Š åˆ†æç»„ç»‡: {tissue_names[tissue_idx]}")
    print(f"ğŸ¯ ç›®æ ‡ç±»åˆ«: {label_names[target_class]}")
    
    NUCLEOTIDES = ['A', 'T', 'C', 'G']
    seq_len = len(sequence)
    importance_scores = np.zeros((len(NUCLEOTIDES), seq_len))
    
    baseline_score = get_prediction_prob(model, sequence, tissue_idx, target_class)
    print(f"ğŸ“ˆ åŸºå‡†åºåˆ—é¢„æµ‹ä¸º'{label_names[target_class]}'çš„æ¦‚ç‡: {baseline_score:.4f}")
    
    for i in range(seq_len):
        original_nucleotide = sequence[i].upper()
        
        for j, mutated_nucleotide in enumerate(NUCLEOTIDES):
            if original_nucleotide == mutated_nucleotide:
                importance_scores[j, i] = 0
                continue
            
            mutated_sequence = list(sequence)
            mutated_sequence[i] = mutated_nucleotide
            mutated_sequence = "".join(mutated_sequence)
            
            mutated_score = get_prediction_prob(model, mutated_sequence, tissue_idx, target_class)
            score_change = baseline_score - mutated_score
            importance_scores[j, i] = score_change
        
        if (i + 1) % 50 == 0 or i == seq_len - 1:
            print(f"â³ è¿›åº¦: {i + 1}/{seq_len} ä¸ªä½ç½® ({(i+1)/seq_len*100:.1f}%)")
    
    print("âœ… å®šç‚¹çªå˜åˆ†æå®Œæˆï¼")
    return importance_scores, baseline_score


# ==================== å¯è§†åŒ– ====================

def visualize_importance(importance_scores, baseline_score, sequence, 
                        tissue_name, target_class_name, save_path=None):
    """å¯è§†åŒ–é‡è¦æ€§å¾—åˆ†çƒ­åŠ›å›¾"""
    print("\nğŸ¨ æ­£åœ¨ç”Ÿæˆé‡è¦æ€§çƒ­åŠ›å›¾...")
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 8))
    
    # çƒ­åŠ›å›¾
    sns.heatmap(
        importance_scores,
        cmap="RdBu_r",
        center=0,
        yticklabels=['A', 'T', 'C', 'G'],
        cbar_kws={'label': 'Score Change (Importance)'},
        ax=ax1
    )
    ax1.set_title(f"In-Silico Mutagenesis Saliency Map\nç»„ç»‡: {tissue_name} | ç›®æ ‡ç±»åˆ«: {target_class_name} | åŸºå‡†æ¦‚ç‡: {baseline_score:.4f}")
    ax1.set_xlabel("åºåˆ—ä½ç½®")
    ax1.set_ylabel("çªå˜ä¸º")
    
    # æŸ±çŠ¶å›¾
    max_importance = np.max(importance_scores, axis=0)
    positions = np.arange(len(sequence))
    
    ax2.bar(positions, max_importance, color='steelblue', alpha=0.7)
    ax2.set_xlabel("åºåˆ—ä½ç½®")
    ax2.set_ylabel("æœ€å¤§é‡è¦æ€§å¾—åˆ†")
    ax2.set_title("æ¯ä¸ªä½ç½®çš„æœ€å¤§é‡è¦æ€§å¾—åˆ†")
    ax2.grid(True, alpha=0.3)
    
    # æ ‡æ³¨æœ€é‡è¦çš„ä½ç½®
    top_positions = np.argsort(max_importance)[-min(10, len(sequence)):][::-1]
    for pos in top_positions:
        ax2.text(pos, max_importance[pos], f'{sequence[pos]}', 
                ha='center', va='bottom', fontsize=8, color='red')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ å›¾åƒå·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()


# ==================== ç»¼åˆåˆ†æ ====================

def analyze_sequence_comprehensive(model, sequence, tissue_idx=0):
    """å¯¹ä¸€ä¸ªåºåˆ—åœ¨ç‰¹å®šç»„ç»‡ä¸Šè¿›è¡Œå…¨é¢åˆ†æ"""
    tissue_name = tissue_names[tissue_idx]
    print(f"\n{'='*70}")
    print(f"ğŸ” å¼€å§‹ç»¼åˆåˆ†æ")
    print(f"ğŸ§¬ åºåˆ—é•¿åº¦: {len(sequence)} bp")
    print(f"ğŸ¥ åˆ†æç»„ç»‡: {tissue_name}")
    print(f"{'='*70}")
    
    # 1. è·å–é¢„æµ‹ç»“æœ
    predictions, probabilities = get_all_predictions(model, sequence)
    
    print(f"\nğŸ“Š æ‰€æœ‰ç»„ç»‡çš„é¢„æµ‹ç»“æœ:")
    for i, (pred, probs) in enumerate(zip(predictions, probabilities)):
        print(f"  {tissue_names[i]:25s}: {label_names[pred]:6s} "
              f"(Low: {probs[0]:.3f}, Medium: {probs[1]:.3f}, High: {probs[2]:.3f})")
    
    # 2. çªå˜åˆ†æ
    target_class = predictions[tissue_idx]
    print(f"\nğŸ¯ å°†å¯¹ {tissue_name} ç»„ç»‡çš„é¢„æµ‹ç±»åˆ« '{label_names[target_class]}' è¿›è¡Œçªå˜åˆ†æ...")
    
    importance_scores, baseline_score = in_silico_mutagenesis(
        model, sequence, tissue_idx, target_class
    )
    
    # 3. å¯è§†åŒ–
    save_path = f"/home/sw1136/OmniGenBench/examples/dingling_te/mutagenesis_{tissue_name}_{label_names[target_class]}.png"
    visualize_importance(
        importance_scores, baseline_score, sequence,
        tissue_name, label_names[target_class], save_path
    )
    
    # 4. è¾“å‡ºå…³é”®ä½ç½®
    max_importance = np.max(importance_scores, axis=0)
    top_positions = np.argsort(max_importance)[-min(10, len(sequence)):][::-1]
    
    print(f"\nğŸ” æœ€é‡è¦çš„{len(top_positions)}ä¸ªä½ç½®:")
    for rank, pos in enumerate(top_positions, 1):
        nt = sequence[pos]
        score = max_importance[pos]
        best_mut_idx = np.argmax(importance_scores[:, pos])
        best_mut = ['A', 'T', 'C', 'G'][best_mut_idx]
        print(f"  {rank:2d}. ä½ç½® {pos:4d}: {nt} â†’ {best_mut} (é‡è¦æ€§: {score:.4f})")
    
    return importance_scores, baseline_score


# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    # ç¤ºä¾‹åºåˆ—ï¼ˆä½¿ç”¨è¾ƒçŸ­åºåˆ—è¿›è¡Œæ¼”ç¤ºï¼‰
    # å®Œæ•´åˆ†æ500bpåºåˆ—å¤§çº¦éœ€è¦10-15åˆ†é’Ÿ


    # ==================== åŠ è½½æ¨¡å‹ ====================

    print("ğŸ”„ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„ä¸‰åˆ†ç±» TE æ¨¡å‹...")

    model_path = "/home/sw1136/OmniGenBench/examples/dingling_te/ogb_te_3class_finetuned_epoch_19_seed_42_accuracy_score_0.9900_seed_42_f1_score_0.9900"

    inference_model = ModelHub.load(model_path)


    example_sequence = "GAGGGAGGGAAACGGGGGAGGGGAATGGGATGCTCCATTAGCTAAGCTCTGGTCTGATTACACGCCATTTCAGGAGCCATCGGTGGATCCGCCTCCCCCTCGCCCCTCGCCTACACCCCC"
    
    # é€‰æ‹©è¦åˆ†æçš„ç»„ç»‡
    tissues_to_analyze = [0]  
    
    for tissue_idx in tissues_to_analyze:
        analyze_sequence_comprehensive(inference_model, example_sequence, tissue_idx)
        print("\n" + "="*70 + "\n")
    
    print("ğŸ‰ æ‰€æœ‰å¯è§£é‡Šæ€§åˆ†æå®Œæˆï¼")
