{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q omnigenbench torch datasets scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we will introduce the custom finetuning of OmniGenome models.\n",
    "model_name = \"yangheng/OmniGenome-52M\" # 52M parameters\n",
    "\n",
    "# 1. Load the model and tokenizer according to model_name for later use\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)  # trust_remote_code=True is used to load the model from the remote repository, which is necessary for OmniGenome models\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    " # OmniGenome-52M tokenizer can be initialized by AutoTokenizer, for other models, you can use can define a wrapper to initialize the tokenizer to be compatible with transformers tokenizer APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Wrap a tokenizer to accommodate transformers APIs\n",
    "### This is an optional step, you can skip it if you are using the default tokenizer provided by OmniGenome. The core idea is to create a tokenizer that use __call__ method to process the input sequence and return the tokenized inputs in a format compatible with transformers tokenizer APIs.\n",
    "\n",
    "```\n",
    "import itertools\n",
    "import warnings\n",
    "import torch\n",
    "from ViennaRNA import ViennaRNA\n",
    "from transformers import AutoTokenizer\n",
    "from omnigenbench import OmniSingleNucleotideTokenizer\n",
    "from omnigenbench import RNA2StructureCache\n",
    "\n",
    "\n",
    "class Tokenizer(OmniSingleNucleotideTokenizer):\n",
    "    def __init__(\n",
    "        self, base_tokenizer=None, **kwargs):\n",
    "        super(Tokenizer, self).__init__(base_tokenizer, **kwargs)\n",
    "        self.metadata[\"tokenizer_name\"] = self.__class__.__name__\n",
    "        self.rna2str = RNA2StructureCache()\n",
    "\n",
    "    bases = [4, 5, 6, 7]\n",
    "    triplet_combinations = list(itertools.product(bases, repeat=3))\n",
    "    kmer_to_index = {tuple(triplet): i for i, triplet in enumerate(triplet_combinations)}\n",
    "\n",
    "    def process_input_ids(self, input_ids, k=3):\n",
    "        kmer_input_ids = [64]\n",
    "        for i in range(len(input_ids) - k + 1):\n",
    "            kmer = tuple(input_ids[i:i + k].tolist())\n",
    "            kmer_input_ids.append(self.kmer_to_index.get(kmer, 64))\n",
    "        kmer_input_ids.append(64)\n",
    "        return torch.tensor(kmer_input_ids)\n",
    "\n",
    "    def __call__(self, sequence, **kwargs):\n",
    "        sequence = sequence.replace(\"U\", \"T\")\n",
    "        structure, mfe = self.rna2str.fold(sequence, return_mfe=True)\n",
    "        structure_inputs = self.base_tokenizer(structure, **kwargs)\n",
    "        tokenized_inputs = self.base_tokenizer(sequence, **kwargs)\n",
    "        kmer_ids = self.process_input_ids(tokenized_inputs['input_ids'][0], k=3)\n",
    "        tokenized_inputs[\"kmer_ids\"] = kmer_ids.unsqueeze(0)\n",
    "        tokenized_inputs[\"str_ids\"] = structure_inputs[\"input_ids\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(model_name_or_path, **kwargs):\n",
    "        self = OmniSingleNucleotideTokenizer(\n",
    "            AutoTokenizer.from_pretrained(model_name_or_path, **kwargs)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def tokenize(self, sequence, **kwargs):\n",
    "        if isinstance(sequence, str):\n",
    "            sequences = [sequence]\n",
    "        else:\n",
    "            sequences = sequence\n",
    "\n",
    "        sequence_tokens = []\n",
    "        for i in range(len(sequences)):\n",
    "            tokens = []\n",
    "            for j in range(0, len(sequences[i]), self.k - self.overlap):\n",
    "                tokens.append(sequences[i][j : j + self.k])\n",
    "\n",
    "            sequence_tokens.append(tokens)\n",
    "\n",
    "        return sequence_tokens\n",
    "\n",
    "    def encode(self, input_ids, **kwargs):\n",
    "        return self.base_tokenizer.encode(input_ids, **kwargs)\n",
    "\n",
    "    def decode(self, input_ids, **kwargs):\n",
    "        return self.base_tokenizer.decode(input_ids, **kwargs)\n",
    "\n",
    "    def encode_plus(self, sequence, **kwargs):\n",
    "        raise NotImplementedError(\"The encode_plus() function is not implemented yet.\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Model with a Classification Head for Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a sequence classification task,\n",
    "from omnigenbench import OmniModelForSequenceClassification\n",
    "model = OmniModelForSequenceClassification(\n",
    "    config_or_model=model_name,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    num_labels=3,  # Binary classification\n",
    ")\n",
    "# For a token classification task, you can use OmniModelForTokenClassification\n",
    "from omnigenbench import OmniModelForTokenClassification\n",
    "model = OmniModelForTokenClassification(\n",
    "    config_or_model=model_name,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    num_labels=3,  # Binary classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Define a Custom Model for Downstream Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import OmniModel, OmniPooling\n",
    "import torch\n",
    "\n",
    "class OmniModelForSequenceClassification(OmniModel):\n",
    "    def __init__(self, config_or_model, tokenizer, *args, **kwargs):\n",
    "        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n",
    "        self.metadata[\"model_name\"] = self.__class__.__name__\n",
    "        self.pooler = OmniPooling(self.config)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.classifier = torch.nn.Linear(\n",
    "            self.config.hidden_size, self.config.num_labels\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # self.model_info()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        last_hidden_state = self.last_hidden_state_forward(**inputs)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.activation(last_hidden_state)\n",
    "        last_hidden_state = self.pooler(inputs, last_hidden_state)\n",
    "        logits = self.classifier(last_hidden_state)\n",
    "        logits = self.softmax(logits)\n",
    "        outputs = {\n",
    "            \"logits\": logits,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        logits = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(logits.shape[0]):\n",
    "            predictions.append(logits[i].argmax(dim=-1))\n",
    "\n",
    "        outputs = {\n",
    "            \"predictions\": (\n",
    "                torch.vstack(predictions).to(self.model.device)\n",
    "                if predictions[0].shape\n",
    "                else torch.tensor(predictions).to(self.model.device)\n",
    "            ),\n",
    "            \"logits\": logits,\n",
    "            \"last_hidden_state\": last_hidden_state,\n",
    "        }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, sequence_or_inputs, **kwargs):\n",
    "        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)\n",
    "\n",
    "        logits = raw_outputs[\"logits\"]\n",
    "        last_hidden_state = raw_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(logits.shape[0]):\n",
    "            predictions.append(\n",
    "                self.config.id2label.get(logits[i].argmax(dim=-1).item(), \"\")\n",
    "            )\n",
    "\n",
    "        if not isinstance(sequence_or_inputs, list):\n",
    "            outputs = {\n",
    "                \"predictions\": predictions[0],\n",
    "                \"logits\": logits[0],\n",
    "                \"confidence\": torch.max(logits[0]),\n",
    "                \"last_hidden_state\": last_hidden_state[0],\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"predictions\": predictions,\n",
    "                \"logits\": logits,\n",
    "                \"confidence\": torch.max(logits, dim=-1)[0],\n",
    "                \"last_hidden_state\": last_hidden_state,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, logits, labels):\n",
    "        loss = self.loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Dataset for Downstream Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a sequence classification task\n",
    "from omnigenbench import OmniDatasetForSequenceClassification\n",
    "# For a token classification task\n",
    "from omnigenbench import OmniDatasetForTokenClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option) Define a Custom Dataset for Downstream Task\n",
    "\n",
    "### To define a custom dataset for a sequence classification task, you can inherit from `OmniDataset` and implement the `prepare_input` method to process the input data.\n",
    "Make sure your dataset is in a format compatible with the tokenizer API, returning tokenized inputs and labels.\n",
    "```\n",
    "class Dataset(OmniDataset):\n",
    "    def __init__(self, data_source, tokenizer, max_length, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            instance[\"sequence\"],\n",
    "            padding=kwargs.get(\"padding\", \"do_not_pad\"),\n",
    "            truncation=kwargs.get(\"truncation\", True),\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(instance[\"label\"], dtype=torch.long)\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "        return tokenized_inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset according to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"toy_datasets/Archive2/\"  # Path to your dataset files\n",
    "# Archive2 is RNA secondary structure prediction dataset (token classification), containing train.json, test.json, and valid.json files\n",
    "train_file = dataset_path + \"train.json\"\n",
    "test_file = dataset_path + \"test.json\"\n",
    "valid_file = dataset_path + \"valid.json\"\n",
    "train_set = OmniDatasetForTokenClassification(\n",
    "    data_source=train_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=512,  # Set the maximum sequence length\n",
    ")\n",
    "test_set = OmniDatasetForTokenClassification(\n",
    "    data_source=test_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=512,  # Set the maximum sequence length\n",
    ")\n",
    "valid_set = OmniDatasetForTokenClassification(\n",
    "    data_source=valid_file,\n",
    "    tokenizer=base_tokenizer,  # Use the base tokenizer or the custom tokenizer defined above\n",
    "    max_length=512,  # Set the maximum sequence length\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    dataset,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=dataset['label'] if 'label' in dataset.column_names else None\n",
    ")\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnigenbench import ClassificationMetric  # contains all metrics from sklearn.metrics and some custom metrics for classification tasks\n",
    "from omnigenbench import Trainer\n",
    "\n",
    "# necessary hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "batch_size = 8\n",
    "max_length = 512\n",
    "seeds = [45]  # Each seed will be used for one run\n",
    "compute_metrics = [\n",
    "    ClassificationMetric(ignore_y=-100).accuracy_score,\n",
    "    ClassificationMetric(ignore_y=-100, average=\"macro\").f1_score,\n",
    "    ClassificationMetric(ignore_y=-100).matthews_corrcoef,\n",
    "]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        seeds=seed,\n",
    "    )\n",
    "\n",
    "    metrics = trainer.train()\n",
    "    test_metrics = metrics[\"test\"][-1]\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After training, we evaluate the model on the validation set with a classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming model has a predict method returning logits\n",
    "preds = model.predict(val_dataset)\n",
    "y_true = np.array(val_dataset['label'])\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"OmniGenome-52M-SSP\"\n",
    "model.save(path_to_save, overwrite=True)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model = model.load(path_to_save)\n",
    "results = model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "print(results[\"predictions\"])\n",
    "print(\"logits:\", results[\"logits\"])\n",
    "\n",
    "# We can load the model checkpoint using the ModelHub\n",
    "from omnigenbench import ModelHub\n",
    "\n",
    "ssp_model = ModelHub.load(\"OmniGenome-186M-SSP\")\n",
    "results = ssp_model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "print(results[\"predictions\"])\n",
    "print(\"logits:\", results[\"logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
