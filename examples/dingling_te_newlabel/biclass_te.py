# -*- coding: utf-8 -*-
# file: triclass_te.py
# time: 09:35 07/10/2025
# author: YANG, HENG <hy345@exeter.ac.uk> (æ¨æ’)
# homepage: https://yangheng95.github.io
# github: https://github.com/yangheng95
# huggingface: https://huggingface.co/yangheng
# google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
# Copyright (C) 2019-2025. All Rights Reserved.

import torch
import math

from omnigenbench import (
    ClassificationMetric,
    AccelerateTrainer,
    ModelHub,
    OmniTokenizer,
    OmniDatasetForMultiLabelClassification,
    OmniModelForMultiLabelSequenceClassification,
    OmniPooling,
)

model_name_or_path = "yangheng/OmniGenome-52M"
# model_name_or_path = "yangheng/OmniGenome-v1.5"
# model_name_or_path = "SpliceBERT-510nt"
# model_name_or_path = "InstaDeepAI/nucleotide-transformer-v2-100m-multi-species"

# Load tokenizer
tokenizer = OmniTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)


class BiClassTEDataset(OmniDatasetForMultiLabelClassification):
    """Dataset for 3-class (Low/Medium/High) multi-label TE classification
    
    ç»§æ‰¿è¯´æ˜ï¼š
    - ç»§æ‰¿è‡ª OmniDatasetForMultiLabelClassificationï¼Œè·å¾—å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†çš„åŸºç¡€åŠŸèƒ½
    - é‡å†™ prepare_input() æ–¹æ³•ä»¥é€‚é… TE 3åˆ†ç±»ä»»åŠ¡çš„ç‰¹æ®Šéœ€æ±‚
    """

    def __init__(self, **kwargs):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç»§æ‰¿çˆ¶ç±»çš„å±æ€§å’Œè¡Œä¸º
        super().__init__(**kwargs)

    def prepare_input(self, instance, **kwargs):
        # Map labels to indices: Low=0, High=1, NA=-100 (ignored in loss)
        # Handle multiple formats: int, float, string representations
        def map_label(label_value):
            # Convert to string first to handle uniformly
            label_str = str(label_value).lower().strip()
            
            # Handle NaN/None/empty cases
            if label_str in ['nan', 'none', '', 'null']:
                return -100
                
            # Handle numeric values (both int and float)
            try:
                numeric_val = float(label_str)
                if numeric_val == 0.0:
                    return 0
                elif numeric_val == 1.0:
                    return 1
                else:
                    return -100  # Unknown numeric value
            except ValueError:
                # Handle string labels if any
                if label_str in ['low', '0']:
                    return 0
                elif label_str in ['high', '1']:
                    return 1
                else:
                    return -100  # Unknown string value

        # Extract labels for all 9 tissues
        root_TE_label = map_label(instance["root_TE_label"])
        seedling_TE_label = map_label(instance["seedling_TE_label"])
        leaf_TE_label = map_label(instance["leaf_TE_label"])
        FMI_TE_label = map_label(instance["FMI_TE_label"])
        FOD_TE_label = map_label(instance["FOD_TE_label"])
        Prophase_I_pollen_TE_label = map_label(instance["Prophase-I-pollen_TE_label"])
        Tricellular_pollen_TE_label = map_label(instance["Tricellular-pollen_TE_label"])
        flag_TE_label = map_label(instance["flag_TE_label"])
        grain_TE_label = map_label(instance["grain_TE_label"])
        sequence = instance["sequence"]

        # Tokenize sequence
        tokenized_inputs = self.tokenizer(
            sequence,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Stack all labels
        labels = torch.tensor([
            root_TE_label,
            seedling_TE_label,
            leaf_TE_label,
            FMI_TE_label,
            FOD_TE_label,
            Prophase_I_pollen_TE_label,
            Tricellular_pollen_TE_label,
            flag_TE_label,
            grain_TE_label,
        ], dtype=torch.long)  # Use long for CrossEntropyLoss

        tokenized_inputs["labels"] = labels

        return tokenized_inputs


class OmniModelForTriClassTESequenceClassification(OmniModelForMultiLabelSequenceClassification):
    """Model for 3-class multi-label TE classification"""

    def __init__(self, config_or_model, tokenizer, num_labels=9, num_classes=2, *args, **kwargs):
        # For multi-label with 2 classes each, output should be [batch, num_labels, num_classes]
        super().__init__(config_or_model, tokenizer, num_labels=num_labels * num_classes, *args, **kwargs)
        self.metadata["model_name"] = self.__class__.__name__
        self.num_labels = num_labels  # 9 tissues
        self.num_classes = num_classes  # 2 classes (1/0)
        self.pooler = OmniPooling(self.config)
        self.classifier = torch.nn.Linear(self.config.hidden_size, self.num_classes * self.num_labels)
        # Use CrossEntropyLoss for multi-class classification
        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction="mean")

        # ğŸ”‘ NEW: Store dataset class reference for saving
        self.dataset_class = kwargs.pop('dataset_class', BiClassTEDataset)

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        """Forward pass with proper reshaping for multi-label multi-class"""
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )

        # Get the logits from classifier head
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        logits = self.classifier(self.pooler(input_ids, logits))
        # Reshape logits from [batch, num_labels * num_classes] to [batch, num_labels, num_classes]
        batch_size = logits.shape[0]
        logits = logits.view(batch_size, self.num_labels, self.num_classes)

        loss = None
        if labels is not None:
            # labels shape: [batch, num_labels]
            # Flatten for CrossEntropyLoss
            logits_flat = logits.view(-1, self.num_classes)  # [batch * num_labels, num_classes]
            labels_flat = labels.view(-1)  # [batch * num_labels]

            loss = self.loss_fn(logits_flat, labels_flat)

        return {
            "loss": loss,
            "logits": logits,
            "last_hidden_state": outputs.last_hidden_state if hasattr(outputs, 'last_hidden_state') else None,
        }

    def predict(self, sequence_or_inputs, **kwargs):
        """Prediction with softmax for multi-class"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        # Apply softmax to get probabilities for each label
        probabilities = torch.softmax(logits, dim=-1)  # [batch, num_labels, num_classes]

        # Get predicted class for each label
        predictions = torch.argmax(probabilities, dim=-1)  # [batch, num_labels]

        outputs = {
            "predictions": predictions,
            "logits": logits,
            "probabilities": probabilities,
            "last_hidden_state": last_hidden_state,
        }

        return outputs

    def inference(self, sequence_or_inputs, **kwargs):
        """Inference wrapper"""
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        # Apply softmax
        probabilities = torch.softmax(logits, dim=-1)

        # Get predictions
        predictions = torch.argmax(probabilities, dim=-1)

        # Get confidence (max probability for each label)
        confidence, _ = torch.max(probabilities, dim=-1)

        if not isinstance(sequence_or_inputs, list):
            outputs = {
                "predictions": predictions[0],
                "logits": logits[0],
                "probabilities": probabilities[0],
                "confidence": confidence[0],
                "last_hidden_state": last_hidden_state[0] if last_hidden_state is not None else None,
            }
        else:
            outputs = {
                "predictions": predictions,
                "logits": logits,
                "probabilities": probabilities,
                "confidence": confidence,
                "last_hidden_state": last_hidden_state,
            }

        return outputs


# Load datasets
print("ğŸ“Š Loading datasets...")
datasets = BiClassTEDataset.from_hub(
    "examples/dingling_te_newlabel",  # æŒ‡å®šå…·ä½“çš„æ•°æ®ç›®å½•
    tokenizer=tokenizer,
    max_length=512,
    force_padding=False
)

print("ğŸ“ Data loading completed!")
print(f"ğŸ“Š Loaded datasets: {list(datasets.keys())}")
for split, dataset in datasets.items():
    print(f"  - {split}: {len(dataset)} samples")

# Initialize model
print("\nğŸš€ Initializing model...")
model = OmniModelForTriClassTESequenceClassification(
    model_name_or_path,
    tokenizer,
    num_labels=9,  # 9 tissues
    num_classes=2,  # 2 classes: 0, 1
    trust_remote_code=True
)

# Define metrics: accuracy and F1 score
# - accuracy_score: è®¡ç®—æ•´ä½“åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¿½ç•¥æ ‡ç­¾ä¸º-100çš„æ ·æœ¬ï¼ˆé€šå¸¸ç”¨äºpaddingæˆ–æ— æ•ˆæ ‡ç­¾ï¼‰
# - f1_score: è®¡ç®—F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰ï¼Œä½¿ç”¨macroå¹³å‡ï¼ˆå¯¹æ¯ä¸ªç±»åˆ«è®¡ç®—F1åå–å¹³å‡ï¼Œé€‚åˆç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼‰
# 
# è®¡ç®—æ—¶æœºï¼šè¿™äº›metricsä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨äºeval_datasetï¼ˆéªŒè¯é›†ï¼‰å’Œtest_datasetï¼ˆæµ‹è¯•é›†ï¼‰
# è®¡ç®—æ–¹å¼ï¼š
#   1. æ¨¡å‹å¯¹éªŒè¯é›†/æµ‹è¯•é›†è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœï¼ˆlogitsï¼‰
#   2. å°†logitsè½¬æ¢ä¸ºé¢„æµ‹ç±»åˆ«ï¼ˆargmaxï¼‰
#   3. å°†é¢„æµ‹ç±»åˆ«ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œå¿½ç•¥æ ‡ç­¾ä¸º-100çš„ä½ç½®
#   4. accuracy_score: æ­£ç¡®é¢„æµ‹æ•° / æœ‰æ•ˆæ ·æœ¬æ€»æ•°
#   5. f1_score (macro): å¯¹æ¯ä¸ªç±»åˆ«åˆ†åˆ«è®¡ç®—F1å€¼ï¼Œç„¶åå–å¹³å‡å€¼
metric_functions = [
    ClassificationMetric(ignore_y=-100).accuracy_score,  # å‡†ç¡®ç‡ï¼šæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°
    ClassificationMetric(ignore_y=-100, average='macro').f1_score,  # å®å¹³å‡F1ï¼š(TP) / (TP + 0.5*(FP+FN))ï¼Œå¯¹æ‰€æœ‰ç±»åˆ«å–å¹³å‡
]

# Initialize trainer
 # batch_size: æ¯æ¬¡ä»æ•°æ®é›†ä¸­åŠ è½½å¹¶å¤„ç†çš„æ ·æœ¬æ•°é‡
    # - è¿™é‡Œè®¾ç½®ä¸º16ï¼Œè¡¨ç¤ºæ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¼šå¤„ç†16ä¸ªåºåˆ—æ ·æœ¬
    # - è¾ƒå°çš„batch_sizeå¯ä»¥å‡å°‘GPUå†…å­˜å ç”¨ï¼Œä½†è®­ç»ƒå¯èƒ½ä¸å¤Ÿç¨³å®š
    # - è¾ƒå¤§çš„batch_sizeå¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œé€Ÿåº¦ï¼Œä½†éœ€è¦æ›´å¤šGPUå†…å­˜

# gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    # - è¿™é‡Œè®¾ç½®ä¸º4ï¼Œè¡¨ç¤ºæ¯4ä¸ªbatchæ‰è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°
    # - å®é™…æœ‰æ•ˆbatch_size = batch_size Ã— gradient_accumulation_steps = 16 Ã— 4 = 64
    # - ä½œç”¨ï¼šåœ¨GPUå†…å­˜æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç´¯ç§¯å¤šä¸ªå°batchçš„æ¢¯åº¦æ¥æ¨¡æ‹Ÿå¤§batchè®­ç»ƒ
    # - å·¥ä½œåŸç†ï¼š
    #   1. å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼ˆä½†ä¸æ›´æ–°å‚æ•°ï¼‰
    #   2. å°†æ¢¯åº¦ç´¯åŠ åˆ°ä¹‹å‰çš„æ¢¯åº¦ä¸Š
    #   3. é‡å¤æ­¥éª¤1-2å…±4æ¬¡
    #   4. ç¬¬4æ¬¡åï¼Œä½¿ç”¨ç´¯ç§¯çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œç„¶åæ¸…é›¶æ¢¯åº¦
    # - ä¼˜ç‚¹ï¼šå¯ä»¥ç”¨è¾ƒå°çš„GPUå†…å­˜è®­ç»ƒå‡ºä¸å¤§batchç›¸å½“çš„æ•ˆæœ
    
# è®­ç»ƒæ—¶ä¸ä¼šç”¨åˆ°test_datasetï¼Œå®ƒä»…åœ¨è®­ç»ƒå®Œæˆåç”¨äºæœ€ç»ˆè¯„ä¼°
# - train_dataset: ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œæ›´æ–°æ¨¡å‹å‚æ•°
# - eval_dataset: ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯ï¼Œç›‘æ§è¿‡æ‹Ÿåˆï¼Œé€‰æ‹©æœ€ä½³æ¨¡å‹
# - test_dataset: ä»…åœ¨è®­ç»ƒå®Œæˆåç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼Œä¸å‚ä¸è®­ç»ƒè¿‡ç¨‹

trainer = AccelerateTrainer(
    model=model,
    epochs=10,
    learning_rate=2e-5,
    batch_size=16,  # æ¯æ¬¡è®­ç»ƒçš„æ ·æœ¬æ•°é‡
    train_dataset=datasets["train"],
    eval_dataset=datasets["valid"],
    test_dataset=datasets["test"],  # ä»…ç”¨äºè®­ç»ƒåçš„æœ€ç»ˆæµ‹è¯•ï¼Œä¸å½±å“è®­ç»ƒè¿‡ç¨‹
    compute_metrics=metric_functions,
    gradient_accumulation_steps=4,
)
# trainer.save_model(path_to_save="ogb_te_3class_finetuned", dataset_class=BiClassTEDataset)
metrics = trainer.train(path_to_save="ogb_te_3class_finetuned", dataset_class=BiClassTEDataset)
print('ğŸ“Š Final Metrics:', metrics)

# === Model Inference ===
print("\nğŸ”® Starting inference on test samples...")

inference_model = ModelHub.load("/home/sw1136/OmniGenBench/examples/dingling_te/ogb_te_3class_finetuned_epoch_19_seed_42_accuracy_score_0.9900_seed_42_f1_score_0.9900")

# Get some test samples
# sample_sequences = datasets['test'].sample(1000).examples
#sample_sequences = datasets['valid'].sample(1000).examples
sample_sequences = datasets['train'].examples[:1]

label_names = ['0', '1']
tissue_names = [
    'root', 'seedling', 'leaf', 'FMI', 'FOD',
    'Prophase-I-pollen', 'Tricellular-pollen', 'flag', 'grain'
]

with torch.no_grad():
    for row in sample_sequences:
        sequence = row["sequence"]
        print(f"\n{'='*60}")
        print(f"ğŸ§¬ Sample ID: {row['ID']}")
        print(f"ğŸ“ Sequence length: {len(sequence)} bp")

        outputs = inference_model.inference(sequence, **row)
        predictions = outputs['predictions'].cpu().numpy() # tensor([0, 2, 1, 2, 0, 2, 2, 1, 2], device='cuda:0') 9ä¸ªtissueçš„é¢„æµ‹ç±»åˆ«
        probabilities = outputs['probabilities'].cpu().numpy() # 9*3çš„tensorï¼Œæ¯ä¸ªtissueçš„3ä¸ªç±»åˆ«çš„æ¦‚ç‡ ï¼ˆlogits --> softmaxï¼‰
        confidence = outputs['confidence'].cpu().numpy() # 9ä¸ªtissueçš„é¢„æµ‹ç½®ä¿¡åº¦ tensor([0.9990, 1.0000, 0.5112, 0.9834, 1.0000, 0.9985, 0.9990, 0.9995, 1.0000], probabilitiesä¸­çš„æœ€å¤§å€¼
        # last_hidden_state = outputs['last_hidden_state'].cpu().numpy() # 9*512çš„tensorï¼Œæ¯ä¸ªtissueçš„512ä¸ªtokençš„éšè—çŠ¶æ€


        print(f"\nğŸ“Š Predictions for 9 tissues:")
        for i, tissue in enumerate(tissue_names):
            pred_class = predictions[i]
            pred_label = label_names[pred_class]
            conf = confidence[i]
            probs = probabilities[i]

            # Get ground truth if available
            gt_col = f"{tissue}_TE_label"
            if gt_col in row:
                gt_label = row[gt_col]
                if isinstance(gt_label, float) and math.isnan(gt_label):
                    continue
                match_emoji = "âœ…" if pred_label == gt_label else "âŒ"
                print(f"  {match_emoji} {tissue:25s}: {pred_label:6s} (conf: {conf:.3f}) [GT: {gt_label}]")
            else:
                print(f"  ğŸ”¹ {tissue:25s}: {pred_label:6s} (conf: {conf:.3f})")

            # Show probability distribution
            print(f"      Probs - 0: {probs[0]:.3f}, 1: {probs[1]:.3f}")

print("\nğŸ‰ All tasks completed!")