{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Translation Efficiency Prediction with OmniGenBench\n",
    "\n",
    "This notebook demonstrates how to fine-tune a Genomic Foundation Model (GFM) to predict Rice translation efficiency (TE) from mRNA sequences using OmniGenBench. The example focusing on plant RNA translation efficiency.\n",
    "\n",
    "**Background (PlantRNA-FM):**\n",
    "PlantRNA-FM (\"An interpretable RNA foundation model for exploring functional RNA motifs in plants\") introduces an RNA foundation model tailored to plant genomics, highlighting interpretability and motif discovery capabilities. TE prediction is a representative downstream task where models learn sequence determinants associated with efficient translation. In this demo, we use a small rice 5'UTR/mRNA TE dataset to illustrate fine-tuning and evaluation within OmniGenBench.\n",
    "\n",
    "- **Task type**: Binary sequence classification (High-TE vs Low-TE)\n",
    "- **Input**: RNA sequences (string), up to a configurable `max_length`\n",
    "- **Label space**: {0: Low-TE, 1: High-TE}\n",
    "\n",
    "**Estimated runtime:** On a single NVIDIA RTX 4090, a short training run on this toy dataset typically takes ~10â€“30 minutes depending on epochs/model size.\n",
    "\n",
    "##  Notebook Structure\n",
    " **Setup & Installation**: Install optional dependencies.\n",
    "2. **Import Libraries**: Load Python packages and local utilities.\n",
    "3. **Configuration & Data**: Choose a model, set hyperparameters, and point to dataset files.\n",
    "4. **Main Analysis Pipeline**: Run training/evaluation with `utils.run_training`.\n",
    "5. **Results Overview**: Summarize validation/test metrics.\n",
    "6. **Visualization**: Plot validation metrics across epochs.\n",
    "7. **References**: Link to PlantRNA-FM."
   ],
   "id": "37f815534558f161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "This section handles the initial setup, including installing necessary packages. If dependencies are already available, you can skip the installation cell.\n",
    "\n",
    "### Install Dependencies\n",
    "Uncomment and run the following cell to install the required packages."
   ],
   "id": "9ceb2a8181e7820e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optional: install dependencies if not available\n",
    "# !pip install torch transformers pandas autocuda multimolecule biopython scipy scikit-learn tqdm dill findfile requests omnigenbench\n"
   ],
   "id": "2fdadda994b56c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Import Libraries\n",
    "Import all necessary libraries for data processing, model training, and analysis."
   ],
   "id": "368aae70b71a4b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "from autocuda import auto_cuda\n",
    "\n",
    "# Import utilities from the local utils.py file\n",
    "utils_spec = importlib.util.spec_from_file_location(\"utils\", \"utils.py\")\n",
    "utils = importlib.util.module_from_spec(utils_spec)\n",
    "utils_spec.loader.exec_module(utils)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported successfully!\")\n"
   ],
   "id": "d0014e45e882422d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Configuration & Data\n",
    "\n",
    "Set up the analysis parameters, file paths, and model selection here. You can easily change the `MODEL_NAME` to test different GFMs.\n",
    "\n",
    "### Model Selection\n",
    "Choose the Genomic Foundation Model to fine-tune."
   ],
   "id": "59584089ed4d0003"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Using utils for reusable logic\n",
    "from utils import run_finetuning\n",
    "print(\"Core classes and functions imported from utils.\")\n",
    "\n",
    "# --- Available Models for Testing ---\n",
    "AVAILABLE_MODELS = [\n",
    "    # 'yangheng/OmniGenome-52M',\n",
    "    # 'yangheng/OmniGenome-186M',\n",
    "    'yangheng/OmniGenome-v1.5',\n",
    "]\n",
    "MODEL_NAME = AVAILABLE_MODELS[0]  # Model to use for predictions\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n"
   ],
   "id": "547e026cafd913b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter and Dataset Configuration\n",
    "Define the training hyperparameters and paths to the dataset files."
   ],
   "id": "8a9859d376ae79da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import findfile\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 1024\n",
    "SEED = 42\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "LOCAL_PATH = \"te_rice_dataset\"  # Local directory for the dataset\n",
    "from utils import download_te_dataset\n",
    "download_te_dataset(LOCAL_PATH)  # Download the VEP dataset if not already available\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "TRAIN_FILE = findfile.find_cwd_file(\"train.json\")  # training split\n",
    "VALID_FILE = findfile.find_cwd_file(\"valid.json\")  # validation split (optional)\n",
    "TEST_FILE = findfile.find_cwd_file(\"test.json\")  # test split\n",
    "# --- Label Mapping ---\n",
    "# The task is binary classification: 1 for TE, 0 for non-TE.\n",
    "LABEL2ID = {\"0\": 0, \"1\": 1}\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n"
   ],
   "id": "e266997994ee0a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Main Analysis Pipeline\n",
    "\n",
    "This section executes the training/evaluation pipeline using the configuration defined above. The `run_training` function from `examples/translation_efficiency_prediction/utils.py` orchestrates tokenization, dataset creation, training, and evaluation."
   ],
   "id": "e332d80578aba4ed"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Import main pipeline from utils for a concise demo\n",
    "from utils import run_finetuning\n",
    "\n",
    "print(\"Main analysis pipeline imported from utils.\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run the analysis\n",
    "metrics = run_finetuning(\n",
    "\tmodel_name=MODEL_NAME,\n",
    "\ttrain_file=TRAIN_FILE,\n",
    "\tvalid_file=VALID_FILE,\n",
    "\ttest_file=TEST_FILE,\n",
    "\tlabel2id=LABEL2ID,\n",
    "\tepochs=EPOCHS,\n",
    "\tlearning_rate=LEARNING_RATE,\n",
    "\tweight_decay=WEIGHT_DECAY,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tmax_length=MAX_LENGTH,\n",
    "\tseed=SEED,\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Analysis completed!\")\n",
    "\n",
    "# Print final validation and test metrics (if available)\n",
    "if metrics.get('valid'):\n",
    "\tprint(\"\\nValidation Set Performance (last epoch):\")\n",
    "\tfor key, value in metrics['valid'][-1].items():\n",
    "\t\tprint(f\"{key}: {value:.4f}\")\n",
    "\n",
    "if metrics.get('test'):\n",
    "\tprint(\"\\nTest Set Performance (best checkpoint):\")\n",
    "\tfor key, value in metrics['test'][-1].items():\n",
    "\t\tprint(f\"{key}: {value:.4f}\")\n"
   ],
   "id": "ca6e91fa0cb64ec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Inference  Example\n",
    "This section demonstrates how to run inference on a single sequence using the fine-tuned model. The `encode_tokens` function ensures the same preprocessing as during training."
   ],
   "id": "824e800bc61706aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from omnigenbench import ModelHub\n",
    "\n",
    "model = ModelHub.load(\"finetuned_te_model\").to(auto_cuda()).to(torch.float16)\n",
    "\n",
    "# Example sequence for inference\n",
    "sample_sequence1 = \"AAACCAACAAAATGCAGTAGAAGTACTCTCGAGCTATAGTCGCGACGTGCTGCCCCGCAGGAGTACAGTAGTAGTACAACGTAAGCGGGAGCAACAGACTCCCCCCCTGCAACCCACTGTGCCTGTGCCCTCGACGCGTCTCCGTCGCTTTGGCAAATGTCACGTACATATTACCGTCTCAGGCTCTCAGCCATGCTCCCTACCACCCCTGCAGCGAAGCAAAAGCCACGCACGCGGCGCCTGACATGTAACAGGACTAGACCATCTTGTTCATTTCCCGCACCCCCTCCTCTCCTCTTCCTCCATCTGCCTCTTTAAAACAGTAAAAATAACCGTGCATCCCCTGGGCAAAATCTCTCCCATACATACACTACAGCGGCGAACCTTTCCTTATTCTCGCAACGCCTCGGTAACGGGCAGCGCCTGCTCCGCGCCGCGGTTGCGAGTTCGGGAAGGCGGCCGGAGTCGCGGGGAGGAGAGGGAGGATTCGATCGGCCAGA\"  # High-TE sequence\n",
    "\n",
    "sample_sequence2 = \"TGGAGATGGGCAGATGGCACACAAAACATGAATAGAAAACCCAAAAGGAAGGATGAAAAAAACACACACACACACACACACAAAACACAGAGAGAGAGAGAGAGAGAGAGCGAGAAAAGAAAAGAAAAAACCAATTCTTTTGGTCTCTTCCCTCTCCGTTTGTCGTGTCGAAGCCTTTGCCCCCACCACCTCCTCCTCTCCTCTCCCTTCCTCCCCTCCTCCCCATCTCGCTCTCCTCCCTCCTCTCTCCTCTCCTCGTCTCCTCTTCCTCTCCATTCCATTGGCCATTCCATTCCATTCCACCCCCCATGAAACCCCAAACCCTCGTCGGCCTCGCCGCGCTCGCGTAGCGCACCCGCCCTTCTCCTCTCGCCGGTGGTCCGCCGCCAGCCTCCCCCCACCCGATCCCGCCGCCCCCCCCGCCTTCACCCCGCCCACGCGGACGCATCCGATCCCGCCGCATCGCCGCGCGGGGGGGGGGGGGGGGGGGGGAGGGCACG \"  # Low-TE sequence\n",
    "\n",
    "# Run inference on the sample sequences\n",
    "outputs = model.inference(sample_sequence1)\n",
    "print(f\"Sample sequence 1 prediction: {outputs}\")\n",
    "outputs = model.inference(sample_sequence2)\n",
    "print(f\"Sample sequence 2 prediction: {outputs}\")\n",
    "\n"
   ],
   "id": "f4eaf486034752c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Visualization\n",
    "\n",
    "In this section, we visualize validation metrics across epochs to assess learning dynamics.\n",
    "\n",
    "### Plot Validation Curves\n",
    "We plot macro F1 across epochs. Additional metrics (e.g., MCC) can be added if enabled in `utils.py` or dataset config.\n"
   ],
   "id": "10df7a1e8c5c29a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Results Overview â€” Quick summary of metrics\n",
    "if metrics.get('valid'):\n",
    "\tprint('Validation (last epoch):')\n",
    "\tfor k, v in metrics['valid'][-1].items():\n",
    "\t\tprint(f\"{k}: {v:.4f}\")\n",
    "\n",
    "if metrics.get('test'):\n",
    "\tprint('\\nTest (best checkpoint):')\n",
    "\tfor k, v in metrics['test'][-1].items():\n",
    "\t\tprint(f\"{k}: {v:.4f}\")\n"
   ],
   "id": "d7da5f9f72240be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization â€” Plot validation curve\n",
    "valid_key = 'valid' if 'valid' in metrics else ('eval' if 'eval' in metrics else None)\n",
    "if valid_key is None:\n",
    "\tprint(\"No validation metrics found for plotting.\")\n",
    "else:\n",
    "\tvalid_df = pd.DataFrame(metrics[valid_key])\n",
    "\tplt.style.use('seaborn-v0_8-whitegrid')\n",
    "\tfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\tif 'f1_score' in valid_df.columns:\n",
    "\t\tsns.lineplot(data=valid_df, x=valid_df.index, y='f1_score', ax=ax, label='Validation F1 (Macro)')\n",
    "\t\tax.set_ylabel('F1 Score')\n",
    "\telif 'matthews_corrcoef' in valid_df.columns:\n",
    "\t\tsns.lineplot(data=valid_df, x=valid_df.index, y='matthews_corrcoef', ax=ax, label='Validation MCC')\n",
    "\t\tax.set_ylabel('MCC')\n",
    "\telse:\n",
    "\t\tfirst_col = [c for c in valid_df.columns if isinstance(valid_df[c].iloc[-1], (int, float))]\n",
    "\t\tif first_col:\n",
    "\t\t\tsns.lineplot(data=valid_df, x=valid_df.index, y=first_col[0], ax=ax, label=first_col[0])\n",
    "\t\t\tax.set_ylabel(first_col[0])\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Validation metrics exist but no numeric columns to plot.\")\n",
    "\tax.set_title('Validation Metric across Epochs')\n",
    "\tax.set_xlabel('Epoch')\n",
    "\tax.legend()\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ],
   "id": "8c1f58a326b90338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "- PlantRNA-FM: \"An interpretable RNA foundation model for exploring functional RNA motifs in plants\" (Nature Machine Intelligence, 2024).\n"
   ],
   "id": "f03109d6a613f0a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
