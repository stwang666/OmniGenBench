{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c44b76",
   "metadata": {},
   "source": [
    "# 🧬 Translation Efficiency Prediction with OmniGenBench\n",
    "\n",
    "Welcome to this comprehensive tutorial where we'll explore how to predict **Translation Efficiency (TE)** from mRNA sequences using **OmniGenBench**. This guide focuses on the specific biological problem of translation efficiency and demonstrates the practical application of genomic deep learning.\n",
    "\n",
    "> 📚 **Prerequisite**: If you are new to OmniGenBench, it is highly recommended to first study the **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)**, which covers general knowledge such as language model concepts, machine learning task classification, and genomic foundation model principles.\n",
    "\n",
    "### 1. The Biological Challenge: What is Translation Efficiency?\n",
    "\n",
    "**Translation** is one of the most fundamental processes in molecular biology - it's the mechanism by which cells read mRNA sequences and synthesize proteins. **Translation Efficiency (TE)** quantifies how effectively this process occurs for a given mRNA molecule, directly impacting protein production levels.\n",
    "\n",
    "Understanding and predicting TE has profound implications across multiple domains:\n",
    "- **Synthetic Biology**: Designing optimized gene circuits that produce precise amounts of target proteins\n",
    "- **Disease Research**: Understanding how sequence variations affect protein levels and contribute to pathogenesis\n",
    "- **Biotechnology**: Engineering microorganisms for enhanced production of therapeutic proteins or industrial enzymes\n",
    "- **Agriculture**: Developing crops with improved nutritional content or stress resistance\n",
    "\n",
    "However, experimentally measuring TE across thousands of mRNA sequences is time-consuming and costly. This is where computational methods, particularly deep learning with Genomic Foundation Models, can provide transformative solutions.\n",
    "\n",
    "### 2. The Data: Rice Translation Efficiency Dataset\n",
    "\n",
    "To train our predictive model, we utilize a carefully curated dataset from *Oryza sativa* (rice), a model organism in plant biology.\n",
    "\n",
    "- **What it contains**: mRNA sequences with experimentally determined translation efficiency measurements\n",
    "- **What it labels**: Each sequence is classified as either \"High TE\" (1) or \"Low TE\" (0) based on ribosome profiling data\n",
    "- **Our Goal**: Train a model that can accurately classify any rice mRNA sequence by its translation efficiency potential\n",
    "\n",
    "**Dataset Structure:**\n",
    "\n",
    "| sequence | label | \n",
    "|---------|-------|\n",
    "| AUGGCCAUUGUAAUUGGCCGACUUGA... | 1 (High TE) | \n",
    "| AUGGCUACUAGCUAGCUAGCUAGC...    | 0 (Low TE) | \n",
    "| ...                                | ...  | \n",
    "\n",
    "Find the dataset template in **[Dataset Template](./05_advanced_dataset_creation.ipynb)** and customize it as needed for your experiments.\n",
    "\n",
    "### 3. Quick Start: Translation Efficiency Prediction Workflow\n",
    "\n",
    "This tutorial demonstrates the practical application of the **[Fundamental Concepts](./00_fundamental_concepts.ipynb)** to a specific biological problem. We'll use the standard 4-step OmniGenBench workflow:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph \"4-Step Workflow for TE Prediction\"\n",
    "        A[\"📥 Step 1: Data Preparation<br/>Download and process the TE dataset\"] --> B[\"🔧 Step 2: Model Initialization<br/>Load the pre-trained OmniGenome model\"]\n",
    "        B --> C[\"🎓 Step 3: Model Training<br/>Fine-tune the model on the TE dataset\"]\n",
    "        C --> D[\"🔮 Step 4: Model Inference<br/>Use the trained model to predict TE on new sequences\"]\n",
    "    end\n",
    "\n",
    "    style A fill:#e1f5fe,stroke:#333,stroke-width:2px\n",
    "    style B fill:#f3e5f5,stroke:#333,stroke-width:2px\n",
    "    style C fill:#e8f5e8,stroke:#333,stroke-width:2px\n",
    "    style D fill:#fff3e0,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "**Translation Efficiency Prediction** is a **sequence classification** task where we predict binary labels (High TE vs Low TE) for mRNA sequences. We'll use `OmniModelForSequenceClassification` as demonstrated in the fundamental concepts tutorial.\n",
    "\n",
    "### 4. Tutorial Structure\n",
    "\n",
    "1. **[Data Preparation](./01_data_preparation.ipynb)**: Download and preprocess the translation efficiency dataset\n",
    "2. **[Model Initialization](./02_model_initialization.ipynb)**: Load the pre-trained OmniGenome model and set it up for binary classification\n",
    "3. **[Training Implementation](./03_model_training.ipynb)**: Fine-tune the model using our dataset and validate its performance\n",
    "4. **[Inference: Make Predictions](./04_model_inference.ipynb)**: Use the trained model to predict translation efficiency on new mRNA sequences\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59944a",
   "metadata": {},
   "source": [
    "## 🚀 Step 1: Data Preparation\n",
    "\n",
    "This first step is all about getting our data ready for in-silico analysis. It involves four key parts:\n",
    "1.  **Environment Setup**: Installing and importing the necessary libraries.\n",
    "2.  **Configuration**: Defining all our important parameters in one place.\n",
    "3.  **Data Acquisition**: Downloading and preparing the raw dataset.\n",
    "4.  **Data Loading**: Creating a pipeline to efficiently feed data to the model.\n",
    "\n",
    "### 1.1: Environment Setup\n",
    "\n",
    "First, let's install the required Python packages. `omnigenbench` is our core library, `transformers` provides the underlying model architecture, and the other packages are utilities for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5573019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install omnigenbench -U  # Install the latest version of omnigenbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc72ffc",
   "metadata": {},
   "source": [
    "Next, we import the libraries we just installed. This gives us the tools for data processing, deep learning, and interacting with the operating system.\n",
    "\n",
    "A key part of this setup is determining the best available hardware for training. Our script will automatically prioritize a **CUDA-enabled GPU** if one is available, as this can accelerate training by 10-100x compared to a CPU. This makes a huge difference when working with large models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07902125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from omnigenbench import (\n",
    "    ClassificationMetric,\n",
    "    AccelerateTrainer,\n",
    "    ModelHub,\n",
    "    OmniTokenizer,\n",
    "    OmniDatasetForSequenceClassification,\n",
    "    OmniModelForSequenceClassification,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7baaf1",
   "metadata": {},
   "source": [
    "### 1.2: Global Configuration\n",
    "\n",
    "To make our tutorial easy to modify and understand, we'll centralize all important parameters in this section. This is a best practice in software development that makes experiments more reproducible.\n",
    "\n",
    "#### Key Parameters\n",
    "-   **Dataset**: We define the local path and download URL for our dataset.\n",
    "-   **Model**: We select which pre-trained OmniGenome model to use. For this tutorial, we'll start with `OmniGenome-52M` because it's fast and efficient, making it perfect for learning and prototyping.\n",
    "\n",
    "This centralized approach allows you to easily experiment with different settings (e.g., a larger model, a different learning rate) without having to hunt through the code.\n",
    "\n",
    "#### Note\n",
    "Almost all the parameters here are standard in machine learning workflows and have a default value that works well if you don't set them. Don't worry if some of these terms are unfamiliar right now. We'll explain each one as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"yangheng/OmniGenome-52M\"\n",
    "dataset_name = \"translation_efficiency_prediction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bec55e",
   "metadata": {},
   "source": [
    "### 1.3: Data Acquisition\n",
    "\n",
    "With our environment configured, it's time to download the DeepSEA dataset. The function below automates this process by:\n",
    "1.  Checking if the data already exists.\n",
    "2.  Downloading the dataset from the specified URL if needed.\n",
    "3.  Extracting the files.\n",
    "4.  Cleaning up the temporary zip file.\n",
    "\n",
    "This ensures we have the `train.jsonl`, `valid.jsonl`, and `test.jsonl` files ready for the next stage. These files represent the standard splits for training, validating, and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Tokenizer\n",
    "\n",
    "# We define the label mapping in the training\n",
    "label2id = {\"0\": 0, \"1\": 1}  # 0: Low TE, 1: High TE\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = OmniTokenizer.from_pretrained(model_name_or_path)\n",
    "print(f\"✅ Tokenizer loaded: {model_name_or_path}\")\n",
    "\n",
    "# Load datasets using the enhanced OmniDataset framework\n",
    "print(\"🏗️ Loading datasets with automatic download...\")\n",
    "datasets = OmniDatasetForSequenceClassification.from_huggingface(\n",
    "    dataset_name=\"translation_efficiency_prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(f\"📊 Datasets loaded: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3fef9",
   "metadata": {},
   "source": [
    "### 1.4: Dataset Loading with OmniGenBench\n",
    "\n",
    "With OmniGenBench, data loading is significantly simplified! The framework automatically handles:\n",
    "\n",
    "#### Automatic Data Processing\n",
    "The `OmniDatasetForSequenceClassification` class automatically:\n",
    "1. **Downloads and processes** the dataset from our curated collection\n",
    "2. **Handles sequence preprocessing** including truncation, padding, and tokenization\n",
    "3. **Manages binary classification formatting** for translation efficiency prediction\n",
    "4. **Creates train/validation/test splits** ready for training\n",
    "\n",
    "This streamlined approach eliminates the need for custom dataset classes while maintaining full flexibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 Data loading completed! Using  OmniDataset framework.\")\n",
    "print(f\"📊 Loaded datasets: {list(datasets.keys())}\")\n",
    "for split, dataset in datasets.items():\n",
    "    print(f\"  - {split}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab694a",
   "metadata": {},
   "source": [
    "## 🚀 Step 2: Model Initialization\n",
    "\n",
    "With our data pipeline in place, it's time to set up the model. This is where the power of Genomic Foundation Models (GFMs) comes into play. Instead of building a model from scratch, we will load the pre-trained **OmniGenome** model and adapt it for our specific task.\n",
    "\n",
    "This process involves three key components:\n",
    "1.  **The Tokenizer**: This is the same tokenizer we used in the data preparation step. It's responsible for converting DNA sequences into a numerical format the model can process. It's crucial that we use the *exact same* tokenizer that the model was pre-trained with.\n",
    "2.  **The Base Model**: This is the core OmniGenome model, which has already learned the fundamental patterns of genomic sequences from its extensive pre-training. We load it directly from the Hugging Face Hub.\n",
    "3.  **The Classification Head**: The base model is great at understanding sequences, but it doesn't know anything about our specific TFB prediction task. To fix this, we add a \"classification head\" on top of the base model. This is a simple neural network layer that takes the sequence representation from the base model and maps it to our desired output, in this case, a prediction for each of the TF labels.\n",
    "\n",
    "\n",
    "The `OmniModelForMultiLabelSequenceClassification` class from our library handles this process for us, seamlessly combining the base model and the new classification head.\n",
    "In adddition, most of the models and datasets have been integrated into the OmniGenBench package, making it easy to load them with just a few lines of code. Please refer to the sub-tutorials of [data preparation](./02_data_preparation.ipynb) and [model initialization](./03_model_initialization.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10959394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Initialization ===\n",
    "# We support all genomic foundation models from Hugging Face Hub.\n",
    "\n",
    "model = OmniModelForSequenceClassification(\n",
    "    model_name_or_path,\n",
    "    tokenizer,\n",
    "    num_labels=len(list(label2id.keys())),  # Binary classification: Low TE vs High TE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e14e5",
   "metadata": {},
   "source": [
    "## 🚀 Step 3: Model Training\n",
    "\n",
    "This is the most exciting part! With our data and model ready, we can now begin the **fine-tuning** process. During training, the model will learn to associate specific patterns in the mRNA sequences with high or low translation efficiency.\n",
    "\n",
    "The `AccelerateTrainer` from `omnigenbench` wraps all this logic into a simple interface, allowing us to launch the training process with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215efa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_functions = [ClassificationMetric().f1_score]\n",
    "\n",
    "trainer = AccelerateTrainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    test_dataset=datasets[\"test\"],\n",
    "    compute_metrics=metric_functions,\n",
    ")\n",
    "print(\"🎓 Starting training...\")\n",
    "\n",
    "metrics = trainer.train()\n",
    "trainer.save_model(\"ogb_te_finetuned\")\n",
    "\n",
    "print('Metrics:', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1f81d",
   "metadata": {},
   "source": [
    "## 🔮 Step 4: Model Inference and Interpretation\n",
    "\n",
    "Now that we have a trained model, let's use it for its intended purpose: predicting translation efficiency on new mRNA sequences. This process is called **inference**.\n",
    "\n",
    "### The Inference Pipeline\n",
    "\n",
    "Our inference pipeline consists of a few key steps:\n",
    "1. **Load the Model**: We load the best-performing model that was saved during training.\n",
    "2. **Process the Input**: We take new mRNA sequences and apply the same preprocessing steps we used for our training data (truncating/padding and tokenizing).\n",
    "3. **Run Prediction**: We feed the processed sequence to the model and get its predictions. We use `torch.no_grad()` to disable gradient calculations, which makes inference faster and uses less memory.\n",
    "4. **Interpret the Results**: The model's raw output is a probability score. We'll interpret these to make them more understandable, identifying whether sequences have high or low translation efficiency and with what level of confidence.\n",
    "\n",
    "To demonstrate, we'll test our model on a few sample sequences and print out a user-friendly summary of the results. This shows how the model can be used in a real-world application to analyze sequences of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_model = ModelHub.load(\"yangheng/ogb_te_finetuned\")\n",
    "\n",
    "sample_sequences = {\n",
    "    \"Optimized sequence\": \"AAACCAACAAAATGCAGTAGAAGTACTCTCGAGCTATAGTCGCGACGTGCTGCCCCGCAGGAGTACAGTAGTAGTACAACGTAAGCGGGAGCAACAGACTCCCCCCCTGCAACCCACTGTGCCTGTGCCCTCGACGCGTCTCCGTCGCTTTGGCAAATGTCACGTACATATTACCGTCTCAGGCTCTCAGCCATGCTCCCTACCACCCCTGCAGCGAAGCAAAAGCCACGCACGCGGCGCCTGACATGTAACAGGACTAGACCATCTTGTTCATTTCCCGCACCCCCTCCTCTCCTCTTCCTCCATCTGCCTCTTTAAAACAGTAAAAATAACCGTGCATCCCCTGGGCAAAATCTCTCCCATACATACACTACAGCGGCGAACCTTTCCTTATTCTCGCAACGCCTCGGTAACGGGCAGCGCCTGCTCCGCGCCGCGGTTGCGAGTTCGGGAAGGCGGCCGGAGTCGCGGGGAGGAGAGGGAGGATTCGATCGGCCAGA\",\n",
    "    \"Suboptimal sequence\": \"TGGAGATGGGCAGATGGCACACAAAACATGAATAGAAAACCCAAAAGGAAGGATGAAAAAAACACACACACACACACACACAAAACACAGAGAGAGAGAGAGAGAGAGCGAGAAAAGAAAAGAAAAAACCAATTCTTTTGGTCTCTTCCCTCTCCGTTTGTCGTGTCGAAGCCTTTGCCCCCACCACCTCCTCCTCTCCTCTCCCTTCCTCCCCTCCTCCCCATCTCGCTCTCCTCCCTCCTCTCTCCTCTCCTCGTCTCCTCTTCCTCTCCATTCCATTGGCCATTCCATTCCATTCCACCCCCCATGAAACCCCAAACCCTCGTCGGCCTCGCCGCGCTCGCGTAGCGCACCCGCCCTTCTCCTCTCGCCGGTGGTCCGCCGCCAGCCTCCCCCCACCCGATCCCGCCGCCCCCCCCGCCTTCACCCCGCCCACGCGGACGCATCCGATCCCGCCGCATCGCCGCGCGGGGGGGGGGGGGGGGGGGGGGGGGAGGGCACG\",\n",
    "    \"Random sequence\": \"AUGC\" * (128 // 4),\n",
    "}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq_name, sequence in sample_sequences.items():\n",
    "        outputs = inference_model.inference(sequence)\n",
    "        print(\"✅ Prediction completed!\")\n",
    "\n",
    "        # —— Result Interpretation ——\n",
    "        prediction = outputs.get('predictions', [0])[0]\n",
    "        probability = outputs.get('probabilities', [0.5])[0] if 'probabilities' in outputs else 0.5\n",
    "\n",
    "        te_class = \"High TE\" if prediction == 1 else \"Low TE\"\n",
    "        confidence = probability if prediction == 1 else (1 - probability)\n",
    "\n",
    "        print(f\"📊 Analysis for {seq_name}:\")\n",
    "        print(f\"  🎯 Prediction: {te_class}\")\n",
    "        print(f\"  📈 Confidence: {confidence:.3f}\")\n",
    "        print(f\"  📏 Sequence length: {len(sequence)} nucleotides\")\n",
    "\n",
    "        if confidence > 0.8:\n",
    "            emoji = \"🟢 High confidence\"\n",
    "        elif confidence > 0.6:\n",
    "            emoji = \"🟡 Moderate confidence\"\n",
    "        else:\n",
    "            emoji = \"🔴 Low confidence\"\n",
    "        print(f\"  {emoji} Model confidence level for this prediction\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fd227",
   "metadata": {},
   "source": [
    "## 🎉 Tutorial Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully completed this comprehensive tutorial on translation efficiency prediction with OmniGenBench.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "You've walked through a complete, end-to-end application of genomic deep learning, demonstrating how the concepts from the **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)** apply to a real biological problem. Specifically, you have:\n",
    "\n",
    "1. **Applied Task Formulation**: Successfully framed translation efficiency prediction as a sequence classification problem\n",
    "2. **Mastered the 4-Step Workflow**:\n",
    "   - **Step 1: Data Preparation**: Acquired, processed, and loaded the rice translation efficiency dataset\n",
    "   - **Step 2: Model Initialization**: Set up OmniModelForSequenceClassification for binary classification\n",
    "   - **Step 3: Model Training**: Fine-tuned the model using best practices and appropriate evaluation metrics\n",
    "   - **Step 4: Model Inference**: Generated predictions on new mRNA sequences and interpreted results\n",
    "3. **Understood Practical Application**: Gained hands-on experience with a biologically relevant prediction task\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "Now that you've mastered translation efficiency prediction, you can:\n",
    "\n",
    "#### 🧬 **Explore Other Sequence Classification Tasks**\n",
    "- **Promoter Recognition**: Identify regulatory sequences\n",
    "- **Subcellular Localization**: Predict protein cellular destinations  \n",
    "- **Functional Annotation**: Classify protein or RNA functions\n",
    "\n",
    "#### 📊 **Try Different Task Types**\n",
    "- **Sequence Regression**: Gene expression level prediction\n",
    "- **Token Classification**: Binding site identification\n",
    "- **Multi-label Classification**: Multi-functional sequence prediction\n",
    "\n",
    "#### 🔬 **Advanced Techniques**\n",
    "- **Custom Dataset Creation**: Use the [Advanced Dataset Creation Tutorial](./05_advanced_dataset_creation.ipynb)\n",
    "- **Model Comparison**: Benchmark different foundation models\n",
    "- **Hyperparameter Optimization**: Fine-tune model performance\n",
    "- **Biological Validation**: Compare predictions with experimental data\n",
    "\n",
    "### 📚 Resources\n",
    "\n",
    "- **[Fundamental Concepts Tutorial](../00_fundamental_concepts.ipynb)**: Review core concepts anytime\n",
    "- **[OmniGenBench Documentation](https://omnigenbench.readthedocs.io/)**: Complete API reference\n",
    "- **[GitHub Repository](https://github.com/yangheng95/OmniGenBench)**: Source code and community discussions\n",
    "\n",
    "Thank you for following along. We hope this tutorial has provided you with the knowledge and confidence to apply deep learning to your own genomics research. Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c665",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
