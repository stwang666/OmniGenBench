{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Factor Binding Prediction with OmniGenBench\n",
    "\n",
    "This notebook provides a step-by-step guide to extend OmniGenBench to the TFB task based on the **OmniGenome-52M** model on the **DeepSEA dataset**. The goal is to perform multi-label classification to predict the binding sites of various transcription factors based on DNA sequences.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized into several sections, each focusing on a specific aspect of the Transcription Factor Binding (TFB) prediction pipeline. Below is an overview of the structure:\n",
    "\n",
    "1. **Setup & Installation**: Ensures all required libraries and dependencies are installed.\n",
    "2. **Import Libraries**: Loads the necessary Python libraries for genomic data processing, model inference, and analysis.\n",
    "3. **Configuration**: Defines key parameters such as file paths, model selection, and training hyperparameters.\n",
    "4. **Model Definition**: Implements a custom model class that integrates the OmniGenome backbone with a classification head tailored for the DeepSEA task.\n",
    "5. **Data Loading and Preprocessing**: Handles the loading and preprocessing of the DeepSEA dataset, converting DNA sequences into tokenized inputs.\n",
    "6. **Initialization**: Sets up the tokenizer, model, datasets, and data loaders for training and evaluation.\n",
    "7. **Training the Model**: Fine-tunes the model using the `AccelerateTrainer` for efficient training and evaluation.\n",
    "8. **Evaluation**: Assesses the model's performance on the test set using metrics such as ROC AUC.\n",
    "9. **Inference Example**: Demonstrates how to use the trained model to make predictions on new DNA sequences.\n",
    "\n",
    "Each section is designed to be modular, allowing for easy customization and extension. Follow the notebook sequentially to understand and execute the TFB prediction pipeline effectively."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's ensure all the required packages are installed. If you have already installed them, you can skip this cell. Otherwise, uncomment and run the cell to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T22:50:28.478067Z",
     "start_time": "2025-07-31T22:50:28.474438Z"
    }
   },
   "source": "# Uncomment the following line to install the necessary packages\n# !pip install torch numpy transformers omnigenbench autocuda",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for genomic data processing, model inference, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import random\nimport os\nimport autocuda\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, BatchEncoding\nfrom omnigenbench import (\n    OmniDataset,\n    OmniModel,\n    OmniPooling,\n    Trainer,\n    ClassificationMetric,\n    AccelerateTrainer\n)\n\nprint(\"Libraries imported successfully.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Configuration\n\nHere, we define all the hyperparameters and settings for our experiment. This centralized configuration makes it easy to modify parameters and track experiments."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T22:50:31.535578Z",
     "start_time": "2025-07-31T22:50:31.469507Z"
    }
   },
   "source": [
    "# --- Data File Paths ---\n",
    "# Ensure these .npy files are in the same directory as the notebook, or provide the full path.\n",
    "TRAIN_FILE = \"train_tfb.npy\"\n",
    "TEST_FILE = \"test_tfb.npy\"\n",
    "VALID_FILE = \"valid_tfb.npy\"\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# --- Available Models for Testing ---\n",
    "AVAILABLE_MODELS = [\n",
    "    'yangheng/OmniGenome-52M',\n",
    "    'yangheng/OmniGenome-186M',\n",
    "    'yangheng/OmniGenome-v1.5',\n",
    "    # You can add more models here as needed,\n",
    "    # 'DNABERT-2-117M',\n",
    "    # 'hyenadna-large-1m-seqlen-hf',\n",
    "    # 'InstaDeepAI/nucleotide-transformer-500m-human-ref',\n",
    "    # 'multimolecule/rnafm', # RNA-specific models\n",
    "    # 'multimolecule/rnabert',\n",
    "    # 'SpliceBERT-510nt', # Splice-specific model\n",
    "]\n",
    "\n",
    "MODEL_NAME_OR_PATH = AVAILABLE_MODELS[1]\n",
    "USE_CONV_LAYERS = False  # Set to True to add DeepSEA-style convolutional layers on top of OmniGenome\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "PATIENCE = 3  # For early stopping\n",
    "MAX_LENGTH = 200  # The length of the DNA sequence to be processed\n",
    "SEED = 45\n",
    "MAX_EXAMPLES = 100000  # Use a smaller number for quick testing (e.g., 1000), or None for all data\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "CACHE_DATASET = True  # Set to True to cache preprocessed data for faster re-runs\n",
    "\n",
    "# --- Device Setup ---\n",
    "DEVICE = autocuda.auto_cuda()\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Definition\n\nWe define the `OmniModelForMultiLabelClassification`, which wraps the OmniGenome transformer. This class adds a classification head on top of the pre-trained backbone, tailored for the DeepSEA multi-label prediction task. It also includes an option to add convolutional layers, allowing for a hybrid architecture that combines the strengths of both transformers and CNNs."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T22:50:31.614900Z",
     "start_time": "2025-07-31T22:50:31.607738Z"
    }
   },
   "source": "class OmniModelForMultiLabelClassification(OmniModel):\n    \"\"\"\n    A custom model for multi-label classification of genomic sequences using an OmniGenome backbone.\n\n    This model replaces the original DeepSEA CNN architecture with a pre-trained Transformer encoder.\n    It can optionally add convolutional layers after the transformer embeddings before the final classification head.\n    \n    Args:\n        config_or_model: The Hugging Face model configuration or a pre-trained model instance.\n        tokenizer: The tokenizer corresponding to the model.\n        use_conv (bool): If True, add convolutional layers on top of the transformer output.\n    \"\"\"\n    def __init__(self, config_or_model, tokenizer, use_conv=False, *args, **kwargs):\n        self.threshold = kwargs.pop(\"threshold\", 0.5)\n        self.use_conv = use_conv\n        super().__init__(config_or_model, tokenizer, *args, **kwargs)\n        self.metadata[\"model_name\"] = \"DeepSEA_OmniGenome\"\n\n        if self.use_conv:\n            # Optional convolutional layers, mimicking the original DeepSEA CNN architecture\n            self.conv_layers = torch.nn.Sequential(\n                torch.nn.Conv1d(self.config.hidden_size, 320, kernel_size=8, padding=4),\n                torch.nn.ReLU(),\n                torch.nn.MaxPool1d(kernel_size=4, stride=4),\n                torch.nn.Dropout(0.2),\n                torch.nn.Conv1d(320, 480, kernel_size=8, padding=4),\n                torch.nn.ReLU(),\n                torch.nn.MaxPool1d(kernel_size=4, stride=4),\n                torch.nn.Dropout(0.2),\n                torch.nn.Conv1d(480, 960, kernel_size=8, padding=4),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(0.5),\n            )\n            conv_output_dim = 960\n        else:\n            # If not using conv layers, the input to the classifier is the transformer's hidden size\n            conv_output_dim = self.config.hidden_size\n\n        # The original DeepSEA classification head architecture\n        self.deepsea_classifier = torch.nn.Sequential(\n            torch.nn.Linear(conv_output_dim, 925),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(925, self.config.num_labels)  # num_labels should be 919 for DeepSEA\n        )\n\n        self.loss_fn = torch.nn.BCEWithLogitsLoss() # Suitable for multi-label classification\n        self.pooler = OmniPooling(self.config)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.model_info() # Print model summary\n\n    def forward(self, **inputs):\n        \"\"\"Defines the forward pass of the model.\"\"\"\n        labels = inputs.pop(\"labels\", None)\n        \n        # Get embeddings from the OmniGenome backbone\n        last_hidden_state = self.last_hidden_state_forward(**inputs)\n        last_hidden_state = self.dropout(last_hidden_state)\n        last_hidden_state = self.activation(last_hidden_state)\n\n        if self.use_conv:\n            # Apply convolutional layers\n            # Reshape from (batch, seq_len, hidden) to (batch, hidden, seq_len) for Conv1d\n            conv_input = last_hidden_state.transpose(1, 2)\n            conv_output = self.conv_layers(conv_input)\n            # Pool the output of the conv layers to a fixed size vector\n            pooled_output = torch.nn.functional.adaptive_avg_pool1d(conv_output, 1).squeeze(-1)\n        else:\n            # Use standard pooling on the transformer output\n            pooled_output = self.pooler(inputs, last_hidden_state)\n\n        # Get logits from the final classification head\n        logits = self.deepsea_classifier(pooled_output)\n        outputs = {\"logits\": logits, \"last_hidden_state\": last_hidden_state}\n\n        # Calculate loss if labels are provided\n        if labels is not None:\n            loss = self.loss_fn(logits, labels.to(torch.float32))\n            outputs[\"loss\"] = loss\n\n        return outputs\n\n    def predict(self, sequence_or_inputs, **kwargs):\n        \"\"\"Generates predictions for a given sequence or tokenized input.\"\"\"\n        if not isinstance(sequence_or_inputs, (BatchEncoding, dict)):\n            # If input is a raw sequence, tokenize it\n            inputs = self.tokenizer(\n                sequence_or_inputs,\n                padding=kwargs.pop(\"padding\", \"max_length\"),\n                max_length=kwargs.pop(\"max_length\", 1024),\n                truncation=True,\n                return_tensors=\"pt\",\n                **kwargs,\n            )\n        else:\n            inputs = sequence_or_inputs\n        \n        # Move inputs to the correct device\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = self(**inputs)\n        \n        # Convert logits to probabilities and then to binary predictions\n        probabilities = self.sigmoid(outputs[\"logits\"])\n        predictions = (probabilities >= self.threshold).to(torch.int)\n\n        return {\n            \"predictions\": predictions,\n            \"probabilities\": probabilities,\n            \"logits\": outputs[\"logits\"],\n            \"last_hidden_state\": outputs[\"last_hidden_state\"],\n        }\n\nprint(\"OmniModelForMultiLabelClassification defined.\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniModelForMultiLabelClassification defined.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Data Loading and Preprocessing\n\nThis section handles the data loading. We define a helper function `load_deepsea_npy_data` to parse the specific format of the DeepSEA `.npy` files. Then, we create a `DeepSEADataset` class that inherits from `OmniDataset` and uses this loader. The dataset class is responsible for converting DNA sequences into a format suitable for the OmniGenome tokenizer (i.e., space-separated tokens)."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T22:50:31.729241Z",
     "start_time": "2025-07-31T22:50:31.724913Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "class DeepSEADataset(OmniDataset):\n",
    "    \"\"\"\n",
    "    为DeepSEA任务设计的数据集，处理DNA序列到token序列的转换\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metadata[key] = value\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        \"\"\"\n",
    "        准备DeepSEA的输入数据\n",
    "\n",
    "        Expected instance format:\n",
    "        {\n",
    "            'sequence': DNA sequence string (e.g., \"ATCGATCG...\")\n",
    "            'labels': binary labels as numpy array of shape (919,)\n",
    "        }\n",
    "        \"\"\"\n",
    "        labels = None\n",
    "        if isinstance(instance, str):\n",
    "            sequence = instance\n",
    "        elif isinstance(instance, dict):\n",
    "            sequence = (\n",
    "                instance.get(\"seq\", None)\n",
    "                if \"seq\" in instance\n",
    "                else instance.get(\"sequence\", None)\n",
    "            )\n",
    "            label = instance.get(\"label\", None)\n",
    "            labels = instance.get(\"labels\", None)\n",
    "            labels = labels if labels is not None else label\n",
    "        else:\n",
    "            raise Exception(\"Unknown instance format.\")\n",
    "\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Sequence is required\")\n",
    "\n",
    "        if isinstance(sequence, str):\n",
    "            spaced_sequence = ' '.join(list(sequence))\n",
    "        else:\n",
    "            if isinstance(sequence, np.ndarray) and sequence.shape[1] == 4:\n",
    "                base_map = {0: 'A', 1: 'T', 2: 'C', 3: 'G'}\n",
    "                sequence_str = ''.join([base_map[np.argmax(sequence[i])] for i in range(len(sequence))])\n",
    "                spaced_sequence = ' '.join(list(sequence_str))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported sequence format: {type(sequence)}\")\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            spaced_sequence[500-self.max_length//2:500+self.max_length//2],\n",
    "            # spaced_sequence,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "\n",
    "        if labels is not None:\n",
    "            if isinstance(labels, np.ndarray):\n",
    "                labels = torch.from_numpy(labels).float()\n",
    "            elif not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Initialization\n\nNow, let's initialize the tokenizer, the model, and the datasets. This step brings everything together and prepares for the training phase."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Initialize Tokenizer and Model\n",
    "print(\"--- Initializing Tokenizer and Model ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "base_model = AutoModel.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "\n",
    "model = OmniModelForMultiLabelClassification(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    num_labels=919,  # DeepSEA has 919 binary labels for different chromatin features\n",
    "    threshold=0.5,\n",
    "    use_conv=USE_CONV_LAYERS\n",
    ")\n",
    "model.to(DEVICE).to(torch.float32) # Move model to the selected device\n",
    "\n",
    "# 2. Create Datasets\n",
    "print(\"\\n--- Creating Datasets ---\")\n",
    "train_set = DeepSEADataset(\n",
    "    data_source=TRAIN_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    ")\n",
    "test_set = DeepSEADataset(\n",
    "    data_source=TEST_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    ")\n",
    "valid_set = DeepSEADataset(\n",
    "    data_source=VALID_FILE,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_examples=MAX_EXAMPLES,\n",
    ") if os.path.exists(VALID_FILE) else None\n",
    "\n",
    "print(\"\\n--- Initialization Complete ---\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "if valid_set:\n",
    "    print(f\"Validation set size: {len(valid_set)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Training the Model\n\nWith everything set up, we can now train the model. We'll use the `AccelerateTrainer` for a streamlined and efficient training loop. The trainer handles the training loop, evaluation, early stopping, and device placement automatically."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Set random seed for reproducibility across all libraries\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# Create DataLoaders for batching\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\nvalid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE) if valid_set else None\n\n# Define the metric for evaluation. For DeepSEA, ROC AUC is a standard metric.\nmetrics = [ClassificationMetric(ignore_y=-100).roc_auc_score]\n\n# Create the optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# Initialize the Trainer\ntrainer = AccelerateTrainer(\n    model=model,\n    train_loader=train_loader,\n    eval_loader=valid_loader, # Use validation set for early stopping and checkpointing\n    test_loader=test_loader,\n    optimizer=optimizer,\n    epochs=EPOCHS,\n    metrics=metrics,\n    patience=PATIENCE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    device=DEVICE\n)\n\n# Start Training\nprint(\"--- Starting Training ---\")\ntrainer.train()\nprint(\"--- Training Finished ---\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Evaluation\n\nAfter training is complete, the `AccelerateTrainer` automatically loads the best performing model (based on the validation set). We can now evaluate this model on the held-out test set to get a final, unbiased measure of its performance."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"--- Evaluating on Test Set ---\")\ntest_results = trainer.evaluate(loader=test_loader)\nprint(\"\\nTest Set Performance (based on the best model from training):\")\nfor metric, value in test_results.items():\n    print(f\"  {metric}: {value:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Inference Example\n\nFinally, let's see how to use the fine-tuned model to make a prediction on a new, unseen DNA sequence. This demonstrates the practical application of the trained model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create a sample DNA sequence (must be at least MAX_LENGTH base pairs long)\nsample_sequence = \"AGCT\" * (MAX_LENGTH // 4) # Create a sequence of the required length\n\n# Prepare the sequence for the model (add spaces between characters)\nspaced_sequence = ' '.join(list(sample_sequence))\ninputs = tokenizer(spaced_sequence, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Make a prediction\nwith torch.no_grad():\n    outputs = model.predict(inputs)\n\n# Get the predictions and probabilities\npredictions = outputs['predictions'].cpu().numpy().flatten()\nprobabilities = outputs['probabilities'].cpu().numpy().flatten()\n\nprint(f\"Input sequence length: {len(sample_sequence)} bp\")\nprint(f\"Number of predicted labels: {len(predictions)}\")\n\n# Display predictions for the first 10 transcription factors\nprint(\"\\n--- Predictions for the first 10 TFs ---\")\nfor i in range(10):\n    pred_label = 'Binds' if predictions[i] == 1 else 'Does not bind'\n    print(f\"Label {i+1}: Prediction={pred_label}, Probability={probabilities[i]:.4f}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
